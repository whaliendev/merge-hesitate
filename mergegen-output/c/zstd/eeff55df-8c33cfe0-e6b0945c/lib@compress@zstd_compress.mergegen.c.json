[
    {
        "a_contents": "typedef struct ZSTD_prefixDict_s {\n    const void* dict;\n    size_t dictSize;\n    ZSTD_dictMode_e dictMode;\n} ZSTD_prefixDict;\n\nstruct ZSTD_CCtx_s {\n    const BYTE* nextSrc;    /* next block here to continue on current prefix */\n    const BYTE* base;       /* All regular indexes relative to this position */\n    const BYTE* dictBase;   /* extDict indexes relative to this position */\n    U32   dictLimit;        /* below that point, need extDict */\n    U32   lowLimit;         /* below that point, no more data */\n    U32   nextToUpdate;     /* index from which to continue dictionary update */\n    U32   nextToUpdate3;    /* index from which to continue dictionary update */\n    U32   hashLog3;         /* dispatch table : larger == faster, more memory */\n    U32   loadedDictEnd;    /* index of end of dictionary */\n    ZSTD_compressionStage_e stage;\n    U32   dictID;\n    ZSTD_CCtx_params requestedParams;\n    ZSTD_CCtx_params appliedParams;\n    void* workSpace;\n    size_t workSpaceSize;\n    size_t blockSize;\n    U64 pledgedSrcSizePlusOne;  /* this way, 0 (default) == unknown */\n    U64 consumedSrcSize;\n    XXH64_state_t xxhState;\n    ZSTD_customMem customMem;\n    size_t staticSize;\n\n    seqStore_t seqStore;    /* sequences storage ptrs */\n    optState_t optState;\n    ldmState_t ldmState;    /* long distance matching state */\n    U32* hashTable;\n    U32* hashTable3;\n    U32* chainTable;\n    ZSTD_entropyCTables_t* entropy;\n\n    /* streaming */\n    char*  inBuff;\n    size_t inBuffSize;\n    size_t inToCompress;\n    size_t inBuffPos;\n    size_t inBuffTarget;\n    char*  outBuff;\n    size_t outBuffSize;\n    size_t outBuffContentSize;\n    size_t outBuffFlushedSize;\n    ZSTD_cStreamStage streamStage;\n    U32    frameEnded;\n\n    /* Dictionary */\n    ZSTD_CDict* cdictLocal;\n    const ZSTD_CDict* cdict;\n    ZSTD_prefixDict prefixDict;   /* single-usage dictionary */\n\n    /* Multi-threading */\n    ZSTDMT_CCtx* mtctx;\n};\n",
        "b_contents": "",
        "base_contents": "typedef struct ZSTD_prefixDict_s {\n    const void* dict;\n    size_t dictSize;\n    ZSTD_dictMode_e dictMode;\n} ZSTD_prefixDict;\n\nstruct ZSTD_CCtx_s {\n    const BYTE* nextSrc;    /* next block here to continue on current prefix */\n    const BYTE* base;       /* All regular indexes relative to this position */\n    const BYTE* dictBase;   /* extDict indexes relative to this position */\n    U32   dictLimit;        /* below that point, need extDict */\n    U32   lowLimit;         /* below that point, no more data */\n    U32   nextToUpdate;     /* index from which to continue dictionary update */\n    U32   nextToUpdate3;    /* index from which to continue dictionary update */\n    U32   hashLog3;         /* dispatch table : larger == faster, more memory */\n    U32   loadedDictEnd;    /* index of end of dictionary */\n    ZSTD_compressionStage_e stage;\n    U32   dictID;\n    ZSTD_CCtx_params requestedParams;\n    ZSTD_CCtx_params appliedParams;\n    void* workSpace;\n    size_t workSpaceSize;\n    size_t blockSize;\n    U64 pledgedSrcSizePlusOne;  /* this way, 0 (default) == unknown */\n    U64 consumedSrcSize;\n    XXH64_state_t xxhState;\n    ZSTD_customMem customMem;\n    size_t staticSize;\n\n    seqStore_t seqStore;    /* sequences storage ptrs */\n    optState_t optState;\n    U32* hashTable;\n    U32* hashTable3;\n    U32* chainTable;\n    ZSTD_entropyCTables_t* entropy;\n\n    /* streaming */\n    char*  inBuff;\n    size_t inBuffSize;\n    size_t inToCompress;\n    size_t inBuffPos;\n    size_t inBuffTarget;\n    char*  outBuff;\n    size_t outBuffSize;\n    size_t outBuffContentSize;\n    size_t outBuffFlushedSize;\n    ZSTD_cStreamStage streamStage;\n    U32    frameEnded;\n\n    /* Dictionary */\n    ZSTD_CDict* cdictLocal;\n    const ZSTD_CDict* cdict;\n    ZSTD_prefixDict prefixDict;   /* single-usage dictionary */\n\n    /* Multi-threading */\n    ZSTDMT_CCtx* mtctx;\n};\n",
        "res_region": "",
        "lookback": "ZSTD_CCtx* ZSTD_createCCtx(void)",
        "lookahead": "",
        "label": [
            "base",
            "base_empty"
        ],
        "start_line": 73,
        "end_line": 193,
        "merge_gen_region": null,
        "confidence": 0.6483660340309143,
        "hesitated": true
    },
    {
        "a_contents": "    {   ZSTD_CCtx_params params = dstCCtx->requestedParams;\n        params.cParams = srcCCtx->appliedParams.cParams;",
        "b_contents": "    {   ZSTD_CCtx_params params = dstCCtx->requestedParams;\n        /* Copy only compression parameters related to tables. */\n        params.cParams = srcCCtx->appliedParams.cParams;",
        "base_contents": "    {   ZSTD_CCtx_params params = srcCCtx->appliedParams;",
        "res_region": "    {   ZSTD_CCtx_params params = dstCCtx->requestedParams;\n        /* Copy only compression parameters related to tables. */\n        params.cParams = srcCCtx->appliedParams.cParams;",
        "lookback": "        /* Copy only compression parameters related to tables. */",
        "lookahead": "        params.fParams = fParams;",
        "label": [
            "base",
            "complex_conflict"
        ],
        "start_line": 1161,
        "end_line": 1170,
        "merge_gen_region": null,
        "confidence": 0.8403277397155762,
        "hesitated": true
    },
    {
        "a_contents": "/*! ZSTD_storeSeq() :\n    Store a sequence (literal length, literals, offset code and match length code) into seqStore_t.\n    `offsetCode` : distance to match, or 0 == repCode.\n    `matchCode` : matchLength - MINMATCH\n*/\nMEM_STATIC void ZSTD_storeSeq(seqStore_t* seqStorePtr, size_t litLength, const void* literals, U32 offsetCode, size_t matchCode)\n{\n#if defined(ZSTD_DEBUG) && (ZSTD_DEBUG >= 6)\n    static const BYTE* g_start = NULL;\n    U32 const pos = (U32)((const BYTE*)literals - g_start);\n    if (g_start==NULL) g_start = (const BYTE*)literals;\n    if ((pos > 0) && (pos < 1000000000))\n        DEBUGLOG(6, \"Cpos %6u :%5u literals & match %3u bytes at distance %6u\",\n               pos, (U32)litLength, (U32)matchCode+MINMATCH, (U32)offsetCode);\n#endif\n    /* copy Literals */\n    assert(seqStorePtr->lit + litLength <= seqStorePtr->litStart + 128 KB);\n    ZSTD_wildcopy(seqStorePtr->lit, literals, litLength);\n    seqStorePtr->lit += litLength;\n\n    /* literal Length */\n    if (litLength>0xFFFF) {\n        seqStorePtr->longLengthID = 1;\n        seqStorePtr->longLengthPos = (U32)(seqStorePtr->sequences - seqStorePtr->sequencesStart);\n    }\n    seqStorePtr->sequences[0].litLength = (U16)litLength;\n\n    /* match offset */\n    seqStorePtr->sequences[0].offset = offsetCode + 1;\n\n    /* match Length */\n    if (matchCode>0xFFFF) {\n        seqStorePtr->longLengthID = 2;\n        seqStorePtr->longLengthPos = (U32)(seqStorePtr->sequences - seqStorePtr->sequencesStart);\n    }\n    seqStorePtr->sequences[0].matchLength = (U16)matchCode;\n\n    seqStorePtr->sequences++;\n}\n\n\n/*-*************************************\n*  Match length counter\n***************************************/\nstatic unsigned ZSTD_NbCommonBytes (register size_t val)\n{\n    if (MEM_isLittleEndian()) {\n        if (MEM_64bits()) {\n#       if defined(_MSC_VER) && defined(_WIN64)\n            unsigned long r = 0;\n            _BitScanForward64( &r, (U64)val );\n            return (unsigned)(r>>3);\n#       elif defined(__GNUC__) && (__GNUC__ >= 3)\n            return (__builtin_ctzll((U64)val) >> 3);\n#       else\n            static const int DeBruijnBytePos[64] = { 0, 0, 0, 0, 0, 1, 1, 2,\n                                                     0, 3, 1, 3, 1, 4, 2, 7,\n                                                     0, 2, 3, 6, 1, 5, 3, 5,\n                                                     1, 3, 4, 4, 2, 5, 6, 7,\n                                                     7, 0, 1, 2, 3, 3, 4, 6,\n                                                     2, 6, 5, 5, 3, 4, 5, 6,\n                                                     7, 1, 2, 4, 6, 4, 4, 5,\n                                                     7, 2, 6, 5, 7, 6, 7, 7 };\n            return DeBruijnBytePos[((U64)((val & -(long long)val) * 0x0218A392CDABBD3FULL)) >> 58];\n#       endif\n        } else { /* 32 bits */\n#       if defined(_MSC_VER)\n            unsigned long r=0;\n            _BitScanForward( &r, (U32)val );\n            return (unsigned)(r>>3);\n#       elif defined(__GNUC__) && (__GNUC__ >= 3)\n            return (__builtin_ctz((U32)val) >> 3);\n#       else\n            static const int DeBruijnBytePos[32] = { 0, 0, 3, 0, 3, 1, 3, 0,\n                                                     3, 2, 2, 1, 3, 2, 0, 1,\n                                                     3, 3, 1, 2, 2, 2, 2, 0,\n                                                     3, 1, 2, 0, 1, 0, 1, 1 };\n            return DeBruijnBytePos[((U32)((val & -(S32)val) * 0x077CB531U)) >> 27];\n#       endif\n        }\n    } else {  /* Big Endian CPU */\n        if (MEM_64bits()) {\n#       if defined(_MSC_VER) && defined(_WIN64)\n            unsigned long r = 0;\n            _BitScanReverse64( &r, val );\n            return (unsigned)(r>>3);\n#       elif defined(__GNUC__) && (__GNUC__ >= 3)\n            return (__builtin_clzll(val) >> 3);\n#       else\n            unsigned r;\n            const unsigned n32 = sizeof(size_t)*4;   /* calculate this way due to compiler complaining in 32-bits mode */\n            if (!(val>>n32)) { r=4; } else { r=0; val>>=n32; }\n            if (!(val>>16)) { r+=2; val>>=8; } else { val>>=24; }\n            r += (!val);\n            return r;\n#       endif\n        } else { /* 32 bits */\n#       if defined(_MSC_VER)\n            unsigned long r = 0;\n            _BitScanReverse( &r, (unsigned long)val );\n            return (unsigned)(r>>3);\n#       elif defined(__GNUC__) && (__GNUC__ >= 3)\n            return (__builtin_clz((U32)val) >> 3);\n#       else\n            unsigned r;\n            if (!(val>>16)) { r=2; val>>=8; } else { r=0; val>>=24; }\n            r += (!val);\n            return r;\n#       endif\n    }   }\n}\n\n\nstatic size_t ZSTD_count(const BYTE* pIn, const BYTE* pMatch, const BYTE* const pInLimit)\n{\n    const BYTE* const pStart = pIn;\n    const BYTE* const pInLoopLimit = pInLimit - (sizeof(size_t)-1);\n\n    while (pIn < pInLoopLimit) {\n        size_t const diff = MEM_readST(pMatch) ^ MEM_readST(pIn);\n        if (!diff) { pIn+=sizeof(size_t); pMatch+=sizeof(size_t); continue; }\n        pIn += ZSTD_NbCommonBytes(diff);\n        return (size_t)(pIn - pStart);\n    }\n    if (MEM_64bits()) if ((pIn<(pInLimit-3)) && (MEM_read32(pMatch) == MEM_read32(pIn))) { pIn+=4; pMatch+=4; }\n    if ((pIn<(pInLimit-1)) && (MEM_read16(pMatch) == MEM_read16(pIn))) { pIn+=2; pMatch+=2; }\n    if ((pIn<pInLimit) && (*pMatch == *pIn)) pIn++;\n    return (size_t)(pIn - pStart);\n}\n\n/** ZSTD_count_2segments() :\n*   can count match length with `ip` & `match` in 2 different segments.\n*   convention : on reaching mEnd, match count continue starting from iStart\n*/\nstatic size_t ZSTD_count_2segments(const BYTE* ip, const BYTE* match, const BYTE* iEnd, const BYTE* mEnd, const BYTE* iStart)\n{\n    const BYTE* const vEnd = MIN( ip + (mEnd - match), iEnd);\n    size_t const matchLength = ZSTD_count(ip, match, vEnd);\n    if (match + matchLength != mEnd) return matchLength;\n    return matchLength + ZSTD_count(ip+matchLength, iStart, iEnd);\n}\n\n\n/*-*************************************\n*  Hashes\n***************************************/\nstatic const U32 prime3bytes = 506832829U;\nstatic U32    ZSTD_hash3(U32 u, U32 h) { return ((u << (32-24)) * prime3bytes)  >> (32-h) ; }\nMEM_STATIC size_t ZSTD_hash3Ptr(const void* ptr, U32 h) { return ZSTD_hash3(MEM_readLE32(ptr), h); } /* only in zstd_opt.h */\n\nstatic const U32 prime4bytes = 2654435761U;\nstatic U32    ZSTD_hash4(U32 u, U32 h) { return (u * prime4bytes) >> (32-h) ; }\nstatic size_t ZSTD_hash4Ptr(const void* ptr, U32 h) { return ZSTD_hash4(MEM_read32(ptr), h); }\n\nstatic const U64 prime5bytes = 889523592379ULL;\nstatic size_t ZSTD_hash5(U64 u, U32 h) { return (size_t)(((u  << (64-40)) * prime5bytes) >> (64-h)) ; }\nstatic size_t ZSTD_hash5Ptr(const void* p, U32 h) { return ZSTD_hash5(MEM_readLE64(p), h); }\n\nstatic const U64 prime6bytes = 227718039650203ULL;\nstatic size_t ZSTD_hash6(U64 u, U32 h) { return (size_t)(((u  << (64-48)) * prime6bytes) >> (64-h)) ; }\nstatic size_t ZSTD_hash6Ptr(const void* p, U32 h) { return ZSTD_hash6(MEM_readLE64(p), h); }\n\nstatic const U64 prime7bytes = 58295818150454627ULL;\nstatic size_t ZSTD_hash7(U64 u, U32 h) { return (size_t)(((u  << (64-56)) * prime7bytes) >> (64-h)) ; }\nstatic size_t ZSTD_hash7Ptr(const void* p, U32 h) { return ZSTD_hash7(MEM_readLE64(p), h); }\n\nstatic const U64 prime8bytes = 0xCF1BBCDCB7A56463ULL;\nstatic size_t ZSTD_hash8(U64 u, U32 h) { return (size_t)(((u) * prime8bytes) >> (64-h)) ; }\nstatic size_t ZSTD_hash8Ptr(const void* p, U32 h) { return ZSTD_hash8(MEM_readLE64(p), h); }\n\nstatic size_t ZSTD_hashPtr(const void* p, U32 hBits, U32 mls)\n{\n    switch(mls)\n    {\n    default:\n    case 4: return ZSTD_hash4Ptr(p, hBits);\n    case 5: return ZSTD_hash5Ptr(p, hBits);\n    case 6: return ZSTD_hash6Ptr(p, hBits);\n    case 7: return ZSTD_hash7Ptr(p, hBits);\n    case 8: return ZSTD_hash8Ptr(p, hBits);\n    }\n}\n\n\n/*-*************************************\n*  Fast Scan\n***************************************/\nstatic void ZSTD_fillHashTable (ZSTD_CCtx* zc, const void* end, const U32 mls)\n{\n    U32* const hashTable = zc->hashTable;\n    U32  const hBits = zc->appliedParams.cParams.hashLog;\n    const BYTE* const base = zc->base;\n    const BYTE* ip = base + zc->nextToUpdate;\n    const BYTE* const iend = ((const BYTE*)end) - HASH_READ_SIZE;\n    const size_t fastHashFillStep = 3;\n\n    while(ip <= iend) {\n        hashTable[ZSTD_hashPtr(ip, hBits, mls)] = (U32)(ip - base);\n        ip += fastHashFillStep;\n    }\n}\n\n\nFORCE_INLINE_TEMPLATE\nsize_t ZSTD_compressBlock_fast_generic(ZSTD_CCtx* cctx,\n                                       const void* src, size_t srcSize,\n                                       const U32 mls)\n{\n    U32* const hashTable = cctx->hashTable;\n    U32  const hBits = cctx->appliedParams.cParams.hashLog;\n    seqStore_t* seqStorePtr = &(cctx->seqStore);\n    const BYTE* const base = cctx->base;\n    const BYTE* const istart = (const BYTE*)src;\n    const BYTE* ip = istart;\n    const BYTE* anchor = istart;\n    const U32   lowestIndex = cctx->dictLimit;\n    const BYTE* const lowest = base + lowestIndex;\n    const BYTE* const iend = istart + srcSize;\n    const BYTE* const ilimit = iend - HASH_READ_SIZE;\n    U32 offset_1=seqStorePtr->rep[0], offset_2=seqStorePtr->rep[1];\n    U32 offsetSaved = 0;\n\n    /* init */\n    ip += (ip==lowest);\n    {   U32 const maxRep = (U32)(ip-lowest);\n        if (offset_2 > maxRep) offsetSaved = offset_2, offset_2 = 0;\n        if (offset_1 > maxRep) offsetSaved = offset_1, offset_1 = 0;\n    }\n\n    /* Main Search Loop */\n    while (ip < ilimit) {   /* < instead of <=, because repcode check at (ip+1) */\n        size_t mLength;\n        size_t const h = ZSTD_hashPtr(ip, hBits, mls);\n        U32 const current = (U32)(ip-base);\n        U32 const matchIndex = hashTable[h];\n        const BYTE* match = base + matchIndex;\n        hashTable[h] = current;   /* update hash table */\n\n        if ((offset_1 > 0) & (MEM_read32(ip+1-offset_1) == MEM_read32(ip+1))) {\n            mLength = ZSTD_count(ip+1+4, ip+1+4-offset_1, iend) + 4;\n            ip++;\n            ZSTD_storeSeq(seqStorePtr, ip-anchor, anchor, 0, mLength-MINMATCH);\n        } else {\n            U32 offset;\n            if ( (matchIndex <= lowestIndex) || (MEM_read32(match) != MEM_read32(ip)) ) {\n                ip += ((ip-anchor) >> g_searchStrength) + 1;\n                continue;\n            }\n            mLength = ZSTD_count(ip+4, match+4, iend) + 4;\n            offset = (U32)(ip-match);\n            while (((ip>anchor) & (match>lowest)) && (ip[-1] == match[-1])) { ip--; match--; mLength++; } /* catch up */\n            offset_2 = offset_1;\n            offset_1 = offset;\n\n            ZSTD_storeSeq(seqStorePtr, ip-anchor, anchor, offset + ZSTD_REP_MOVE, mLength-MINMATCH);\n        }\n\n        /* match found */\n        ip += mLength;\n        anchor = ip;\n\n        if (ip <= ilimit) {\n            /* Fill Table */\n            hashTable[ZSTD_hashPtr(base+current+2, hBits, mls)] = current+2;  /* here because current+2 could be > iend-8 */\n            hashTable[ZSTD_hashPtr(ip-2, hBits, mls)] = (U32)(ip-2-base);\n            /* check immediate repcode */\n            while ( (ip <= ilimit)\n                 && ( (offset_2>0)\n                 & (MEM_read32(ip) == MEM_read32(ip - offset_2)) )) {\n                /* store sequence */\n                size_t const rLength = ZSTD_count(ip+4, ip+4-offset_2, iend) + 4;\n                { U32 const tmpOff = offset_2; offset_2 = offset_1; offset_1 = tmpOff; }  /* swap offset_2 <=> offset_1 */\n                hashTable[ZSTD_hashPtr(ip, hBits, mls)] = (U32)(ip-base);\n                ZSTD_storeSeq(seqStorePtr, 0, anchor, 0, rLength-MINMATCH);\n                ip += rLength;\n                anchor = ip;\n                continue;   /* faster when present ... (?) */\n    }   }   }\n\n    /* save reps for next block */\n    seqStorePtr->repToConfirm[0] = offset_1 ? offset_1 : offsetSaved;\n    seqStorePtr->repToConfirm[1] = offset_2 ? offset_2 : offsetSaved;\n\n    /* Return the last literals size */\n    return iend - anchor;\n}\n\nstatic size_t ZSTD_compressBlock_fast(ZSTD_CCtx* ctx,\n                       const void* src, size_t srcSize)\n{\n    const U32 mls = ctx->appliedParams.cParams.searchLength;\n    switch(mls)\n    {\n    default: /* includes case 3 */\n    case 4 :\n        return ZSTD_compressBlock_fast_generic(ctx, src, srcSize, 4);\n    case 5 :\n        return ZSTD_compressBlock_fast_generic(ctx, src, srcSize, 5);\n    case 6 :\n        return ZSTD_compressBlock_fast_generic(ctx, src, srcSize, 6);\n    case 7 :\n        return ZSTD_compressBlock_fast_generic(ctx, src, srcSize, 7);\n    }\n}\n\n\nstatic size_t ZSTD_compressBlock_fast_extDict_generic(\n                                 ZSTD_CCtx* ctx,\n                                 const void* src, size_t srcSize,\n                                 const U32 mls)\n{\n    U32* hashTable = ctx->hashTable;\n    const U32 hBits = ctx->appliedParams.cParams.hashLog;\n    seqStore_t* seqStorePtr = &(ctx->seqStore);\n    const BYTE* const base = ctx->base;\n    const BYTE* const dictBase = ctx->dictBase;\n    const BYTE* const istart = (const BYTE*)src;\n    const BYTE* ip = istart;\n    const BYTE* anchor = istart;\n    const U32   lowestIndex = ctx->lowLimit;\n    const BYTE* const dictStart = dictBase + lowestIndex;\n    const U32   dictLimit = ctx->dictLimit;\n    const BYTE* const lowPrefixPtr = base + dictLimit;\n    const BYTE* const dictEnd = dictBase + dictLimit;\n    const BYTE* const iend = istart + srcSize;\n    const BYTE* const ilimit = iend - 8;\n    U32 offset_1=seqStorePtr->rep[0], offset_2=seqStorePtr->rep[1];\n\n    /* Search Loop */\n    while (ip < ilimit) {  /* < instead of <=, because (ip+1) */\n        const size_t h = ZSTD_hashPtr(ip, hBits, mls);\n        const U32 matchIndex = hashTable[h];\n        const BYTE* matchBase = matchIndex < dictLimit ? dictBase : base;\n        const BYTE* match = matchBase + matchIndex;\n        const U32 current = (U32)(ip-base);\n        const U32 repIndex = current + 1 - offset_1;   /* offset_1 expected <= current +1 */\n        const BYTE* repBase = repIndex < dictLimit ? dictBase : base;\n        const BYTE* repMatch = repBase + repIndex;\n        size_t mLength;\n        hashTable[h] = current;   /* update hash table */\n\n        if ( (((U32)((dictLimit-1) - repIndex) >= 3) /* intentional underflow */ & (repIndex > lowestIndex))\n           && (MEM_read32(repMatch) == MEM_read32(ip+1)) ) {\n            const BYTE* repMatchEnd = repIndex < dictLimit ? dictEnd : iend;\n            mLength = ZSTD_count_2segments(ip+1+4, repMatch+4, iend, repMatchEnd, lowPrefixPtr) + 4;\n            ip++;\n            ZSTD_storeSeq(seqStorePtr, ip-anchor, anchor, 0, mLength-MINMATCH);\n        } else {\n            if ( (matchIndex < lowestIndex) ||\n                 (MEM_read32(match) != MEM_read32(ip)) ) {\n                ip += ((ip-anchor) >> g_searchStrength) + 1;\n                continue;\n            }\n            {   const BYTE* matchEnd = matchIndex < dictLimit ? dictEnd : iend;\n                const BYTE* lowMatchPtr = matchIndex < dictLimit ? dictStart : lowPrefixPtr;\n                U32 offset;\n                mLength = ZSTD_count_2segments(ip+4, match+4, iend, matchEnd, lowPrefixPtr) + 4;\n                while (((ip>anchor) & (match>lowMatchPtr)) && (ip[-1] == match[-1])) { ip--; match--; mLength++; }   /* catch up */\n                offset = current - matchIndex;\n                offset_2 = offset_1;\n                offset_1 = offset;\n                ZSTD_storeSeq(seqStorePtr, ip-anchor, anchor, offset + ZSTD_REP_MOVE, mLength-MINMATCH);\n        }   }\n\n        /* found a match : store it */\n        ip += mLength;\n        anchor = ip;\n\n        if (ip <= ilimit) {\n            /* Fill Table */\n            hashTable[ZSTD_hashPtr(base+current+2, hBits, mls)] = current+2;\n            hashTable[ZSTD_hashPtr(ip-2, hBits, mls)] = (U32)(ip-2-base);\n            /* check immediate repcode */\n            while (ip <= ilimit) {\n                U32 const current2 = (U32)(ip-base);\n                U32 const repIndex2 = current2 - offset_2;\n                const BYTE* repMatch2 = repIndex2 < dictLimit ? dictBase + repIndex2 : base + repIndex2;\n                if ( (((U32)((dictLimit-1) - repIndex2) >= 3) & (repIndex2 > lowestIndex))  /* intentional overflow */\n                   && (MEM_read32(repMatch2) == MEM_read32(ip)) ) {\n                    const BYTE* const repEnd2 = repIndex2 < dictLimit ? dictEnd : iend;\n                    size_t const repLength2 = ZSTD_count_2segments(ip+4, repMatch2+4, iend, repEnd2, lowPrefixPtr) + 4;\n                    U32 tmpOffset = offset_2; offset_2 = offset_1; offset_1 = tmpOffset;   /* swap offset_2 <=> offset_1 */\n                    ZSTD_storeSeq(seqStorePtr, 0, anchor, 0, repLength2-MINMATCH);\n                    hashTable[ZSTD_hashPtr(ip, hBits, mls)] = current2;\n                    ip += repLength2;\n                    anchor = ip;\n                    continue;\n                }\n                break;\n    }   }   }\n\n    /* save reps for next block */\n    seqStorePtr->repToConfirm[0] = offset_1; seqStorePtr->repToConfirm[1] = offset_2;\n\n    /* Return the last literals size */\n    return iend - anchor;\n}\n\nstatic size_t ZSTD_compressBlock_fast_extDict(ZSTD_CCtx* ctx,\n                         const void* src, size_t srcSize)\n{\n    U32 const mls = ctx->appliedParams.cParams.searchLength;\n    switch(mls)\n    {\n    default: /* includes case 3 */\n    case 4 :\n        return ZSTD_compressBlock_fast_extDict_generic(ctx, src, srcSize, 4);\n    case 5 :\n        return ZSTD_compressBlock_fast_extDict_generic(ctx, src, srcSize, 5);\n    case 6 :\n        return ZSTD_compressBlock_fast_extDict_generic(ctx, src, srcSize, 6);\n    case 7 :\n        return ZSTD_compressBlock_fast_extDict_generic(ctx, src, srcSize, 7);\n    }\n}\n\n\n/*-*************************************\n*  Double Fast\n***************************************/\nstatic void ZSTD_fillDoubleHashTable (ZSTD_CCtx* cctx, const void* end, const U32 mls)\n{\n    U32* const hashLarge = cctx->hashTable;\n    U32  const hBitsL = cctx->appliedParams.cParams.hashLog;\n    U32* const hashSmall = cctx->chainTable;\n    U32  const hBitsS = cctx->appliedParams.cParams.chainLog;\n    const BYTE* const base = cctx->base;\n    const BYTE* ip = base + cctx->nextToUpdate;\n    const BYTE* const iend = ((const BYTE*)end) - HASH_READ_SIZE;\n    const size_t fastHashFillStep = 3;\n\n    while(ip <= iend) {\n        hashSmall[ZSTD_hashPtr(ip, hBitsS, mls)] = (U32)(ip - base);\n        hashLarge[ZSTD_hashPtr(ip, hBitsL, 8)] = (U32)(ip - base);\n        ip += fastHashFillStep;\n    }\n}\n\n\nFORCE_INLINE_TEMPLATE\nsize_t ZSTD_compressBlock_doubleFast_generic(ZSTD_CCtx* cctx,\n                                 const void* src, size_t srcSize,\n                                 const U32 mls)\n{\n    U32* const hashLong = cctx->hashTable;\n    const U32 hBitsL = cctx->appliedParams.cParams.hashLog;\n    U32* const hashSmall = cctx->chainTable;\n    const U32 hBitsS = cctx->appliedParams.cParams.chainLog;\n    seqStore_t* seqStorePtr = &(cctx->seqStore);\n    const BYTE* const base = cctx->base;\n    const BYTE* const istart = (const BYTE*)src;\n    const BYTE* ip = istart;\n    const BYTE* anchor = istart;\n    const U32 lowestIndex = cctx->dictLimit;\n    const BYTE* const lowest = base + lowestIndex;\n    const BYTE* const iend = istart + srcSize;\n    const BYTE* const ilimit = iend - HASH_READ_SIZE;\n    U32 offset_1=seqStorePtr->rep[0], offset_2=seqStorePtr->rep[1];\n    U32 offsetSaved = 0;\n\n    /* init */\n    ip += (ip==lowest);\n    {   U32 const maxRep = (U32)(ip-lowest);\n        if (offset_2 > maxRep) offsetSaved = offset_2, offset_2 = 0;\n        if (offset_1 > maxRep) offsetSaved = offset_1, offset_1 = 0;\n    }\n\n    /* Main Search Loop */\n    while (ip < ilimit) {   /* < instead of <=, because repcode check at (ip+1) */\n        size_t mLength;\n        size_t const h2 = ZSTD_hashPtr(ip, hBitsL, 8);\n        size_t const h = ZSTD_hashPtr(ip, hBitsS, mls);\n        U32 const current = (U32)(ip-base);\n        U32 const matchIndexL = hashLong[h2];\n        U32 const matchIndexS = hashSmall[h];\n        const BYTE* matchLong = base + matchIndexL;\n        const BYTE* match = base + matchIndexS;\n        hashLong[h2] = hashSmall[h] = current;   /* update hash tables */\n\n        assert(offset_1 <= current);   /* supposed guaranteed by construction */\n        if ((offset_1 > 0) & (MEM_read32(ip+1-offset_1) == MEM_read32(ip+1))) {\n            /* favor repcode */\n            mLength = ZSTD_count(ip+1+4, ip+1+4-offset_1, iend) + 4;\n            ip++;\n            ZSTD_storeSeq(seqStorePtr, ip-anchor, anchor, 0, mLength-MINMATCH);\n        } else {\n            U32 offset;\n            if ( (matchIndexL > lowestIndex) && (MEM_read64(matchLong) == MEM_read64(ip)) ) {\n                mLength = ZSTD_count(ip+8, matchLong+8, iend) + 8;\n                offset = (U32)(ip-matchLong);\n                while (((ip>anchor) & (matchLong>lowest)) && (ip[-1] == matchLong[-1])) { ip--; matchLong--; mLength++; } /* catch up */\n            } else if ( (matchIndexS > lowestIndex) && (MEM_read32(match) == MEM_read32(ip)) ) {\n                size_t const hl3 = ZSTD_hashPtr(ip+1, hBitsL, 8);\n                U32 const matchIndexL3 = hashLong[hl3];\n                const BYTE* matchL3 = base + matchIndexL3;\n                hashLong[hl3] = current + 1;\n                if ( (matchIndexL3 > lowestIndex) && (MEM_read64(matchL3) == MEM_read64(ip+1)) ) {\n                    mLength = ZSTD_count(ip+9, matchL3+8, iend) + 8;\n                    ip++;\n                    offset = (U32)(ip-matchL3);\n                    while (((ip>anchor) & (matchL3>lowest)) && (ip[-1] == matchL3[-1])) { ip--; matchL3--; mLength++; } /* catch up */\n                } else {\n                    mLength = ZSTD_count(ip+4, match+4, iend) + 4;\n                    offset = (U32)(ip-match);\n                    while (((ip>anchor) & (match>lowest)) && (ip[-1] == match[-1])) { ip--; match--; mLength++; } /* catch up */\n                }\n            } else {\n                ip += ((ip-anchor) >> g_searchStrength) + 1;\n                continue;\n            }\n\n            offset_2 = offset_1;\n            offset_1 = offset;\n\n            ZSTD_storeSeq(seqStorePtr, ip-anchor, anchor, offset + ZSTD_REP_MOVE, mLength-MINMATCH);\n        }\n\n        /* match found */\n        ip += mLength;\n        anchor = ip;\n\n        if (ip <= ilimit) {\n            /* Fill Table */\n            hashLong[ZSTD_hashPtr(base+current+2, hBitsL, 8)] =\n                hashSmall[ZSTD_hashPtr(base+current+2, hBitsS, mls)] = current+2;  /* here because current+2 could be > iend-8 */\n            hashLong[ZSTD_hashPtr(ip-2, hBitsL, 8)] =\n                hashSmall[ZSTD_hashPtr(ip-2, hBitsS, mls)] = (U32)(ip-2-base);\n\n            /* check immediate repcode */\n            while ( (ip <= ilimit)\n                 && ( (offset_2>0)\n                 & (MEM_read32(ip) == MEM_read32(ip - offset_2)) )) {\n                /* store sequence */\n                size_t const rLength = ZSTD_count(ip+4, ip+4-offset_2, iend) + 4;\n                { U32 const tmpOff = offset_2; offset_2 = offset_1; offset_1 = tmpOff; } /* swap offset_2 <=> offset_1 */\n                hashSmall[ZSTD_hashPtr(ip, hBitsS, mls)] = (U32)(ip-base);\n                hashLong[ZSTD_hashPtr(ip, hBitsL, 8)] = (U32)(ip-base);\n                ZSTD_storeSeq(seqStorePtr, 0, anchor, 0, rLength-MINMATCH);\n                ip += rLength;\n                anchor = ip;\n                continue;   /* faster when present ... (?) */\n    }   }   }\n\n    /* save reps for next block */\n    seqStorePtr->repToConfirm[0] = offset_1 ? offset_1 : offsetSaved;\n    seqStorePtr->repToConfirm[1] = offset_2 ? offset_2 : offsetSaved;\n\n    /* Return the last literals size */\n    return iend - anchor;\n}\n\n\nstatic size_t ZSTD_compressBlock_doubleFast(ZSTD_CCtx* ctx,\n                                            const void* src, size_t srcSize)\n{\n    const U32 mls = ctx->appliedParams.cParams.searchLength;\n    switch(mls)\n    {\n    default: /* includes case 3 */\n    case 4 :\n        return ZSTD_compressBlock_doubleFast_generic(ctx, src, srcSize, 4);\n    case 5 :\n        return ZSTD_compressBlock_doubleFast_generic(ctx, src, srcSize, 5);\n    case 6 :\n        return ZSTD_compressBlock_doubleFast_generic(ctx, src, srcSize, 6);\n    case 7 :\n        return ZSTD_compressBlock_doubleFast_generic(ctx, src, srcSize, 7);\n    }\n}\n\n\nstatic size_t ZSTD_compressBlock_doubleFast_extDict_generic(\n                                 ZSTD_CCtx* ctx,\n                                 const void* src, size_t srcSize,\n                                 const U32 mls)\n{\n    U32* const hashLong = ctx->hashTable;\n    U32  const hBitsL = ctx->appliedParams.cParams.hashLog;\n    U32* const hashSmall = ctx->chainTable;\n    U32  const hBitsS = ctx->appliedParams.cParams.chainLog;\n    seqStore_t* seqStorePtr = &(ctx->seqStore);\n    const BYTE* const base = ctx->base;\n    const BYTE* const dictBase = ctx->dictBase;\n    const BYTE* const istart = (const BYTE*)src;\n    const BYTE* ip = istart;\n    const BYTE* anchor = istart;\n    const U32   lowestIndex = ctx->lowLimit;\n    const BYTE* const dictStart = dictBase + lowestIndex;\n    const U32   dictLimit = ctx->dictLimit;\n    const BYTE* const lowPrefixPtr = base + dictLimit;\n    const BYTE* const dictEnd = dictBase + dictLimit;\n    const BYTE* const iend = istart + srcSize;\n    const BYTE* const ilimit = iend - 8;\n    U32 offset_1=seqStorePtr->rep[0], offset_2=seqStorePtr->rep[1];\n\n    /* Search Loop */\n    while (ip < ilimit) {  /* < instead of <=, because (ip+1) */\n        const size_t hSmall = ZSTD_hashPtr(ip, hBitsS, mls);\n        const U32 matchIndex = hashSmall[hSmall];\n        const BYTE* matchBase = matchIndex < dictLimit ? dictBase : base;\n        const BYTE* match = matchBase + matchIndex;\n\n        const size_t hLong = ZSTD_hashPtr(ip, hBitsL, 8);\n        const U32 matchLongIndex = hashLong[hLong];\n        const BYTE* matchLongBase = matchLongIndex < dictLimit ? dictBase : base;\n        const BYTE* matchLong = matchLongBase + matchLongIndex;\n\n        const U32 current = (U32)(ip-base);\n        const U32 repIndex = current + 1 - offset_1;   /* offset_1 expected <= current +1 */\n        const BYTE* repBase = repIndex < dictLimit ? dictBase : base;\n        const BYTE* repMatch = repBase + repIndex;\n        size_t mLength;\n        hashSmall[hSmall] = hashLong[hLong] = current;   /* update hash table */\n\n        if ( (((U32)((dictLimit-1) - repIndex) >= 3) /* intentional underflow */ & (repIndex > lowestIndex))\n           && (MEM_read32(repMatch) == MEM_read32(ip+1)) ) {\n            const BYTE* repMatchEnd = repIndex < dictLimit ? dictEnd : iend;\n            mLength = ZSTD_count_2segments(ip+1+4, repMatch+4, iend, repMatchEnd, lowPrefixPtr) + 4;\n            ip++;\n            ZSTD_storeSeq(seqStorePtr, ip-anchor, anchor, 0, mLength-MINMATCH);\n        } else {\n            if ((matchLongIndex > lowestIndex) && (MEM_read64(matchLong) == MEM_read64(ip))) {\n                const BYTE* matchEnd = matchLongIndex < dictLimit ? dictEnd : iend;\n                const BYTE* lowMatchPtr = matchLongIndex < dictLimit ? dictStart : lowPrefixPtr;\n                U32 offset;\n                mLength = ZSTD_count_2segments(ip+8, matchLong+8, iend, matchEnd, lowPrefixPtr) + 8;\n                offset = current - matchLongIndex;\n                while (((ip>anchor) & (matchLong>lowMatchPtr)) && (ip[-1] == matchLong[-1])) { ip--; matchLong--; mLength++; }   /* catch up */\n                offset_2 = offset_1;\n                offset_1 = offset;\n                ZSTD_storeSeq(seqStorePtr, ip-anchor, anchor, offset + ZSTD_REP_MOVE, mLength-MINMATCH);\n\n            } else if ((matchIndex > lowestIndex) && (MEM_read32(match) == MEM_read32(ip))) {\n                size_t const h3 = ZSTD_hashPtr(ip+1, hBitsL, 8);\n                U32 const matchIndex3 = hashLong[h3];\n                const BYTE* const match3Base = matchIndex3 < dictLimit ? dictBase : base;\n                const BYTE* match3 = match3Base + matchIndex3;\n                U32 offset;\n                hashLong[h3] = current + 1;\n                if ( (matchIndex3 > lowestIndex) && (MEM_read64(match3) == MEM_read64(ip+1)) ) {\n                    const BYTE* matchEnd = matchIndex3 < dictLimit ? dictEnd : iend;\n                    const BYTE* lowMatchPtr = matchIndex3 < dictLimit ? dictStart : lowPrefixPtr;\n                    mLength = ZSTD_count_2segments(ip+9, match3+8, iend, matchEnd, lowPrefixPtr) + 8;\n                    ip++;\n                    offset = current+1 - matchIndex3;\n                    while (((ip>anchor) & (match3>lowMatchPtr)) && (ip[-1] == match3[-1])) { ip--; match3--; mLength++; } /* catch up */\n                } else {\n                    const BYTE* matchEnd = matchIndex < dictLimit ? dictEnd : iend;\n                    const BYTE* lowMatchPtr = matchIndex < dictLimit ? dictStart : lowPrefixPtr;\n                    mLength = ZSTD_count_2segments(ip+4, match+4, iend, matchEnd, lowPrefixPtr) + 4;\n                    offset = current - matchIndex;\n                    while (((ip>anchor) & (match>lowMatchPtr)) && (ip[-1] == match[-1])) { ip--; match--; mLength++; }   /* catch up */\n                }\n                offset_2 = offset_1;\n                offset_1 = offset;\n                ZSTD_storeSeq(seqStorePtr, ip-anchor, anchor, offset + ZSTD_REP_MOVE, mLength-MINMATCH);\n\n            } else {\n                ip += ((ip-anchor) >> g_searchStrength) + 1;\n                continue;\n        }   }\n\n        /* found a match : store it */\n        ip += mLength;\n        anchor = ip;\n\n        if (ip <= ilimit) {\n            /* Fill Table */\n            hashSmall[ZSTD_hashPtr(base+current+2, hBitsS, mls)] = current+2;\n            hashLong[ZSTD_hashPtr(base+current+2, hBitsL, 8)] = current+2;\n            hashSmall[ZSTD_hashPtr(ip-2, hBitsS, mls)] = (U32)(ip-2-base);\n            hashLong[ZSTD_hashPtr(ip-2, hBitsL, 8)] = (U32)(ip-2-base);\n            /* check immediate repcode */\n            while (ip <= ilimit) {\n                U32 const current2 = (U32)(ip-base);\n                U32 const repIndex2 = current2 - offset_2;\n                const BYTE* repMatch2 = repIndex2 < dictLimit ? dictBase + repIndex2 : base + repIndex2;\n                if ( (((U32)((dictLimit-1) - repIndex2) >= 3) & (repIndex2 > lowestIndex))  /* intentional overflow */\n                   && (MEM_read32(repMatch2) == MEM_read32(ip)) ) {\n                    const BYTE* const repEnd2 = repIndex2 < dictLimit ? dictEnd : iend;\n                    size_t const repLength2 = ZSTD_count_2segments(ip+4, repMatch2+4, iend, repEnd2, lowPrefixPtr) + 4;\n                    U32 tmpOffset = offset_2; offset_2 = offset_1; offset_1 = tmpOffset;   /* swap offset_2 <=> offset_1 */\n                    ZSTD_storeSeq(seqStorePtr, 0, anchor, 0, repLength2-MINMATCH);\n                    hashSmall[ZSTD_hashPtr(ip, hBitsS, mls)] = current2;\n                    hashLong[ZSTD_hashPtr(ip, hBitsL, 8)] = current2;\n                    ip += repLength2;\n                    anchor = ip;\n                    continue;\n                }\n                break;\n    }   }   }\n\n    /* save reps for next block */\n    seqStorePtr->repToConfirm[0] = offset_1; seqStorePtr->repToConfirm[1] = offset_2;\n\n    /* Return the last literals size */\n    return iend - anchor;\n}\n\n\nstatic size_t ZSTD_compressBlock_doubleFast_extDict(ZSTD_CCtx* ctx,\n                         const void* src, size_t srcSize)\n{\n    U32 const mls = ctx->appliedParams.cParams.searchLength;\n    switch(mls)\n    {\n    default: /* includes case 3 */\n    case 4 :\n        return ZSTD_compressBlock_doubleFast_extDict_generic(ctx, src, srcSize, 4);\n    case 5 :\n        return ZSTD_compressBlock_doubleFast_extDict_generic(ctx, src, srcSize, 5);\n    case 6 :\n        return ZSTD_compressBlock_doubleFast_extDict_generic(ctx, src, srcSize, 6);\n    case 7 :\n        return ZSTD_compressBlock_doubleFast_extDict_generic(ctx, src, srcSize, 7);\n    }\n}\n\n\n/*-*************************************\n*  Binary Tree search\n***************************************/\n/** ZSTD_insertBt1() : add one or multiple positions to tree.\n*   ip : assumed <= iend-8 .\n*   @return : nb of positions added */\nstatic U32 ZSTD_insertBt1(ZSTD_CCtx* zc, const BYTE* const ip, const U32 mls, const BYTE* const iend, U32 nbCompares,\n                          U32 extDict)\n{\n    U32*   const hashTable = zc->hashTable;\n    U32    const hashLog = zc->appliedParams.cParams.hashLog;\n    size_t const h  = ZSTD_hashPtr(ip, hashLog, mls);\n    U32*   const bt = zc->chainTable;\n    U32    const btLog  = zc->appliedParams.cParams.chainLog - 1;\n    U32    const btMask = (1 << btLog) - 1;\n    U32 matchIndex = hashTable[h];\n    size_t commonLengthSmaller=0, commonLengthLarger=0;\n    const BYTE* const base = zc->base;\n    const BYTE* const dictBase = zc->dictBase;\n    const U32 dictLimit = zc->dictLimit;\n    const BYTE* const dictEnd = dictBase + dictLimit;\n    const BYTE* const prefixStart = base + dictLimit;\n    const BYTE* match;\n    const U32 current = (U32)(ip-base);\n    const U32 btLow = btMask >= current ? 0 : current - btMask;\n    U32* smallerPtr = bt + 2*(current&btMask);\n    U32* largerPtr  = smallerPtr + 1;\n    U32 dummy32;   /* to be nullified at the end */\n    U32 const windowLow = zc->lowLimit;\n    U32 matchEndIdx = current+8;\n    size_t bestLength = 8;\n#ifdef ZSTD_C_PREDICT\n    U32 predictedSmall = *(bt + 2*((current-1)&btMask) + 0);\n    U32 predictedLarge = *(bt + 2*((current-1)&btMask) + 1);\n    predictedSmall += (predictedSmall>0);\n    predictedLarge += (predictedLarge>0);\n#endif /* ZSTD_C_PREDICT */\n\n    hashTable[h] = current;   /* Update Hash Table */\n\n    while (nbCompares-- && (matchIndex > windowLow)) {\n        U32* const nextPtr = bt + 2*(matchIndex & btMask);\n        size_t matchLength = MIN(commonLengthSmaller, commonLengthLarger);   /* guaranteed minimum nb of common bytes */\n\n#ifdef ZSTD_C_PREDICT   /* note : can create issues when hlog small <= 11 */\n        const U32* predictPtr = bt + 2*((matchIndex-1) & btMask);   /* written this way, as bt is a roll buffer */\n        if (matchIndex == predictedSmall) {\n            /* no need to check length, result known */\n            *smallerPtr = matchIndex;\n            if (matchIndex <= btLow) { smallerPtr=&dummy32; break; }   /* beyond tree size, stop the search */\n            smallerPtr = nextPtr+1;               /* new \"smaller\" => larger of match */\n            matchIndex = nextPtr[1];              /* new matchIndex larger than previous (closer to current) */\n            predictedSmall = predictPtr[1] + (predictPtr[1]>0);\n            continue;\n        }\n        if (matchIndex == predictedLarge) {\n            *largerPtr = matchIndex;\n            if (matchIndex <= btLow) { largerPtr=&dummy32; break; }   /* beyond tree size, stop the search */\n            largerPtr = nextPtr;\n            matchIndex = nextPtr[0];\n            predictedLarge = predictPtr[0] + (predictPtr[0]>0);\n            continue;\n        }\n#endif\n        if ((!extDict) || (matchIndex+matchLength >= dictLimit)) {\n            match = base + matchIndex;\n            if (match[matchLength] == ip[matchLength])\n                matchLength += ZSTD_count(ip+matchLength+1, match+matchLength+1, iend) +1;\n        } else {\n            match = dictBase + matchIndex;\n            matchLength += ZSTD_count_2segments(ip+matchLength, match+matchLength, iend, dictEnd, prefixStart);\n            if (matchIndex+matchLength >= dictLimit)\n                match = base + matchIndex;   /* to prepare for next usage of match[matchLength] */\n        }\n\n        if (matchLength > bestLength) {\n            bestLength = matchLength;\n            if (matchLength > matchEndIdx - matchIndex)\n                matchEndIdx = matchIndex + (U32)matchLength;\n        }\n\n        if (ip+matchLength == iend)   /* equal : no way to know if inf or sup */\n            break;   /* drop , to guarantee consistency ; miss a bit of compression, but other solutions can corrupt the tree */\n\n        if (match[matchLength] < ip[matchLength]) {  /* necessarily within correct buffer */\n            /* match is smaller than current */\n            *smallerPtr = matchIndex;             /* update smaller idx */\n            commonLengthSmaller = matchLength;    /* all smaller will now have at least this guaranteed common length */\n            if (matchIndex <= btLow) { smallerPtr=&dummy32; break; }   /* beyond tree size, stop the search */\n            smallerPtr = nextPtr+1;               /* new \"smaller\" => larger of match */\n            matchIndex = nextPtr[1];              /* new matchIndex larger than previous (closer to current) */\n        } else {\n            /* match is larger than current */\n            *largerPtr = matchIndex;\n            commonLengthLarger = matchLength;\n            if (matchIndex <= btLow) { largerPtr=&dummy32; break; }   /* beyond tree size, stop the search */\n            largerPtr = nextPtr;\n            matchIndex = nextPtr[0];\n    }   }\n\n    *smallerPtr = *largerPtr = 0;\n    if (bestLength > 384) return MIN(192, (U32)(bestLength - 384));   /* speed optimization */\n    if (matchEndIdx > current + 8) return matchEndIdx - current - 8;\n    return 1;\n}\n\n\nstatic size_t ZSTD_insertBtAndFindBestMatch (\n                        ZSTD_CCtx* zc,\n                        const BYTE* const ip, const BYTE* const iend,\n                        size_t* offsetPtr,\n                        U32 nbCompares, const U32 mls,\n                        U32 extDict)\n{\n    U32*   const hashTable = zc->hashTable;\n    U32    const hashLog = zc->appliedParams.cParams.hashLog;\n    size_t const h  = ZSTD_hashPtr(ip, hashLog, mls);\n    U32*   const bt = zc->chainTable;\n    U32    const btLog  = zc->appliedParams.cParams.chainLog - 1;\n    U32    const btMask = (1 << btLog) - 1;\n    U32 matchIndex  = hashTable[h];\n    size_t commonLengthSmaller=0, commonLengthLarger=0;\n    const BYTE* const base = zc->base;\n    const BYTE* const dictBase = zc->dictBase;\n    const U32 dictLimit = zc->dictLimit;\n    const BYTE* const dictEnd = dictBase + dictLimit;\n    const BYTE* const prefixStart = base + dictLimit;\n    const U32 current = (U32)(ip-base);\n    const U32 btLow = btMask >= current ? 0 : current - btMask;\n    const U32 windowLow = zc->lowLimit;\n    U32* smallerPtr = bt + 2*(current&btMask);\n    U32* largerPtr  = bt + 2*(current&btMask) + 1;\n    U32 matchEndIdx = current+8;\n    U32 dummy32;   /* to be nullified at the end */\n    size_t bestLength = 0;\n\n    hashTable[h] = current;   /* Update Hash Table */\n\n    while (nbCompares-- && (matchIndex > windowLow)) {\n        U32* const nextPtr = bt + 2*(matchIndex & btMask);\n        size_t matchLength = MIN(commonLengthSmaller, commonLengthLarger);   /* guaranteed minimum nb of common bytes */\n        const BYTE* match;\n\n        if ((!extDict) || (matchIndex+matchLength >= dictLimit)) {\n            match = base + matchIndex;\n            if (match[matchLength] == ip[matchLength])\n                matchLength += ZSTD_count(ip+matchLength+1, match+matchLength+1, iend) +1;\n        } else {\n            match = dictBase + matchIndex;\n            matchLength += ZSTD_count_2segments(ip+matchLength, match+matchLength, iend, dictEnd, prefixStart);\n            if (matchIndex+matchLength >= dictLimit)\n                match = base + matchIndex;   /* to prepare for next usage of match[matchLength] */\n        }\n\n        if (matchLength > bestLength) {\n            if (matchLength > matchEndIdx - matchIndex)\n                matchEndIdx = matchIndex + (U32)matchLength;\n            if ( (4*(int)(matchLength-bestLength)) > (int)(ZSTD_highbit32(current-matchIndex+1) - ZSTD_highbit32((U32)offsetPtr[0]+1)) )\n                bestLength = matchLength, *offsetPtr = ZSTD_REP_MOVE + current - matchIndex;\n            if (ip+matchLength == iend)   /* equal : no way to know if inf or sup */\n                break;   /* drop, to guarantee consistency (miss a little bit of compression) */\n        }\n\n        if (match[matchLength] < ip[matchLength]) {\n            /* match is smaller than current */\n            *smallerPtr = matchIndex;             /* update smaller idx */\n            commonLengthSmaller = matchLength;    /* all smaller will now have at least this guaranteed common length */\n            if (matchIndex <= btLow) { smallerPtr=&dummy32; break; }   /* beyond tree size, stop the search */\n            smallerPtr = nextPtr+1;               /* new \"smaller\" => larger of match */\n            matchIndex = nextPtr[1];              /* new matchIndex larger than previous (closer to current) */\n        } else {\n            /* match is larger than current */\n            *largerPtr = matchIndex;\n            commonLengthLarger = matchLength;\n            if (matchIndex <= btLow) { largerPtr=&dummy32; break; }   /* beyond tree size, stop the search */\n            largerPtr = nextPtr;\n            matchIndex = nextPtr[0];\n    }   }\n\n    *smallerPtr = *largerPtr = 0;\n\n    zc->nextToUpdate = (matchEndIdx > current + 8) ? matchEndIdx - 8 : current+1;\n    return bestLength;\n}\n\n\nstatic void ZSTD_updateTree(ZSTD_CCtx* zc, const BYTE* const ip, const BYTE* const iend, const U32 nbCompares, const U32 mls)\n{\n    const BYTE* const base = zc->base;\n    const U32 target = (U32)(ip - base);\n    U32 idx = zc->nextToUpdate;\n\n    while(idx < target)\n        idx += ZSTD_insertBt1(zc, base+idx, mls, iend, nbCompares, 0);\n}\n\n/** ZSTD_BtFindBestMatch() : Tree updater, providing best match */\nstatic size_t ZSTD_BtFindBestMatch (\n                        ZSTD_CCtx* zc,\n                        const BYTE* const ip, const BYTE* const iLimit,\n                        size_t* offsetPtr,\n                        const U32 maxNbAttempts, const U32 mls)\n{\n    if (ip < zc->base + zc->nextToUpdate) return 0;   /* skipped area */\n    ZSTD_updateTree(zc, ip, iLimit, maxNbAttempts, mls);\n    return ZSTD_insertBtAndFindBestMatch(zc, ip, iLimit, offsetPtr, maxNbAttempts, mls, 0);\n}\n\n\nstatic size_t ZSTD_BtFindBestMatch_selectMLS (\n                        ZSTD_CCtx* zc,   /* Index table will be updated */\n                        const BYTE* ip, const BYTE* const iLimit,\n                        size_t* offsetPtr,\n                        const U32 maxNbAttempts, const U32 matchLengthSearch)\n{\n    switch(matchLengthSearch)\n    {\n    default : /* includes case 3 */\n    case 4 : return ZSTD_BtFindBestMatch(zc, ip, iLimit, offsetPtr, maxNbAttempts, 4);\n    case 5 : return ZSTD_BtFindBestMatch(zc, ip, iLimit, offsetPtr, maxNbAttempts, 5);\n    case 7 :\n    case 6 : return ZSTD_BtFindBestMatch(zc, ip, iLimit, offsetPtr, maxNbAttempts, 6);\n    }\n}\n\n\nstatic void ZSTD_updateTree_extDict(ZSTD_CCtx* zc, const BYTE* const ip, const BYTE* const iend, const U32 nbCompares, const U32 mls)\n{\n    const BYTE* const base = zc->base;\n    const U32 target = (U32)(ip - base);\n    U32 idx = zc->nextToUpdate;\n\n    while (idx < target) idx += ZSTD_insertBt1(zc, base+idx, mls, iend, nbCompares, 1);\n}\n\n\n/** Tree updater, providing best match */\nstatic size_t ZSTD_BtFindBestMatch_extDict (\n                        ZSTD_CCtx* zc,\n                        const BYTE* const ip, const BYTE* const iLimit,\n                        size_t* offsetPtr,\n                        const U32 maxNbAttempts, const U32 mls)\n{\n    if (ip < zc->base + zc->nextToUpdate) return 0;   /* skipped area */\n    ZSTD_updateTree_extDict(zc, ip, iLimit, maxNbAttempts, mls);\n    return ZSTD_insertBtAndFindBestMatch(zc, ip, iLimit, offsetPtr, maxNbAttempts, mls, 1);\n}\n\n\nstatic size_t ZSTD_BtFindBestMatch_selectMLS_extDict (\n                        ZSTD_CCtx* zc,   /* Index table will be updated */\n                        const BYTE* ip, const BYTE* const iLimit,\n                        size_t* offsetPtr,\n                        const U32 maxNbAttempts, const U32 matchLengthSearch)\n{\n    switch(matchLengthSearch)\n    {\n    default : /* includes case 3 */\n    case 4 : return ZSTD_BtFindBestMatch_extDict(zc, ip, iLimit, offsetPtr, maxNbAttempts, 4);\n    case 5 : return ZSTD_BtFindBestMatch_extDict(zc, ip, iLimit, offsetPtr, maxNbAttempts, 5);\n    case 7 :\n    case 6 : return ZSTD_BtFindBestMatch_extDict(zc, ip, iLimit, offsetPtr, maxNbAttempts, 6);\n    }\n}\n\n\n\n/* *********************************\n*  Hash Chain\n***********************************/\n#define NEXT_IN_CHAIN(d, mask)   chainTable[(d) & mask]\n\n/* Update chains up to ip (excluded)\n   Assumption : always within prefix (i.e. not within extDict) */\nFORCE_INLINE_TEMPLATE\nU32 ZSTD_insertAndFindFirstIndex (ZSTD_CCtx* zc, const BYTE* ip, U32 mls)\n{\n    U32* const hashTable  = zc->hashTable;\n    const U32 hashLog = zc->appliedParams.cParams.hashLog;\n    U32* const chainTable = zc->chainTable;\n    const U32 chainMask = (1 << zc->appliedParams.cParams.chainLog) - 1;\n    const BYTE* const base = zc->base;\n    const U32 target = (U32)(ip - base);\n    U32 idx = zc->nextToUpdate;\n\n    while(idx < target) { /* catch up */\n        size_t const h = ZSTD_hashPtr(base+idx, hashLog, mls);\n        NEXT_IN_CHAIN(idx, chainMask) = hashTable[h];\n        hashTable[h] = idx;\n        idx++;\n    }\n\n    zc->nextToUpdate = target;\n    return hashTable[ZSTD_hashPtr(ip, hashLog, mls)];\n}\n\n\n/* inlining is important to hardwire a hot branch (template emulation) */\nFORCE_INLINE_TEMPLATE\nsize_t ZSTD_HcFindBestMatch_generic (\n                        ZSTD_CCtx* zc,   /* Index table will be updated */\n                        const BYTE* const ip, const BYTE* const iLimit,\n                        size_t* offsetPtr,\n                        const U32 maxNbAttempts, const U32 mls, const U32 extDict)\n{\n    U32* const chainTable = zc->chainTable;\n    const U32 chainSize = (1 << zc->appliedParams.cParams.chainLog);\n    const U32 chainMask = chainSize-1;\n    const BYTE* const base = zc->base;\n    const BYTE* const dictBase = zc->dictBase;\n    const U32 dictLimit = zc->dictLimit;\n    const BYTE* const prefixStart = base + dictLimit;\n    const BYTE* const dictEnd = dictBase + dictLimit;\n    const U32 lowLimit = zc->lowLimit;\n    const U32 current = (U32)(ip-base);\n    const U32 minChain = current > chainSize ? current - chainSize : 0;\n    int nbAttempts=maxNbAttempts;\n    size_t ml=4-1;\n\n    /* HC4 match finder */\n    U32 matchIndex = ZSTD_insertAndFindFirstIndex (zc, ip, mls);\n\n    for ( ; (matchIndex>lowLimit) & (nbAttempts>0) ; nbAttempts--) {\n        const BYTE* match;\n        size_t currentMl=0;\n        if ((!extDict) || matchIndex >= dictLimit) {\n            match = base + matchIndex;\n            if (match[ml] == ip[ml])   /* potentially better */\n                currentMl = ZSTD_count(ip, match, iLimit);\n        } else {\n            match = dictBase + matchIndex;\n            if (MEM_read32(match) == MEM_read32(ip))   /* assumption : matchIndex <= dictLimit-4 (by table construction) */\n                currentMl = ZSTD_count_2segments(ip+4, match+4, iLimit, dictEnd, prefixStart) + 4;\n        }\n\n        /* save best solution */\n        if (currentMl > ml) {\n            ml = currentMl;\n            *offsetPtr = current - matchIndex + ZSTD_REP_MOVE;\n            if (ip+currentMl == iLimit) break; /* best possible, avoids read overflow on next attempt */\n        }\n\n        if (matchIndex <= minChain) break;\n        matchIndex = NEXT_IN_CHAIN(matchIndex, chainMask);\n    }\n\n    return ml;\n}\n\n\nFORCE_INLINE_TEMPLATE size_t ZSTD_HcFindBestMatch_selectMLS (\n                        ZSTD_CCtx* zc,\n                        const BYTE* ip, const BYTE* const iLimit,\n                        size_t* offsetPtr,\n                        const U32 maxNbAttempts, const U32 matchLengthSearch)\n{\n    switch(matchLengthSearch)\n    {\n    default : /* includes case 3 */\n    case 4 : return ZSTD_HcFindBestMatch_generic(zc, ip, iLimit, offsetPtr, maxNbAttempts, 4, 0);\n    case 5 : return ZSTD_HcFindBestMatch_generic(zc, ip, iLimit, offsetPtr, maxNbAttempts, 5, 0);\n    case 7 :\n    case 6 : return ZSTD_HcFindBestMatch_generic(zc, ip, iLimit, offsetPtr, maxNbAttempts, 6, 0);\n    }\n}\n\n\nFORCE_INLINE_TEMPLATE size_t ZSTD_HcFindBestMatch_extDict_selectMLS (\n                        ZSTD_CCtx* zc,\n                        const BYTE* ip, const BYTE* const iLimit,\n                        size_t* offsetPtr,\n                        const U32 maxNbAttempts, const U32 matchLengthSearch)\n{\n    switch(matchLengthSearch)\n    {\n    default : /* includes case 3 */\n    case 4 : return ZSTD_HcFindBestMatch_generic(zc, ip, iLimit, offsetPtr, maxNbAttempts, 4, 1);\n    case 5 : return ZSTD_HcFindBestMatch_generic(zc, ip, iLimit, offsetPtr, maxNbAttempts, 5, 1);\n    case 7 :\n    case 6 : return ZSTD_HcFindBestMatch_generic(zc, ip, iLimit, offsetPtr, maxNbAttempts, 6, 1);\n    }\n}\n\n\n/* *******************************\n*  Common parser - lazy strategy\n*********************************/\nFORCE_INLINE_TEMPLATE\nsize_t ZSTD_compressBlock_lazy_generic(ZSTD_CCtx* ctx,\n                                       const void* src, size_t srcSize,\n                                       const U32 searchMethod, const U32 depth)\n{\n    seqStore_t* seqStorePtr = &(ctx->seqStore);\n    const BYTE* const istart = (const BYTE*)src;\n    const BYTE* ip = istart;\n    const BYTE* anchor = istart;\n    const BYTE* const iend = istart + srcSize;\n    const BYTE* const ilimit = iend - 8;\n    const BYTE* const base = ctx->base + ctx->dictLimit;\n\n    U32 const maxSearches = 1 << ctx->appliedParams.cParams.searchLog;\n    U32 const mls = ctx->appliedParams.cParams.searchLength;\n\n    typedef size_t (*searchMax_f)(ZSTD_CCtx* zc, const BYTE* ip, const BYTE* iLimit,\n                        size_t* offsetPtr,\n                        U32 maxNbAttempts, U32 matchLengthSearch);\n    searchMax_f const searchMax = searchMethod ? ZSTD_BtFindBestMatch_selectMLS : ZSTD_HcFindBestMatch_selectMLS;\n    U32 offset_1 = seqStorePtr->rep[0], offset_2 = seqStorePtr->rep[1], savedOffset=0;\n\n    /* init */\n    ip += (ip==base);\n    ctx->nextToUpdate3 = ctx->nextToUpdate;\n    {   U32 const maxRep = (U32)(ip-base);\n        if (offset_2 > maxRep) savedOffset = offset_2, offset_2 = 0;\n        if (offset_1 > maxRep) savedOffset = offset_1, offset_1 = 0;\n    }\n\n    /* Match Loop */\n    while (ip < ilimit) {\n        size_t matchLength=0;\n        size_t offset=0;\n        const BYTE* start=ip+1;\n\n        /* check repCode */\n        if ((offset_1>0) & (MEM_read32(ip+1) == MEM_read32(ip+1 - offset_1))) {\n            /* repcode : we take it */\n            matchLength = ZSTD_count(ip+1+4, ip+1+4-offset_1, iend) + 4;\n            if (depth==0) goto _storeSequence;\n        }\n\n        /* first search (depth 0) */\n        {   size_t offsetFound = 99999999;\n            size_t const ml2 = searchMax(ctx, ip, iend, &offsetFound, maxSearches, mls);\n            if (ml2 > matchLength)\n                matchLength = ml2, start = ip, offset=offsetFound;\n        }\n\n        if (matchLength < 4) {\n            ip += ((ip-anchor) >> g_searchStrength) + 1;   /* jump faster over incompressible sections */\n            continue;\n        }\n\n        /* let's try to find a better solution */\n        if (depth>=1)\n        while (ip<ilimit) {\n            ip ++;\n            if ((offset) && ((offset_1>0) & (MEM_read32(ip) == MEM_read32(ip - offset_1)))) {\n                size_t const mlRep = ZSTD_count(ip+4, ip+4-offset_1, iend) + 4;\n                int const gain2 = (int)(mlRep * 3);\n                int const gain1 = (int)(matchLength*3 - ZSTD_highbit32((U32)offset+1) + 1);\n                if ((mlRep >= 4) && (gain2 > gain1))\n                    matchLength = mlRep, offset = 0, start = ip;\n            }\n            {   size_t offset2=99999999;\n                size_t const ml2 = searchMax(ctx, ip, iend, &offset2, maxSearches, mls);\n                int const gain2 = (int)(ml2*4 - ZSTD_highbit32((U32)offset2+1));   /* raw approx */\n                int const gain1 = (int)(matchLength*4 - ZSTD_highbit32((U32)offset+1) + 4);\n                if ((ml2 >= 4) && (gain2 > gain1)) {\n                    matchLength = ml2, offset = offset2, start = ip;\n                    continue;   /* search a better one */\n            }   }\n\n            /* let's find an even better one */\n            if ((depth==2) && (ip<ilimit)) {\n                ip ++;\n                if ((offset) && ((offset_1>0) & (MEM_read32(ip) == MEM_read32(ip - offset_1)))) {\n                    size_t const ml2 = ZSTD_count(ip+4, ip+4-offset_1, iend) + 4;\n                    int const gain2 = (int)(ml2 * 4);\n                    int const gain1 = (int)(matchLength*4 - ZSTD_highbit32((U32)offset+1) + 1);\n                    if ((ml2 >= 4) && (gain2 > gain1))\n                        matchLength = ml2, offset = 0, start = ip;\n                }\n                {   size_t offset2=99999999;\n                    size_t const ml2 = searchMax(ctx, ip, iend, &offset2, maxSearches, mls);\n                    int const gain2 = (int)(ml2*4 - ZSTD_highbit32((U32)offset2+1));   /* raw approx */\n                    int const gain1 = (int)(matchLength*4 - ZSTD_highbit32((U32)offset+1) + 7);\n                    if ((ml2 >= 4) && (gain2 > gain1)) {\n                        matchLength = ml2, offset = offset2, start = ip;\n                        continue;\n            }   }   }\n            break;  /* nothing found : store previous solution */\n        }\n\n        /* NOTE:\n         * start[-offset+ZSTD_REP_MOVE-1] is undefined behavior.\n         * (-offset+ZSTD_REP_MOVE-1) is unsigned, and is added to start, which\n         * overflows the pointer, which is undefined behavior.\n         */\n        /* catch up */\n        if (offset) {\n            while ( (start > anchor)\n                 && (start > base+offset-ZSTD_REP_MOVE)\n                 && (start[-1] == (start-offset+ZSTD_REP_MOVE)[-1]) )  /* only search for offset within prefix */\n                { start--; matchLength++; }\n            offset_2 = offset_1; offset_1 = (U32)(offset - ZSTD_REP_MOVE);\n        }\n        /* store sequence */\n_storeSequence:\n        {   size_t const litLength = start - anchor;\n            ZSTD_storeSeq(seqStorePtr, litLength, anchor, (U32)offset, matchLength-MINMATCH);\n            anchor = ip = start + matchLength;\n        }\n\n        /* check immediate repcode */\n        while ( (ip <= ilimit)\n             && ((offset_2>0)\n             & (MEM_read32(ip) == MEM_read32(ip - offset_2)) )) {\n            /* store sequence */\n            matchLength = ZSTD_count(ip+4, ip+4-offset_2, iend) + 4;\n            offset = offset_2; offset_2 = offset_1; offset_1 = (U32)offset; /* swap repcodes */\n            ZSTD_storeSeq(seqStorePtr, 0, anchor, 0, matchLength-MINMATCH);\n            ip += matchLength;\n            anchor = ip;\n            continue;   /* faster when present ... (?) */\n    }   }\n\n    /* Save reps for next block */\n    seqStorePtr->repToConfirm[0] = offset_1 ? offset_1 : savedOffset;\n    seqStorePtr->repToConfirm[1] = offset_2 ? offset_2 : savedOffset;\n\n    /* Return the last literals size */\n    return iend - anchor;\n}\n\n\nstatic size_t ZSTD_compressBlock_btlazy2(ZSTD_CCtx* ctx, const void* src,\n                                         size_t srcSize)\n{\n    return ZSTD_compressBlock_lazy_generic(ctx, src, srcSize, 1, 2);\n}\n\nstatic size_t ZSTD_compressBlock_lazy2(ZSTD_CCtx* ctx, const void* src,\n                                       size_t srcSize)\n{\n    return ZSTD_compressBlock_lazy_generic(ctx, src, srcSize, 0, 2);\n}\n\nstatic size_t ZSTD_compressBlock_lazy(ZSTD_CCtx* ctx, const void* src,\n                                      size_t srcSize)\n{\n    return ZSTD_compressBlock_lazy_generic(ctx, src, srcSize, 0, 1);\n}\n\nstatic size_t ZSTD_compressBlock_greedy(ZSTD_CCtx* ctx, const void* src,\n                                        size_t srcSize)\n{\n    return ZSTD_compressBlock_lazy_generic(ctx, src, srcSize, 0, 0);\n}\n\n\nFORCE_INLINE_TEMPLATE\nsize_t ZSTD_compressBlock_lazy_extDict_generic(ZSTD_CCtx* ctx,\n                                     const void* src, size_t srcSize,\n                                     const U32 searchMethod, const U32 depth)\n{\n    seqStore_t* seqStorePtr = &(ctx->seqStore);\n    const BYTE* const istart = (const BYTE*)src;\n    const BYTE* ip = istart;\n    const BYTE* anchor = istart;\n    const BYTE* const iend = istart + srcSize;\n    const BYTE* const ilimit = iend - 8;\n    const BYTE* const base = ctx->base;\n    const U32 dictLimit = ctx->dictLimit;\n    const U32 lowestIndex = ctx->lowLimit;\n    const BYTE* const prefixStart = base + dictLimit;\n    const BYTE* const dictBase = ctx->dictBase;\n    const BYTE* const dictEnd  = dictBase + dictLimit;\n    const BYTE* const dictStart  = dictBase + ctx->lowLimit;\n\n    const U32 maxSearches = 1 << ctx->appliedParams.cParams.searchLog;\n    const U32 mls = ctx->appliedParams.cParams.searchLength;\n\n    typedef size_t (*searchMax_f)(ZSTD_CCtx* zc, const BYTE* ip, const BYTE* iLimit,\n                        size_t* offsetPtr,\n                        U32 maxNbAttempts, U32 matchLengthSearch);\n    searchMax_f searchMax = searchMethod ? ZSTD_BtFindBestMatch_selectMLS_extDict : ZSTD_HcFindBestMatch_extDict_selectMLS;\n\n    U32 offset_1 = seqStorePtr->rep[0], offset_2 = seqStorePtr->rep[1];\n\n    /* init */\n    ctx->nextToUpdate3 = ctx->nextToUpdate;\n    ip += (ip == prefixStart);\n\n    /* Match Loop */\n    while (ip < ilimit) {\n        size_t matchLength=0;\n        size_t offset=0;\n        const BYTE* start=ip+1;\n        U32 current = (U32)(ip-base);\n\n        /* check repCode */\n        {   const U32 repIndex = (U32)(current+1 - offset_1);\n            const BYTE* const repBase = repIndex < dictLimit ? dictBase : base;\n            const BYTE* const repMatch = repBase + repIndex;\n            if (((U32)((dictLimit-1) - repIndex) >= 3) & (repIndex > lowestIndex))   /* intentional overflow */\n            if (MEM_read32(ip+1) == MEM_read32(repMatch)) {\n                /* repcode detected we should take it */\n                const BYTE* const repEnd = repIndex < dictLimit ? dictEnd : iend;\n                matchLength = ZSTD_count_2segments(ip+1+4, repMatch+4, iend, repEnd, prefixStart) + 4;\n                if (depth==0) goto _storeSequence;\n        }   }\n\n        /* first search (depth 0) */\n        {   size_t offsetFound = 99999999;\n            size_t const ml2 = searchMax(ctx, ip, iend, &offsetFound, maxSearches, mls);\n            if (ml2 > matchLength)\n                matchLength = ml2, start = ip, offset=offsetFound;\n        }\n\n         if (matchLength < 4) {\n            ip += ((ip-anchor) >> g_searchStrength) + 1;   /* jump faster over incompressible sections */\n            continue;\n        }\n\n        /* let's try to find a better solution */\n        if (depth>=1)\n        while (ip<ilimit) {\n            ip++;\n            current++;\n            /* check repCode */\n            if (offset) {\n                const U32 repIndex = (U32)(current - offset_1);\n                const BYTE* const repBase = repIndex < dictLimit ? dictBase : base;\n                const BYTE* const repMatch = repBase + repIndex;\n                if (((U32)((dictLimit-1) - repIndex) >= 3) & (repIndex > lowestIndex))  /* intentional overflow */\n                if (MEM_read32(ip) == MEM_read32(repMatch)) {\n                    /* repcode detected */\n                    const BYTE* const repEnd = repIndex < dictLimit ? dictEnd : iend;\n                    size_t const repLength = ZSTD_count_2segments(ip+4, repMatch+4, iend, repEnd, prefixStart) + 4;\n                    int const gain2 = (int)(repLength * 3);\n                    int const gain1 = (int)(matchLength*3 - ZSTD_highbit32((U32)offset+1) + 1);\n                    if ((repLength >= 4) && (gain2 > gain1))\n                        matchLength = repLength, offset = 0, start = ip;\n            }   }\n\n            /* search match, depth 1 */\n            {   size_t offset2=99999999;\n                size_t const ml2 = searchMax(ctx, ip, iend, &offset2, maxSearches, mls);\n                int const gain2 = (int)(ml2*4 - ZSTD_highbit32((U32)offset2+1));   /* raw approx */\n                int const gain1 = (int)(matchLength*4 - ZSTD_highbit32((U32)offset+1) + 4);\n                if ((ml2 >= 4) && (gain2 > gain1)) {\n                    matchLength = ml2, offset = offset2, start = ip;\n                    continue;   /* search a better one */\n            }   }\n\n            /* let's find an even better one */\n            if ((depth==2) && (ip<ilimit)) {\n                ip ++;\n                current++;\n                /* check repCode */\n                if (offset) {\n                    const U32 repIndex = (U32)(current - offset_1);\n                    const BYTE* const repBase = repIndex < dictLimit ? dictBase : base;\n                    const BYTE* const repMatch = repBase + repIndex;\n                    if (((U32)((dictLimit-1) - repIndex) >= 3) & (repIndex > lowestIndex))  /* intentional overflow */\n                    if (MEM_read32(ip) == MEM_read32(repMatch)) {\n                        /* repcode detected */\n                        const BYTE* const repEnd = repIndex < dictLimit ? dictEnd : iend;\n                        size_t const repLength = ZSTD_count_2segments(ip+4, repMatch+4, iend, repEnd, prefixStart) + 4;\n                        int const gain2 = (int)(repLength * 4);\n                        int const gain1 = (int)(matchLength*4 - ZSTD_highbit32((U32)offset+1) + 1);\n                        if ((repLength >= 4) && (gain2 > gain1))\n                            matchLength = repLength, offset = 0, start = ip;\n                }   }\n\n                /* search match, depth 2 */\n                {   size_t offset2=99999999;\n                    size_t const ml2 = searchMax(ctx, ip, iend, &offset2, maxSearches, mls);\n                    int const gain2 = (int)(ml2*4 - ZSTD_highbit32((U32)offset2+1));   /* raw approx */\n                    int const gain1 = (int)(matchLength*4 - ZSTD_highbit32((U32)offset+1) + 7);\n                    if ((ml2 >= 4) && (gain2 > gain1)) {\n                        matchLength = ml2, offset = offset2, start = ip;\n                        continue;\n            }   }   }\n            break;  /* nothing found : store previous solution */\n        }\n\n        /* catch up */\n        if (offset) {\n            U32 const matchIndex = (U32)((start-base) - (offset - ZSTD_REP_MOVE));\n            const BYTE* match = (matchIndex < dictLimit) ? dictBase + matchIndex : base + matchIndex;\n            const BYTE* const mStart = (matchIndex < dictLimit) ? dictStart : prefixStart;\n            while ((start>anchor) && (match>mStart) && (start[-1] == match[-1])) { start--; match--; matchLength++; }  /* catch up */\n            offset_2 = offset_1; offset_1 = (U32)(offset - ZSTD_REP_MOVE);\n        }\n\n        /* store sequence */\n_storeSequence:\n        {   size_t const litLength = start - anchor;\n            ZSTD_storeSeq(seqStorePtr, litLength, anchor, (U32)offset, matchLength-MINMATCH);\n            anchor = ip = start + matchLength;\n        }\n\n        /* check immediate repcode */\n        while (ip <= ilimit) {\n            const U32 repIndex = (U32)((ip-base) - offset_2);\n            const BYTE* const repBase = repIndex < dictLimit ? dictBase : base;\n            const BYTE* const repMatch = repBase + repIndex;\n            if (((U32)((dictLimit-1) - repIndex) >= 3) & (repIndex > lowestIndex))  /* intentional overflow */\n            if (MEM_read32(ip) == MEM_read32(repMatch)) {\n                /* repcode detected we should take it */\n                const BYTE* const repEnd = repIndex < dictLimit ? dictEnd : iend;\n                matchLength = ZSTD_count_2segments(ip+4, repMatch+4, iend, repEnd, prefixStart) + 4;\n                offset = offset_2; offset_2 = offset_1; offset_1 = (U32)offset;   /* swap offset history */\n                ZSTD_storeSeq(seqStorePtr, 0, anchor, 0, matchLength-MINMATCH);\n                ip += matchLength;\n                anchor = ip;\n                continue;   /* faster when present ... (?) */\n            }\n            break;\n    }   }\n\n    /* Save reps for next block */\n    seqStorePtr->repToConfirm[0] = offset_1; seqStorePtr->repToConfirm[1] = offset_2;\n\n    /* Return the last literals size */\n    return iend - anchor;\n}\n\n\nsize_t ZSTD_compressBlock_greedy_extDict(ZSTD_CCtx* ctx, const void* src, size_t srcSize)\n{\n    return ZSTD_compressBlock_lazy_extDict_generic(ctx, src, srcSize, 0, 0);\n}\n\nstatic size_t ZSTD_compressBlock_lazy_extDict(ZSTD_CCtx* ctx, const void* src, size_t srcSize)\n{\n    return ZSTD_compressBlock_lazy_extDict_generic(ctx, src, srcSize, 0, 1);\n}\n\nstatic size_t ZSTD_compressBlock_lazy2_extDict(ZSTD_CCtx* ctx, const void* src, size_t srcSize)\n{\n    return ZSTD_compressBlock_lazy_extDict_generic(ctx, src, srcSize, 0, 2);\n}\n\nstatic size_t ZSTD_compressBlock_btlazy2_extDict(ZSTD_CCtx* ctx, const void* src, size_t srcSize)\n{\n    return ZSTD_compressBlock_lazy_extDict_generic(ctx, src, srcSize, 1, 2);\n}\n\n\n/* The optimal parser */\n#include \"zstd_opt.h\"\n\nstatic size_t ZSTD_compressBlock_btopt(ZSTD_CCtx* ctx, const void* src, size_t srcSize)\n{\n#ifdef ZSTD_OPT_H_91842398743\n    return ZSTD_compressBlock_opt_generic(ctx, src, srcSize, 0);\n#else\n    (void)ctx; (void)src; (void)srcSize;\n    return 0;\n#endif\n}\n\nstatic size_t ZSTD_compressBlock_btultra(ZSTD_CCtx* ctx, const void* src, size_t srcSize)\n{\n#ifdef ZSTD_OPT_H_91842398743\n    return ZSTD_compressBlock_opt_generic(ctx, src, srcSize, 1);\n#else\n    (void)ctx; (void)src; (void)srcSize;\n    return 0;\n#endif\n}\n\nstatic size_t ZSTD_compressBlock_btopt_extDict(ZSTD_CCtx* ctx, const void* src, size_t srcSize)\n{\n#ifdef ZSTD_OPT_H_91842398743\n    return ZSTD_compressBlock_opt_extDict_generic(ctx, src, srcSize, 0);\n#else\n    (void)ctx; (void)src; (void)srcSize;\n    return 0;\n#endif\n}\n\nstatic size_t ZSTD_compressBlock_btultra_extDict(ZSTD_CCtx* ctx, const void* src, size_t srcSize)\n{\n#ifdef ZSTD_OPT_H_91842398743\n    return ZSTD_compressBlock_opt_extDict_generic(ctx, src, srcSize, 1);\n#else\n    (void)ctx; (void)src; (void)srcSize;\n    return 0;\n#endif\n}\n",
        "b_contents": "",
        "base_contents": "/*! ZSTD_storeSeq() :\n    Store a sequence (literal length, literals, offset code and match length code) into seqStore_t.\n    `offsetCode` : distance to match, or 0 == repCode.\n    `matchCode` : matchLength - MINMATCH\n*/\nMEM_STATIC void ZSTD_storeSeq(seqStore_t* seqStorePtr, size_t litLength, const void* literals, U32 offsetCode, size_t matchCode)\n{\n#if defined(ZSTD_DEBUG) && (ZSTD_DEBUG >= 6)\n    static const BYTE* g_start = NULL;\n    U32 const pos = (U32)((const BYTE*)literals - g_start);\n    if (g_start==NULL) g_start = (const BYTE*)literals;\n    if ((pos > 0) && (pos < 1000000000))\n        DEBUGLOG(6, \"Cpos %6u :%5u literals & match %3u bytes at distance %6u\",\n               pos, (U32)litLength, (U32)matchCode+MINMATCH, (U32)offsetCode);\n#endif\n    /* copy Literals */\n    assert(seqStorePtr->lit + litLength <= seqStorePtr->litStart + 128 KB);\n    ZSTD_wildcopy(seqStorePtr->lit, literals, litLength);\n    seqStorePtr->lit += litLength;\n\n    /* literal Length */\n    if (litLength>0xFFFF) {\n        seqStorePtr->longLengthID = 1;\n        seqStorePtr->longLengthPos = (U32)(seqStorePtr->sequences - seqStorePtr->sequencesStart);\n    }\n    seqStorePtr->sequences[0].litLength = (U16)litLength;\n\n    /* match offset */\n    seqStorePtr->sequences[0].offset = offsetCode + 1;\n\n    /* match Length */\n    if (matchCode>0xFFFF) {\n        seqStorePtr->longLengthID = 2;\n        seqStorePtr->longLengthPos = (U32)(seqStorePtr->sequences - seqStorePtr->sequencesStart);\n    }\n    seqStorePtr->sequences[0].matchLength = (U16)matchCode;\n\n    seqStorePtr->sequences++;\n}\n\n\n/*-*************************************\n*  Match length counter\n***************************************/\nstatic unsigned ZSTD_NbCommonBytes (register size_t val)\n{\n    if (MEM_isLittleEndian()) {\n        if (MEM_64bits()) {\n#       if defined(_MSC_VER) && defined(_WIN64)\n            unsigned long r = 0;\n            _BitScanForward64( &r, (U64)val );\n            return (unsigned)(r>>3);\n#       elif defined(__GNUC__) && (__GNUC__ >= 3)\n            return (__builtin_ctzll((U64)val) >> 3);\n#       else\n            static const int DeBruijnBytePos[64] = { 0, 0, 0, 0, 0, 1, 1, 2,\n                                                     0, 3, 1, 3, 1, 4, 2, 7,\n                                                     0, 2, 3, 6, 1, 5, 3, 5,\n                                                     1, 3, 4, 4, 2, 5, 6, 7,\n                                                     7, 0, 1, 2, 3, 3, 4, 6,\n                                                     2, 6, 5, 5, 3, 4, 5, 6,\n                                                     7, 1, 2, 4, 6, 4, 4, 5,\n                                                     7, 2, 6, 5, 7, 6, 7, 7 };\n            return DeBruijnBytePos[((U64)((val & -(long long)val) * 0x0218A392CDABBD3FULL)) >> 58];\n#       endif\n        } else { /* 32 bits */\n#       if defined(_MSC_VER)\n            unsigned long r=0;\n            _BitScanForward( &r, (U32)val );\n            return (unsigned)(r>>3);\n#       elif defined(__GNUC__) && (__GNUC__ >= 3)\n            return (__builtin_ctz((U32)val) >> 3);\n#       else\n            static const int DeBruijnBytePos[32] = { 0, 0, 3, 0, 3, 1, 3, 0,\n                                                     3, 2, 2, 1, 3, 2, 0, 1,\n                                                     3, 3, 1, 2, 2, 2, 2, 0,\n                                                     3, 1, 2, 0, 1, 0, 1, 1 };\n            return DeBruijnBytePos[((U32)((val & -(S32)val) * 0x077CB531U)) >> 27];\n#       endif\n        }\n    } else {  /* Big Endian CPU */\n        if (MEM_64bits()) {\n#       if defined(_MSC_VER) && defined(_WIN64)\n            unsigned long r = 0;\n            _BitScanReverse64( &r, val );\n            return (unsigned)(r>>3);\n#       elif defined(__GNUC__) && (__GNUC__ >= 3)\n            return (__builtin_clzll(val) >> 3);\n#       else\n            unsigned r;\n            const unsigned n32 = sizeof(size_t)*4;   /* calculate this way due to compiler complaining in 32-bits mode */\n            if (!(val>>n32)) { r=4; } else { r=0; val>>=n32; }\n            if (!(val>>16)) { r+=2; val>>=8; } else { val>>=24; }\n            r += (!val);\n            return r;\n#       endif\n        } else { /* 32 bits */\n#       if defined(_MSC_VER)\n            unsigned long r = 0;\n            _BitScanReverse( &r, (unsigned long)val );\n            return (unsigned)(r>>3);\n#       elif defined(__GNUC__) && (__GNUC__ >= 3)\n            return (__builtin_clz((U32)val) >> 3);\n#       else\n            unsigned r;\n            if (!(val>>16)) { r=2; val>>=8; } else { r=0; val>>=24; }\n            r += (!val);\n            return r;\n#       endif\n    }   }\n}\n\n\nstatic size_t ZSTD_count(const BYTE* pIn, const BYTE* pMatch, const BYTE* const pInLimit)\n{\n    const BYTE* const pStart = pIn;\n    const BYTE* const pInLoopLimit = pInLimit - (sizeof(size_t)-1);\n\n    while (pIn < pInLoopLimit) {\n        size_t const diff = MEM_readST(pMatch) ^ MEM_readST(pIn);\n        if (!diff) { pIn+=sizeof(size_t); pMatch+=sizeof(size_t); continue; }\n        pIn += ZSTD_NbCommonBytes(diff);\n        return (size_t)(pIn - pStart);\n    }\n    if (MEM_64bits()) if ((pIn<(pInLimit-3)) && (MEM_read32(pMatch) == MEM_read32(pIn))) { pIn+=4; pMatch+=4; }\n    if ((pIn<(pInLimit-1)) && (MEM_read16(pMatch) == MEM_read16(pIn))) { pIn+=2; pMatch+=2; }\n    if ((pIn<pInLimit) && (*pMatch == *pIn)) pIn++;\n    return (size_t)(pIn - pStart);\n}\n\n/** ZSTD_count_2segments() :\n*   can count match length with `ip` & `match` in 2 different segments.\n*   convention : on reaching mEnd, match count continue starting from iStart\n*/\nstatic size_t ZSTD_count_2segments(const BYTE* ip, const BYTE* match, const BYTE* iEnd, const BYTE* mEnd, const BYTE* iStart)\n{\n    const BYTE* const vEnd = MIN( ip + (mEnd - match), iEnd);\n    size_t const matchLength = ZSTD_count(ip, match, vEnd);\n    if (match + matchLength != mEnd) return matchLength;\n    return matchLength + ZSTD_count(ip+matchLength, iStart, iEnd);\n}\n\n\n/*-*************************************\n*  Hashes\n***************************************/\nstatic const U32 prime3bytes = 506832829U;\nstatic U32    ZSTD_hash3(U32 u, U32 h) { return ((u << (32-24)) * prime3bytes)  >> (32-h) ; }\nMEM_STATIC size_t ZSTD_hash3Ptr(const void* ptr, U32 h) { return ZSTD_hash3(MEM_readLE32(ptr), h); } /* only in zstd_opt.h */\n\nstatic const U32 prime4bytes = 2654435761U;\nstatic U32    ZSTD_hash4(U32 u, U32 h) { return (u * prime4bytes) >> (32-h) ; }\nstatic size_t ZSTD_hash4Ptr(const void* ptr, U32 h) { return ZSTD_hash4(MEM_read32(ptr), h); }\n\nstatic const U64 prime5bytes = 889523592379ULL;\nstatic size_t ZSTD_hash5(U64 u, U32 h) { return (size_t)(((u  << (64-40)) * prime5bytes) >> (64-h)) ; }\nstatic size_t ZSTD_hash5Ptr(const void* p, U32 h) { return ZSTD_hash5(MEM_readLE64(p), h); }\n\nstatic const U64 prime6bytes = 227718039650203ULL;\nstatic size_t ZSTD_hash6(U64 u, U32 h) { return (size_t)(((u  << (64-48)) * prime6bytes) >> (64-h)) ; }\nstatic size_t ZSTD_hash6Ptr(const void* p, U32 h) { return ZSTD_hash6(MEM_readLE64(p), h); }\n\nstatic const U64 prime7bytes = 58295818150454627ULL;\nstatic size_t ZSTD_hash7(U64 u, U32 h) { return (size_t)(((u  << (64-56)) * prime7bytes) >> (64-h)) ; }\nstatic size_t ZSTD_hash7Ptr(const void* p, U32 h) { return ZSTD_hash7(MEM_readLE64(p), h); }\n\nstatic const U64 prime8bytes = 0xCF1BBCDCB7A56463ULL;\nstatic size_t ZSTD_hash8(U64 u, U32 h) { return (size_t)(((u) * prime8bytes) >> (64-h)) ; }\nstatic size_t ZSTD_hash8Ptr(const void* p, U32 h) { return ZSTD_hash8(MEM_readLE64(p), h); }\n\nstatic size_t ZSTD_hashPtr(const void* p, U32 hBits, U32 mls)\n{\n    switch(mls)\n    {\n    default:\n    case 4: return ZSTD_hash4Ptr(p, hBits);\n    case 5: return ZSTD_hash5Ptr(p, hBits);\n    case 6: return ZSTD_hash6Ptr(p, hBits);\n    case 7: return ZSTD_hash7Ptr(p, hBits);\n    case 8: return ZSTD_hash8Ptr(p, hBits);\n    }\n}\n\n\n/*-*************************************\n*  Fast Scan\n***************************************/\nstatic void ZSTD_fillHashTable (ZSTD_CCtx* zc, const void* end, const U32 mls)\n{\n    U32* const hashTable = zc->hashTable;\n    U32  const hBits = zc->appliedParams.cParams.hashLog;\n    const BYTE* const base = zc->base;\n    const BYTE* ip = base + zc->nextToUpdate;\n    const BYTE* const iend = ((const BYTE*)end) - HASH_READ_SIZE;\n    const size_t fastHashFillStep = 3;\n\n    while(ip <= iend) {\n        hashTable[ZSTD_hashPtr(ip, hBits, mls)] = (U32)(ip - base);\n        ip += fastHashFillStep;\n    }\n}\n\n\nFORCE_INLINE_TEMPLATE\nvoid ZSTD_compressBlock_fast_generic(ZSTD_CCtx* cctx,\n                               const void* src, size_t srcSize,\n                               const U32 mls)\n{\n    U32* const hashTable = cctx->hashTable;\n    U32  const hBits = cctx->appliedParams.cParams.hashLog;\n    seqStore_t* seqStorePtr = &(cctx->seqStore);\n    const BYTE* const base = cctx->base;\n    const BYTE* const istart = (const BYTE*)src;\n    const BYTE* ip = istart;\n    const BYTE* anchor = istart;\n    const U32   lowestIndex = cctx->dictLimit;\n    const BYTE* const lowest = base + lowestIndex;\n    const BYTE* const iend = istart + srcSize;\n    const BYTE* const ilimit = iend - HASH_READ_SIZE;\n    U32 offset_1=seqStorePtr->rep[0], offset_2=seqStorePtr->rep[1];\n    U32 offsetSaved = 0;\n\n    /* init */\n    ip += (ip==lowest);\n    {   U32 const maxRep = (U32)(ip-lowest);\n        if (offset_2 > maxRep) offsetSaved = offset_2, offset_2 = 0;\n        if (offset_1 > maxRep) offsetSaved = offset_1, offset_1 = 0;\n    }\n\n    /* Main Search Loop */\n    while (ip < ilimit) {   /* < instead of <=, because repcode check at (ip+1) */\n        size_t mLength;\n        size_t const h = ZSTD_hashPtr(ip, hBits, mls);\n        U32 const current = (U32)(ip-base);\n        U32 const matchIndex = hashTable[h];\n        const BYTE* match = base + matchIndex;\n        hashTable[h] = current;   /* update hash table */\n\n        if ((offset_1 > 0) & (MEM_read32(ip+1-offset_1) == MEM_read32(ip+1))) {\n            mLength = ZSTD_count(ip+1+4, ip+1+4-offset_1, iend) + 4;\n            ip++;\n            ZSTD_storeSeq(seqStorePtr, ip-anchor, anchor, 0, mLength-MINMATCH);\n        } else {\n            U32 offset;\n            if ( (matchIndex <= lowestIndex) || (MEM_read32(match) != MEM_read32(ip)) ) {\n                ip += ((ip-anchor) >> g_searchStrength) + 1;\n                continue;\n            }\n            mLength = ZSTD_count(ip+4, match+4, iend) + 4;\n            offset = (U32)(ip-match);\n            while (((ip>anchor) & (match>lowest)) && (ip[-1] == match[-1])) { ip--; match--; mLength++; } /* catch up */\n            offset_2 = offset_1;\n            offset_1 = offset;\n\n            ZSTD_storeSeq(seqStorePtr, ip-anchor, anchor, offset + ZSTD_REP_MOVE, mLength-MINMATCH);\n        }\n\n        /* match found */\n        ip += mLength;\n        anchor = ip;\n\n        if (ip <= ilimit) {\n            /* Fill Table */\n            hashTable[ZSTD_hashPtr(base+current+2, hBits, mls)] = current+2;  /* here because current+2 could be > iend-8 */\n            hashTable[ZSTD_hashPtr(ip-2, hBits, mls)] = (U32)(ip-2-base);\n            /* check immediate repcode */\n            while ( (ip <= ilimit)\n                 && ( (offset_2>0)\n                 & (MEM_read32(ip) == MEM_read32(ip - offset_2)) )) {\n                /* store sequence */\n                size_t const rLength = ZSTD_count(ip+4, ip+4-offset_2, iend) + 4;\n                { U32 const tmpOff = offset_2; offset_2 = offset_1; offset_1 = tmpOff; }  /* swap offset_2 <=> offset_1 */\n                hashTable[ZSTD_hashPtr(ip, hBits, mls)] = (U32)(ip-base);\n                ZSTD_storeSeq(seqStorePtr, 0, anchor, 0, rLength-MINMATCH);\n                ip += rLength;\n                anchor = ip;\n                continue;   /* faster when present ... (?) */\n    }   }   }\n\n    /* save reps for next block */\n    seqStorePtr->repToConfirm[0] = offset_1 ? offset_1 : offsetSaved;\n    seqStorePtr->repToConfirm[1] = offset_2 ? offset_2 : offsetSaved;\n\n    /* Last Literals */\n    {   size_t const lastLLSize = iend - anchor;\n        memcpy(seqStorePtr->lit, anchor, lastLLSize);\n        seqStorePtr->lit += lastLLSize;\n    }\n}\n\n\nstatic void ZSTD_compressBlock_fast(ZSTD_CCtx* ctx,\n                       const void* src, size_t srcSize)\n{\n    const U32 mls = ctx->appliedParams.cParams.searchLength;\n    switch(mls)\n    {\n    default: /* includes case 3 */\n    case 4 :\n        ZSTD_compressBlock_fast_generic(ctx, src, srcSize, 4); return;\n    case 5 :\n        ZSTD_compressBlock_fast_generic(ctx, src, srcSize, 5); return;\n    case 6 :\n        ZSTD_compressBlock_fast_generic(ctx, src, srcSize, 6); return;\n    case 7 :\n        ZSTD_compressBlock_fast_generic(ctx, src, srcSize, 7); return;\n    }\n}\n\n\nstatic void ZSTD_compressBlock_fast_extDict_generic(ZSTD_CCtx* ctx,\n                                 const void* src, size_t srcSize,\n                                 const U32 mls)\n{\n    U32* hashTable = ctx->hashTable;\n    const U32 hBits = ctx->appliedParams.cParams.hashLog;\n    seqStore_t* seqStorePtr = &(ctx->seqStore);\n    const BYTE* const base = ctx->base;\n    const BYTE* const dictBase = ctx->dictBase;\n    const BYTE* const istart = (const BYTE*)src;\n    const BYTE* ip = istart;\n    const BYTE* anchor = istart;\n    const U32   lowestIndex = ctx->lowLimit;\n    const BYTE* const dictStart = dictBase + lowestIndex;\n    const U32   dictLimit = ctx->dictLimit;\n    const BYTE* const lowPrefixPtr = base + dictLimit;\n    const BYTE* const dictEnd = dictBase + dictLimit;\n    const BYTE* const iend = istart + srcSize;\n    const BYTE* const ilimit = iend - 8;\n    U32 offset_1=seqStorePtr->rep[0], offset_2=seqStorePtr->rep[1];\n\n    /* Search Loop */\n    while (ip < ilimit) {  /* < instead of <=, because (ip+1) */\n        const size_t h = ZSTD_hashPtr(ip, hBits, mls);\n        const U32 matchIndex = hashTable[h];\n        const BYTE* matchBase = matchIndex < dictLimit ? dictBase : base;\n        const BYTE* match = matchBase + matchIndex;\n        const U32 current = (U32)(ip-base);\n        const U32 repIndex = current + 1 - offset_1;   /* offset_1 expected <= current +1 */\n        const BYTE* repBase = repIndex < dictLimit ? dictBase : base;\n        const BYTE* repMatch = repBase + repIndex;\n        size_t mLength;\n        hashTable[h] = current;   /* update hash table */\n\n        if ( (((U32)((dictLimit-1) - repIndex) >= 3) /* intentional underflow */ & (repIndex > lowestIndex))\n           && (MEM_read32(repMatch) == MEM_read32(ip+1)) ) {\n            const BYTE* repMatchEnd = repIndex < dictLimit ? dictEnd : iend;\n            mLength = ZSTD_count_2segments(ip+1+4, repMatch+4, iend, repMatchEnd, lowPrefixPtr) + 4;\n            ip++;\n            ZSTD_storeSeq(seqStorePtr, ip-anchor, anchor, 0, mLength-MINMATCH);\n        } else {\n            if ( (matchIndex < lowestIndex) ||\n                 (MEM_read32(match) != MEM_read32(ip)) ) {\n                ip += ((ip-anchor) >> g_searchStrength) + 1;\n                continue;\n            }\n            {   const BYTE* matchEnd = matchIndex < dictLimit ? dictEnd : iend;\n                const BYTE* lowMatchPtr = matchIndex < dictLimit ? dictStart : lowPrefixPtr;\n                U32 offset;\n                mLength = ZSTD_count_2segments(ip+4, match+4, iend, matchEnd, lowPrefixPtr) + 4;\n                while (((ip>anchor) & (match>lowMatchPtr)) && (ip[-1] == match[-1])) { ip--; match--; mLength++; }   /* catch up */\n                offset = current - matchIndex;\n                offset_2 = offset_1;\n                offset_1 = offset;\n                ZSTD_storeSeq(seqStorePtr, ip-anchor, anchor, offset + ZSTD_REP_MOVE, mLength-MINMATCH);\n        }   }\n\n        /* found a match : store it */\n        ip += mLength;\n        anchor = ip;\n\n        if (ip <= ilimit) {\n            /* Fill Table */\n            hashTable[ZSTD_hashPtr(base+current+2, hBits, mls)] = current+2;\n            hashTable[ZSTD_hashPtr(ip-2, hBits, mls)] = (U32)(ip-2-base);\n            /* check immediate repcode */\n            while (ip <= ilimit) {\n                U32 const current2 = (U32)(ip-base);\n                U32 const repIndex2 = current2 - offset_2;\n                const BYTE* repMatch2 = repIndex2 < dictLimit ? dictBase + repIndex2 : base + repIndex2;\n                if ( (((U32)((dictLimit-1) - repIndex2) >= 3) & (repIndex2 > lowestIndex))  /* intentional overflow */\n                   && (MEM_read32(repMatch2) == MEM_read32(ip)) ) {\n                    const BYTE* const repEnd2 = repIndex2 < dictLimit ? dictEnd : iend;\n                    size_t const repLength2 = ZSTD_count_2segments(ip+4, repMatch2+4, iend, repEnd2, lowPrefixPtr) + 4;\n                    U32 tmpOffset = offset_2; offset_2 = offset_1; offset_1 = tmpOffset;   /* swap offset_2 <=> offset_1 */\n                    ZSTD_storeSeq(seqStorePtr, 0, anchor, 0, repLength2-MINMATCH);\n                    hashTable[ZSTD_hashPtr(ip, hBits, mls)] = current2;\n                    ip += repLength2;\n                    anchor = ip;\n                    continue;\n                }\n                break;\n    }   }   }\n\n    /* save reps for next block */\n    seqStorePtr->repToConfirm[0] = offset_1; seqStorePtr->repToConfirm[1] = offset_2;\n\n    /* Last Literals */\n    {   size_t const lastLLSize = iend - anchor;\n        memcpy(seqStorePtr->lit, anchor, lastLLSize);\n        seqStorePtr->lit += lastLLSize;\n    }\n}\n\n\nstatic void ZSTD_compressBlock_fast_extDict(ZSTD_CCtx* ctx,\n                         const void* src, size_t srcSize)\n{\n    U32 const mls = ctx->appliedParams.cParams.searchLength;\n    switch(mls)\n    {\n    default: /* includes case 3 */\n    case 4 :\n        ZSTD_compressBlock_fast_extDict_generic(ctx, src, srcSize, 4); return;\n    case 5 :\n        ZSTD_compressBlock_fast_extDict_generic(ctx, src, srcSize, 5); return;\n    case 6 :\n        ZSTD_compressBlock_fast_extDict_generic(ctx, src, srcSize, 6); return;\n    case 7 :\n        ZSTD_compressBlock_fast_extDict_generic(ctx, src, srcSize, 7); return;\n    }\n}\n\n\n/*-*************************************\n*  Double Fast\n***************************************/\nstatic void ZSTD_fillDoubleHashTable (ZSTD_CCtx* cctx, const void* end, const U32 mls)\n{\n    U32* const hashLarge = cctx->hashTable;\n    U32  const hBitsL = cctx->appliedParams.cParams.hashLog;\n    U32* const hashSmall = cctx->chainTable;\n    U32  const hBitsS = cctx->appliedParams.cParams.chainLog;\n    const BYTE* const base = cctx->base;\n    const BYTE* ip = base + cctx->nextToUpdate;\n    const BYTE* const iend = ((const BYTE*)end) - HASH_READ_SIZE;\n    const size_t fastHashFillStep = 3;\n\n    while(ip <= iend) {\n        hashSmall[ZSTD_hashPtr(ip, hBitsS, mls)] = (U32)(ip - base);\n        hashLarge[ZSTD_hashPtr(ip, hBitsL, 8)] = (U32)(ip - base);\n        ip += fastHashFillStep;\n    }\n}\n\n\nFORCE_INLINE_TEMPLATE\nvoid ZSTD_compressBlock_doubleFast_generic(ZSTD_CCtx* cctx,\n                                 const void* src, size_t srcSize,\n                                 const U32 mls)\n{\n    U32* const hashLong = cctx->hashTable;\n    const U32 hBitsL = cctx->appliedParams.cParams.hashLog;\n    U32* const hashSmall = cctx->chainTable;\n    const U32 hBitsS = cctx->appliedParams.cParams.chainLog;\n    seqStore_t* seqStorePtr = &(cctx->seqStore);\n    const BYTE* const base = cctx->base;\n    const BYTE* const istart = (const BYTE*)src;\n    const BYTE* ip = istart;\n    const BYTE* anchor = istart;\n    const U32 lowestIndex = cctx->dictLimit;\n    const BYTE* const lowest = base + lowestIndex;\n    const BYTE* const iend = istart + srcSize;\n    const BYTE* const ilimit = iend - HASH_READ_SIZE;\n    U32 offset_1=seqStorePtr->rep[0], offset_2=seqStorePtr->rep[1];\n    U32 offsetSaved = 0;\n\n    /* init */\n    ip += (ip==lowest);\n    {   U32 const maxRep = (U32)(ip-lowest);\n        if (offset_2 > maxRep) offsetSaved = offset_2, offset_2 = 0;\n        if (offset_1 > maxRep) offsetSaved = offset_1, offset_1 = 0;\n    }\n\n    /* Main Search Loop */\n    while (ip < ilimit) {   /* < instead of <=, because repcode check at (ip+1) */\n        size_t mLength;\n        size_t const h2 = ZSTD_hashPtr(ip, hBitsL, 8);\n        size_t const h = ZSTD_hashPtr(ip, hBitsS, mls);\n        U32 const current = (U32)(ip-base);\n        U32 const matchIndexL = hashLong[h2];\n        U32 const matchIndexS = hashSmall[h];\n        const BYTE* matchLong = base + matchIndexL;\n        const BYTE* match = base + matchIndexS;\n        hashLong[h2] = hashSmall[h] = current;   /* update hash tables */\n\n        assert(offset_1 <= current);   /* supposed guaranteed by construction */\n        if ((offset_1 > 0) & (MEM_read32(ip+1-offset_1) == MEM_read32(ip+1))) {\n            /* favor repcode */\n            mLength = ZSTD_count(ip+1+4, ip+1+4-offset_1, iend) + 4;\n            ip++;\n            ZSTD_storeSeq(seqStorePtr, ip-anchor, anchor, 0, mLength-MINMATCH);\n        } else {\n            U32 offset;\n            if ( (matchIndexL > lowestIndex) && (MEM_read64(matchLong) == MEM_read64(ip)) ) {\n                mLength = ZSTD_count(ip+8, matchLong+8, iend) + 8;\n                offset = (U32)(ip-matchLong);\n                while (((ip>anchor) & (matchLong>lowest)) && (ip[-1] == matchLong[-1])) { ip--; matchLong--; mLength++; } /* catch up */\n            } else if ( (matchIndexS > lowestIndex) && (MEM_read32(match) == MEM_read32(ip)) ) {\n                size_t const hl3 = ZSTD_hashPtr(ip+1, hBitsL, 8);\n                U32 const matchIndexL3 = hashLong[hl3];\n                const BYTE* matchL3 = base + matchIndexL3;\n                hashLong[hl3] = current + 1;\n                if ( (matchIndexL3 > lowestIndex) && (MEM_read64(matchL3) == MEM_read64(ip+1)) ) {\n                    mLength = ZSTD_count(ip+9, matchL3+8, iend) + 8;\n                    ip++;\n                    offset = (U32)(ip-matchL3);\n                    while (((ip>anchor) & (matchL3>lowest)) && (ip[-1] == matchL3[-1])) { ip--; matchL3--; mLength++; } /* catch up */\n                } else {\n                    mLength = ZSTD_count(ip+4, match+4, iend) + 4;\n                    offset = (U32)(ip-match);\n                    while (((ip>anchor) & (match>lowest)) && (ip[-1] == match[-1])) { ip--; match--; mLength++; } /* catch up */\n                }\n            } else {\n                ip += ((ip-anchor) >> g_searchStrength) + 1;\n                continue;\n            }\n\n            offset_2 = offset_1;\n            offset_1 = offset;\n\n            ZSTD_storeSeq(seqStorePtr, ip-anchor, anchor, offset + ZSTD_REP_MOVE, mLength-MINMATCH);\n        }\n\n        /* match found */\n        ip += mLength;\n        anchor = ip;\n\n        if (ip <= ilimit) {\n            /* Fill Table */\n            hashLong[ZSTD_hashPtr(base+current+2, hBitsL, 8)] =\n                hashSmall[ZSTD_hashPtr(base+current+2, hBitsS, mls)] = current+2;  /* here because current+2 could be > iend-8 */\n            hashLong[ZSTD_hashPtr(ip-2, hBitsL, 8)] =\n                hashSmall[ZSTD_hashPtr(ip-2, hBitsS, mls)] = (U32)(ip-2-base);\n\n            /* check immediate repcode */\n            while ( (ip <= ilimit)\n                 && ( (offset_2>0)\n                 & (MEM_read32(ip) == MEM_read32(ip - offset_2)) )) {\n                /* store sequence */\n                size_t const rLength = ZSTD_count(ip+4, ip+4-offset_2, iend) + 4;\n                { U32 const tmpOff = offset_2; offset_2 = offset_1; offset_1 = tmpOff; } /* swap offset_2 <=> offset_1 */\n                hashSmall[ZSTD_hashPtr(ip, hBitsS, mls)] = (U32)(ip-base);\n                hashLong[ZSTD_hashPtr(ip, hBitsL, 8)] = (U32)(ip-base);\n                ZSTD_storeSeq(seqStorePtr, 0, anchor, 0, rLength-MINMATCH);\n                ip += rLength;\n                anchor = ip;\n                continue;   /* faster when present ... (?) */\n    }   }   }\n\n    /* save reps for next block */\n    seqStorePtr->repToConfirm[0] = offset_1 ? offset_1 : offsetSaved;\n    seqStorePtr->repToConfirm[1] = offset_2 ? offset_2 : offsetSaved;\n\n    /* Last Literals */\n    {   size_t const lastLLSize = iend - anchor;\n        memcpy(seqStorePtr->lit, anchor, lastLLSize);\n        seqStorePtr->lit += lastLLSize;\n    }\n}\n\n\nstatic void ZSTD_compressBlock_doubleFast(ZSTD_CCtx* ctx, const void* src, size_t srcSize)\n{\n    const U32 mls = ctx->appliedParams.cParams.searchLength;\n    switch(mls)\n    {\n    default: /* includes case 3 */\n    case 4 :\n        ZSTD_compressBlock_doubleFast_generic(ctx, src, srcSize, 4); return;\n    case 5 :\n        ZSTD_compressBlock_doubleFast_generic(ctx, src, srcSize, 5); return;\n    case 6 :\n        ZSTD_compressBlock_doubleFast_generic(ctx, src, srcSize, 6); return;\n    case 7 :\n        ZSTD_compressBlock_doubleFast_generic(ctx, src, srcSize, 7); return;\n    }\n}\n\n\nstatic void ZSTD_compressBlock_doubleFast_extDict_generic(ZSTD_CCtx* ctx,\n                                 const void* src, size_t srcSize,\n                                 const U32 mls)\n{\n    U32* const hashLong = ctx->hashTable;\n    U32  const hBitsL = ctx->appliedParams.cParams.hashLog;\n    U32* const hashSmall = ctx->chainTable;\n    U32  const hBitsS = ctx->appliedParams.cParams.chainLog;\n    seqStore_t* seqStorePtr = &(ctx->seqStore);\n    const BYTE* const base = ctx->base;\n    const BYTE* const dictBase = ctx->dictBase;\n    const BYTE* const istart = (const BYTE*)src;\n    const BYTE* ip = istart;\n    const BYTE* anchor = istart;\n    const U32   lowestIndex = ctx->lowLimit;\n    const BYTE* const dictStart = dictBase + lowestIndex;\n    const U32   dictLimit = ctx->dictLimit;\n    const BYTE* const lowPrefixPtr = base + dictLimit;\n    const BYTE* const dictEnd = dictBase + dictLimit;\n    const BYTE* const iend = istart + srcSize;\n    const BYTE* const ilimit = iend - 8;\n    U32 offset_1=seqStorePtr->rep[0], offset_2=seqStorePtr->rep[1];\n\n    /* Search Loop */\n    while (ip < ilimit) {  /* < instead of <=, because (ip+1) */\n        const size_t hSmall = ZSTD_hashPtr(ip, hBitsS, mls);\n        const U32 matchIndex = hashSmall[hSmall];\n        const BYTE* matchBase = matchIndex < dictLimit ? dictBase : base;\n        const BYTE* match = matchBase + matchIndex;\n\n        const size_t hLong = ZSTD_hashPtr(ip, hBitsL, 8);\n        const U32 matchLongIndex = hashLong[hLong];\n        const BYTE* matchLongBase = matchLongIndex < dictLimit ? dictBase : base;\n        const BYTE* matchLong = matchLongBase + matchLongIndex;\n\n        const U32 current = (U32)(ip-base);\n        const U32 repIndex = current + 1 - offset_1;   /* offset_1 expected <= current +1 */\n        const BYTE* repBase = repIndex < dictLimit ? dictBase : base;\n        const BYTE* repMatch = repBase + repIndex;\n        size_t mLength;\n        hashSmall[hSmall] = hashLong[hLong] = current;   /* update hash table */\n\n        if ( (((U32)((dictLimit-1) - repIndex) >= 3) /* intentional underflow */ & (repIndex > lowestIndex))\n           && (MEM_read32(repMatch) == MEM_read32(ip+1)) ) {\n            const BYTE* repMatchEnd = repIndex < dictLimit ? dictEnd : iend;\n            mLength = ZSTD_count_2segments(ip+1+4, repMatch+4, iend, repMatchEnd, lowPrefixPtr) + 4;\n            ip++;\n            ZSTD_storeSeq(seqStorePtr, ip-anchor, anchor, 0, mLength-MINMATCH);\n        } else {\n            if ((matchLongIndex > lowestIndex) && (MEM_read64(matchLong) == MEM_read64(ip))) {\n                const BYTE* matchEnd = matchLongIndex < dictLimit ? dictEnd : iend;\n                const BYTE* lowMatchPtr = matchLongIndex < dictLimit ? dictStart : lowPrefixPtr;\n                U32 offset;\n                mLength = ZSTD_count_2segments(ip+8, matchLong+8, iend, matchEnd, lowPrefixPtr) + 8;\n                offset = current - matchLongIndex;\n                while (((ip>anchor) & (matchLong>lowMatchPtr)) && (ip[-1] == matchLong[-1])) { ip--; matchLong--; mLength++; }   /* catch up */\n                offset_2 = offset_1;\n                offset_1 = offset;\n                ZSTD_storeSeq(seqStorePtr, ip-anchor, anchor, offset + ZSTD_REP_MOVE, mLength-MINMATCH);\n\n            } else if ((matchIndex > lowestIndex) && (MEM_read32(match) == MEM_read32(ip))) {\n                size_t const h3 = ZSTD_hashPtr(ip+1, hBitsL, 8);\n                U32 const matchIndex3 = hashLong[h3];\n                const BYTE* const match3Base = matchIndex3 < dictLimit ? dictBase : base;\n                const BYTE* match3 = match3Base + matchIndex3;\n                U32 offset;\n                hashLong[h3] = current + 1;\n                if ( (matchIndex3 > lowestIndex) && (MEM_read64(match3) == MEM_read64(ip+1)) ) {\n                    const BYTE* matchEnd = matchIndex3 < dictLimit ? dictEnd : iend;\n                    const BYTE* lowMatchPtr = matchIndex3 < dictLimit ? dictStart : lowPrefixPtr;\n                    mLength = ZSTD_count_2segments(ip+9, match3+8, iend, matchEnd, lowPrefixPtr) + 8;\n                    ip++;\n                    offset = current+1 - matchIndex3;\n                    while (((ip>anchor) & (match3>lowMatchPtr)) && (ip[-1] == match3[-1])) { ip--; match3--; mLength++; } /* catch up */\n                } else {\n                    const BYTE* matchEnd = matchIndex < dictLimit ? dictEnd : iend;\n                    const BYTE* lowMatchPtr = matchIndex < dictLimit ? dictStart : lowPrefixPtr;\n                    mLength = ZSTD_count_2segments(ip+4, match+4, iend, matchEnd, lowPrefixPtr) + 4;\n                    offset = current - matchIndex;\n                    while (((ip>anchor) & (match>lowMatchPtr)) && (ip[-1] == match[-1])) { ip--; match--; mLength++; }   /* catch up */\n                }\n                offset_2 = offset_1;\n                offset_1 = offset;\n                ZSTD_storeSeq(seqStorePtr, ip-anchor, anchor, offset + ZSTD_REP_MOVE, mLength-MINMATCH);\n\n            } else {\n                ip += ((ip-anchor) >> g_searchStrength) + 1;\n                continue;\n        }   }\n\n        /* found a match : store it */\n        ip += mLength;\n        anchor = ip;\n\n        if (ip <= ilimit) {\n            /* Fill Table */\n            hashSmall[ZSTD_hashPtr(base+current+2, hBitsS, mls)] = current+2;\n            hashLong[ZSTD_hashPtr(base+current+2, hBitsL, 8)] = current+2;\n            hashSmall[ZSTD_hashPtr(ip-2, hBitsS, mls)] = (U32)(ip-2-base);\n            hashLong[ZSTD_hashPtr(ip-2, hBitsL, 8)] = (U32)(ip-2-base);\n            /* check immediate repcode */\n            while (ip <= ilimit) {\n                U32 const current2 = (U32)(ip-base);\n                U32 const repIndex2 = current2 - offset_2;\n                const BYTE* repMatch2 = repIndex2 < dictLimit ? dictBase + repIndex2 : base + repIndex2;\n                if ( (((U32)((dictLimit-1) - repIndex2) >= 3) & (repIndex2 > lowestIndex))  /* intentional overflow */\n                   && (MEM_read32(repMatch2) == MEM_read32(ip)) ) {\n                    const BYTE* const repEnd2 = repIndex2 < dictLimit ? dictEnd : iend;\n                    size_t const repLength2 = ZSTD_count_2segments(ip+4, repMatch2+4, iend, repEnd2, lowPrefixPtr) + 4;\n                    U32 tmpOffset = offset_2; offset_2 = offset_1; offset_1 = tmpOffset;   /* swap offset_2 <=> offset_1 */\n                    ZSTD_storeSeq(seqStorePtr, 0, anchor, 0, repLength2-MINMATCH);\n                    hashSmall[ZSTD_hashPtr(ip, hBitsS, mls)] = current2;\n                    hashLong[ZSTD_hashPtr(ip, hBitsL, 8)] = current2;\n                    ip += repLength2;\n                    anchor = ip;\n                    continue;\n                }\n                break;\n    }   }   }\n\n    /* save reps for next block */\n    seqStorePtr->repToConfirm[0] = offset_1; seqStorePtr->repToConfirm[1] = offset_2;\n\n    /* Last Literals */\n    {   size_t const lastLLSize = iend - anchor;\n        memcpy(seqStorePtr->lit, anchor, lastLLSize);\n        seqStorePtr->lit += lastLLSize;\n    }\n}\n\n\nstatic void ZSTD_compressBlock_doubleFast_extDict(ZSTD_CCtx* ctx,\n                         const void* src, size_t srcSize)\n{\n    U32 const mls = ctx->appliedParams.cParams.searchLength;\n    switch(mls)\n    {\n    default: /* includes case 3 */\n    case 4 :\n        ZSTD_compressBlock_doubleFast_extDict_generic(ctx, src, srcSize, 4); return;\n    case 5 :\n        ZSTD_compressBlock_doubleFast_extDict_generic(ctx, src, srcSize, 5); return;\n    case 6 :\n        ZSTD_compressBlock_doubleFast_extDict_generic(ctx, src, srcSize, 6); return;\n    case 7 :\n        ZSTD_compressBlock_doubleFast_extDict_generic(ctx, src, srcSize, 7); return;\n    }\n}\n\n\n/*-*************************************\n*  Binary Tree search\n***************************************/\n/** ZSTD_insertBt1() : add one or multiple positions to tree.\n*   ip : assumed <= iend-8 .\n*   @return : nb of positions added */\nstatic U32 ZSTD_insertBt1(ZSTD_CCtx* zc, const BYTE* const ip, const U32 mls, const BYTE* const iend, U32 nbCompares,\n                          U32 extDict)\n{\n    U32*   const hashTable = zc->hashTable;\n    U32    const hashLog = zc->appliedParams.cParams.hashLog;\n    size_t const h  = ZSTD_hashPtr(ip, hashLog, mls);\n    U32*   const bt = zc->chainTable;\n    U32    const btLog  = zc->appliedParams.cParams.chainLog - 1;\n    U32    const btMask = (1 << btLog) - 1;\n    U32 matchIndex = hashTable[h];\n    size_t commonLengthSmaller=0, commonLengthLarger=0;\n    const BYTE* const base = zc->base;\n    const BYTE* const dictBase = zc->dictBase;\n    const U32 dictLimit = zc->dictLimit;\n    const BYTE* const dictEnd = dictBase + dictLimit;\n    const BYTE* const prefixStart = base + dictLimit;\n    const BYTE* match;\n    const U32 current = (U32)(ip-base);\n    const U32 btLow = btMask >= current ? 0 : current - btMask;\n    U32* smallerPtr = bt + 2*(current&btMask);\n    U32* largerPtr  = smallerPtr + 1;\n    U32 dummy32;   /* to be nullified at the end */\n    U32 const windowLow = zc->lowLimit;\n    U32 matchEndIdx = current+8;\n    size_t bestLength = 8;\n#ifdef ZSTD_C_PREDICT\n    U32 predictedSmall = *(bt + 2*((current-1)&btMask) + 0);\n    U32 predictedLarge = *(bt + 2*((current-1)&btMask) + 1);\n    predictedSmall += (predictedSmall>0);\n    predictedLarge += (predictedLarge>0);\n#endif /* ZSTD_C_PREDICT */\n\n    hashTable[h] = current;   /* Update Hash Table */\n\n    while (nbCompares-- && (matchIndex > windowLow)) {\n        U32* const nextPtr = bt + 2*(matchIndex & btMask);\n        size_t matchLength = MIN(commonLengthSmaller, commonLengthLarger);   /* guaranteed minimum nb of common bytes */\n\n#ifdef ZSTD_C_PREDICT   /* note : can create issues when hlog small <= 11 */\n        const U32* predictPtr = bt + 2*((matchIndex-1) & btMask);   /* written this way, as bt is a roll buffer */\n        if (matchIndex == predictedSmall) {\n            /* no need to check length, result known */\n            *smallerPtr = matchIndex;\n            if (matchIndex <= btLow) { smallerPtr=&dummy32; break; }   /* beyond tree size, stop the search */\n            smallerPtr = nextPtr+1;               /* new \"smaller\" => larger of match */\n            matchIndex = nextPtr[1];              /* new matchIndex larger than previous (closer to current) */\n            predictedSmall = predictPtr[1] + (predictPtr[1]>0);\n            continue;\n        }\n        if (matchIndex == predictedLarge) {\n            *largerPtr = matchIndex;\n            if (matchIndex <= btLow) { largerPtr=&dummy32; break; }   /* beyond tree size, stop the search */\n            largerPtr = nextPtr;\n            matchIndex = nextPtr[0];\n            predictedLarge = predictPtr[0] + (predictPtr[0]>0);\n            continue;\n        }\n#endif\n        if ((!extDict) || (matchIndex+matchLength >= dictLimit)) {\n            match = base + matchIndex;\n            if (match[matchLength] == ip[matchLength])\n                matchLength += ZSTD_count(ip+matchLength+1, match+matchLength+1, iend) +1;\n        } else {\n            match = dictBase + matchIndex;\n            matchLength += ZSTD_count_2segments(ip+matchLength, match+matchLength, iend, dictEnd, prefixStart);\n            if (matchIndex+matchLength >= dictLimit)\n                match = base + matchIndex;   /* to prepare for next usage of match[matchLength] */\n        }\n\n        if (matchLength > bestLength) {\n            bestLength = matchLength;\n            if (matchLength > matchEndIdx - matchIndex)\n                matchEndIdx = matchIndex + (U32)matchLength;\n        }\n\n        if (ip+matchLength == iend)   /* equal : no way to know if inf or sup */\n            break;   /* drop , to guarantee consistency ; miss a bit of compression, but other solutions can corrupt the tree */\n\n        if (match[matchLength] < ip[matchLength]) {  /* necessarily within correct buffer */\n            /* match is smaller than current */\n            *smallerPtr = matchIndex;             /* update smaller idx */\n            commonLengthSmaller = matchLength;    /* all smaller will now have at least this guaranteed common length */\n            if (matchIndex <= btLow) { smallerPtr=&dummy32; break; }   /* beyond tree size, stop the search */\n            smallerPtr = nextPtr+1;               /* new \"smaller\" => larger of match */\n            matchIndex = nextPtr[1];              /* new matchIndex larger than previous (closer to current) */\n        } else {\n            /* match is larger than current */\n            *largerPtr = matchIndex;\n            commonLengthLarger = matchLength;\n            if (matchIndex <= btLow) { largerPtr=&dummy32; break; }   /* beyond tree size, stop the search */\n            largerPtr = nextPtr;\n            matchIndex = nextPtr[0];\n    }   }\n\n    *smallerPtr = *largerPtr = 0;\n    if (bestLength > 384) return MIN(192, (U32)(bestLength - 384));   /* speed optimization */\n    if (matchEndIdx > current + 8) return matchEndIdx - current - 8;\n    return 1;\n}\n\n\nstatic size_t ZSTD_insertBtAndFindBestMatch (\n                        ZSTD_CCtx* zc,\n                        const BYTE* const ip, const BYTE* const iend,\n                        size_t* offsetPtr,\n                        U32 nbCompares, const U32 mls,\n                        U32 extDict)\n{\n    U32*   const hashTable = zc->hashTable;\n    U32    const hashLog = zc->appliedParams.cParams.hashLog;\n    size_t const h  = ZSTD_hashPtr(ip, hashLog, mls);\n    U32*   const bt = zc->chainTable;\n    U32    const btLog  = zc->appliedParams.cParams.chainLog - 1;\n    U32    const btMask = (1 << btLog) - 1;\n    U32 matchIndex  = hashTable[h];\n    size_t commonLengthSmaller=0, commonLengthLarger=0;\n    const BYTE* const base = zc->base;\n    const BYTE* const dictBase = zc->dictBase;\n    const U32 dictLimit = zc->dictLimit;\n    const BYTE* const dictEnd = dictBase + dictLimit;\n    const BYTE* const prefixStart = base + dictLimit;\n    const U32 current = (U32)(ip-base);\n    const U32 btLow = btMask >= current ? 0 : current - btMask;\n    const U32 windowLow = zc->lowLimit;\n    U32* smallerPtr = bt + 2*(current&btMask);\n    U32* largerPtr  = bt + 2*(current&btMask) + 1;\n    U32 matchEndIdx = current+8;\n    U32 dummy32;   /* to be nullified at the end */\n    size_t bestLength = 0;\n\n    hashTable[h] = current;   /* Update Hash Table */\n\n    while (nbCompares-- && (matchIndex > windowLow)) {\n        U32* const nextPtr = bt + 2*(matchIndex & btMask);\n        size_t matchLength = MIN(commonLengthSmaller, commonLengthLarger);   /* guaranteed minimum nb of common bytes */\n        const BYTE* match;\n\n        if ((!extDict) || (matchIndex+matchLength >= dictLimit)) {\n            match = base + matchIndex;\n            if (match[matchLength] == ip[matchLength])\n                matchLength += ZSTD_count(ip+matchLength+1, match+matchLength+1, iend) +1;\n        } else {\n            match = dictBase + matchIndex;\n            matchLength += ZSTD_count_2segments(ip+matchLength, match+matchLength, iend, dictEnd, prefixStart);\n            if (matchIndex+matchLength >= dictLimit)\n                match = base + matchIndex;   /* to prepare for next usage of match[matchLength] */\n        }\n\n        if (matchLength > bestLength) {\n            if (matchLength > matchEndIdx - matchIndex)\n                matchEndIdx = matchIndex + (U32)matchLength;\n            if ( (4*(int)(matchLength-bestLength)) > (int)(ZSTD_highbit32(current-matchIndex+1) - ZSTD_highbit32((U32)offsetPtr[0]+1)) )\n                bestLength = matchLength, *offsetPtr = ZSTD_REP_MOVE + current - matchIndex;\n            if (ip+matchLength == iend)   /* equal : no way to know if inf or sup */\n                break;   /* drop, to guarantee consistency (miss a little bit of compression) */\n        }\n\n        if (match[matchLength] < ip[matchLength]) {\n            /* match is smaller than current */\n            *smallerPtr = matchIndex;             /* update smaller idx */\n            commonLengthSmaller = matchLength;    /* all smaller will now have at least this guaranteed common length */\n            if (matchIndex <= btLow) { smallerPtr=&dummy32; break; }   /* beyond tree size, stop the search */\n            smallerPtr = nextPtr+1;               /* new \"smaller\" => larger of match */\n            matchIndex = nextPtr[1];              /* new matchIndex larger than previous (closer to current) */\n        } else {\n            /* match is larger than current */\n            *largerPtr = matchIndex;\n            commonLengthLarger = matchLength;\n            if (matchIndex <= btLow) { largerPtr=&dummy32; break; }   /* beyond tree size, stop the search */\n            largerPtr = nextPtr;\n            matchIndex = nextPtr[0];\n    }   }\n\n    *smallerPtr = *largerPtr = 0;\n\n    zc->nextToUpdate = (matchEndIdx > current + 8) ? matchEndIdx - 8 : current+1;\n    return bestLength;\n}\n\n\nstatic void ZSTD_updateTree(ZSTD_CCtx* zc, const BYTE* const ip, const BYTE* const iend, const U32 nbCompares, const U32 mls)\n{\n    const BYTE* const base = zc->base;\n    const U32 target = (U32)(ip - base);\n    U32 idx = zc->nextToUpdate;\n\n    while(idx < target)\n        idx += ZSTD_insertBt1(zc, base+idx, mls, iend, nbCompares, 0);\n}\n\n/** ZSTD_BtFindBestMatch() : Tree updater, providing best match */\nstatic size_t ZSTD_BtFindBestMatch (\n                        ZSTD_CCtx* zc,\n                        const BYTE* const ip, const BYTE* const iLimit,\n                        size_t* offsetPtr,\n                        const U32 maxNbAttempts, const U32 mls)\n{\n    if (ip < zc->base + zc->nextToUpdate) return 0;   /* skipped area */\n    ZSTD_updateTree(zc, ip, iLimit, maxNbAttempts, mls);\n    return ZSTD_insertBtAndFindBestMatch(zc, ip, iLimit, offsetPtr, maxNbAttempts, mls, 0);\n}\n\n\nstatic size_t ZSTD_BtFindBestMatch_selectMLS (\n                        ZSTD_CCtx* zc,   /* Index table will be updated */\n                        const BYTE* ip, const BYTE* const iLimit,\n                        size_t* offsetPtr,\n                        const U32 maxNbAttempts, const U32 matchLengthSearch)\n{\n    switch(matchLengthSearch)\n    {\n    default : /* includes case 3 */\n    case 4 : return ZSTD_BtFindBestMatch(zc, ip, iLimit, offsetPtr, maxNbAttempts, 4);\n    case 5 : return ZSTD_BtFindBestMatch(zc, ip, iLimit, offsetPtr, maxNbAttempts, 5);\n    case 7 :\n    case 6 : return ZSTD_BtFindBestMatch(zc, ip, iLimit, offsetPtr, maxNbAttempts, 6);\n    }\n}\n\n\nstatic void ZSTD_updateTree_extDict(ZSTD_CCtx* zc, const BYTE* const ip, const BYTE* const iend, const U32 nbCompares, const U32 mls)\n{\n    const BYTE* const base = zc->base;\n    const U32 target = (U32)(ip - base);\n    U32 idx = zc->nextToUpdate;\n\n    while (idx < target) idx += ZSTD_insertBt1(zc, base+idx, mls, iend, nbCompares, 1);\n}\n\n\n/** Tree updater, providing best match */\nstatic size_t ZSTD_BtFindBestMatch_extDict (\n                        ZSTD_CCtx* zc,\n                        const BYTE* const ip, const BYTE* const iLimit,\n                        size_t* offsetPtr,\n                        const U32 maxNbAttempts, const U32 mls)\n{\n    if (ip < zc->base + zc->nextToUpdate) return 0;   /* skipped area */\n    ZSTD_updateTree_extDict(zc, ip, iLimit, maxNbAttempts, mls);\n    return ZSTD_insertBtAndFindBestMatch(zc, ip, iLimit, offsetPtr, maxNbAttempts, mls, 1);\n}\n\n\nstatic size_t ZSTD_BtFindBestMatch_selectMLS_extDict (\n                        ZSTD_CCtx* zc,   /* Index table will be updated */\n                        const BYTE* ip, const BYTE* const iLimit,\n                        size_t* offsetPtr,\n                        const U32 maxNbAttempts, const U32 matchLengthSearch)\n{\n    switch(matchLengthSearch)\n    {\n    default : /* includes case 3 */\n    case 4 : return ZSTD_BtFindBestMatch_extDict(zc, ip, iLimit, offsetPtr, maxNbAttempts, 4);\n    case 5 : return ZSTD_BtFindBestMatch_extDict(zc, ip, iLimit, offsetPtr, maxNbAttempts, 5);\n    case 7 :\n    case 6 : return ZSTD_BtFindBestMatch_extDict(zc, ip, iLimit, offsetPtr, maxNbAttempts, 6);\n    }\n}\n\n\n\n/* *********************************\n*  Hash Chain\n***********************************/\n#define NEXT_IN_CHAIN(d, mask)   chainTable[(d) & mask]\n\n/* Update chains up to ip (excluded)\n   Assumption : always within prefix (i.e. not within extDict) */\nFORCE_INLINE_TEMPLATE\nU32 ZSTD_insertAndFindFirstIndex (ZSTD_CCtx* zc, const BYTE* ip, U32 mls)\n{\n    U32* const hashTable  = zc->hashTable;\n    const U32 hashLog = zc->appliedParams.cParams.hashLog;\n    U32* const chainTable = zc->chainTable;\n    const U32 chainMask = (1 << zc->appliedParams.cParams.chainLog) - 1;\n    const BYTE* const base = zc->base;\n    const U32 target = (U32)(ip - base);\n    U32 idx = zc->nextToUpdate;\n\n    while(idx < target) { /* catch up */\n        size_t const h = ZSTD_hashPtr(base+idx, hashLog, mls);\n        NEXT_IN_CHAIN(idx, chainMask) = hashTable[h];\n        hashTable[h] = idx;\n        idx++;\n    }\n\n    zc->nextToUpdate = target;\n    return hashTable[ZSTD_hashPtr(ip, hashLog, mls)];\n}\n\n\n/* inlining is important to hardwire a hot branch (template emulation) */\nFORCE_INLINE_TEMPLATE\nsize_t ZSTD_HcFindBestMatch_generic (\n                        ZSTD_CCtx* zc,   /* Index table will be updated */\n                        const BYTE* const ip, const BYTE* const iLimit,\n                        size_t* offsetPtr,\n                        const U32 maxNbAttempts, const U32 mls, const U32 extDict)\n{\n    U32* const chainTable = zc->chainTable;\n    const U32 chainSize = (1 << zc->appliedParams.cParams.chainLog);\n    const U32 chainMask = chainSize-1;\n    const BYTE* const base = zc->base;\n    const BYTE* const dictBase = zc->dictBase;\n    const U32 dictLimit = zc->dictLimit;\n    const BYTE* const prefixStart = base + dictLimit;\n    const BYTE* const dictEnd = dictBase + dictLimit;\n    const U32 lowLimit = zc->lowLimit;\n    const U32 current = (U32)(ip-base);\n    const U32 minChain = current > chainSize ? current - chainSize : 0;\n    int nbAttempts=maxNbAttempts;\n    size_t ml=4-1;\n\n    /* HC4 match finder */\n    U32 matchIndex = ZSTD_insertAndFindFirstIndex (zc, ip, mls);\n\n    for ( ; (matchIndex>lowLimit) & (nbAttempts>0) ; nbAttempts--) {\n        const BYTE* match;\n        size_t currentMl=0;\n        if ((!extDict) || matchIndex >= dictLimit) {\n            match = base + matchIndex;\n            if (match[ml] == ip[ml])   /* potentially better */\n                currentMl = ZSTD_count(ip, match, iLimit);\n        } else {\n            match = dictBase + matchIndex;\n            if (MEM_read32(match) == MEM_read32(ip))   /* assumption : matchIndex <= dictLimit-4 (by table construction) */\n                currentMl = ZSTD_count_2segments(ip+4, match+4, iLimit, dictEnd, prefixStart) + 4;\n        }\n\n        /* save best solution */\n        if (currentMl > ml) {\n            ml = currentMl;\n            *offsetPtr = current - matchIndex + ZSTD_REP_MOVE;\n            if (ip+currentMl == iLimit) break; /* best possible, avoids read overflow on next attempt */\n        }\n\n        if (matchIndex <= minChain) break;\n        matchIndex = NEXT_IN_CHAIN(matchIndex, chainMask);\n    }\n\n    return ml;\n}\n\n\nFORCE_INLINE_TEMPLATE size_t ZSTD_HcFindBestMatch_selectMLS (\n                        ZSTD_CCtx* zc,\n                        const BYTE* ip, const BYTE* const iLimit,\n                        size_t* offsetPtr,\n                        const U32 maxNbAttempts, const U32 matchLengthSearch)\n{\n    switch(matchLengthSearch)\n    {\n    default : /* includes case 3 */\n    case 4 : return ZSTD_HcFindBestMatch_generic(zc, ip, iLimit, offsetPtr, maxNbAttempts, 4, 0);\n    case 5 : return ZSTD_HcFindBestMatch_generic(zc, ip, iLimit, offsetPtr, maxNbAttempts, 5, 0);\n    case 7 :\n    case 6 : return ZSTD_HcFindBestMatch_generic(zc, ip, iLimit, offsetPtr, maxNbAttempts, 6, 0);\n    }\n}\n\n\nFORCE_INLINE_TEMPLATE size_t ZSTD_HcFindBestMatch_extDict_selectMLS (\n                        ZSTD_CCtx* zc,\n                        const BYTE* ip, const BYTE* const iLimit,\n                        size_t* offsetPtr,\n                        const U32 maxNbAttempts, const U32 matchLengthSearch)\n{\n    switch(matchLengthSearch)\n    {\n    default : /* includes case 3 */\n    case 4 : return ZSTD_HcFindBestMatch_generic(zc, ip, iLimit, offsetPtr, maxNbAttempts, 4, 1);\n    case 5 : return ZSTD_HcFindBestMatch_generic(zc, ip, iLimit, offsetPtr, maxNbAttempts, 5, 1);\n    case 7 :\n    case 6 : return ZSTD_HcFindBestMatch_generic(zc, ip, iLimit, offsetPtr, maxNbAttempts, 6, 1);\n    }\n}\n\n\n/* *******************************\n*  Common parser - lazy strategy\n*********************************/\nFORCE_INLINE_TEMPLATE\nvoid ZSTD_compressBlock_lazy_generic(ZSTD_CCtx* ctx,\n                                     const void* src, size_t srcSize,\n                                     const U32 searchMethod, const U32 depth)\n{\n    seqStore_t* seqStorePtr = &(ctx->seqStore);\n    const BYTE* const istart = (const BYTE*)src;\n    const BYTE* ip = istart;\n    const BYTE* anchor = istart;\n    const BYTE* const iend = istart + srcSize;\n    const BYTE* const ilimit = iend - 8;\n    const BYTE* const base = ctx->base + ctx->dictLimit;\n\n    U32 const maxSearches = 1 << ctx->appliedParams.cParams.searchLog;\n    U32 const mls = ctx->appliedParams.cParams.searchLength;\n\n    typedef size_t (*searchMax_f)(ZSTD_CCtx* zc, const BYTE* ip, const BYTE* iLimit,\n                        size_t* offsetPtr,\n                        U32 maxNbAttempts, U32 matchLengthSearch);\n    searchMax_f const searchMax = searchMethod ? ZSTD_BtFindBestMatch_selectMLS : ZSTD_HcFindBestMatch_selectMLS;\n    U32 offset_1 = seqStorePtr->rep[0], offset_2 = seqStorePtr->rep[1], savedOffset=0;\n\n    /* init */\n    ip += (ip==base);\n    ctx->nextToUpdate3 = ctx->nextToUpdate;\n    {   U32 const maxRep = (U32)(ip-base);\n        if (offset_2 > maxRep) savedOffset = offset_2, offset_2 = 0;\n        if (offset_1 > maxRep) savedOffset = offset_1, offset_1 = 0;\n    }\n\n    /* Match Loop */\n    while (ip < ilimit) {\n        size_t matchLength=0;\n        size_t offset=0;\n        const BYTE* start=ip+1;\n\n        /* check repCode */\n        if ((offset_1>0) & (MEM_read32(ip+1) == MEM_read32(ip+1 - offset_1))) {\n            /* repcode : we take it */\n            matchLength = ZSTD_count(ip+1+4, ip+1+4-offset_1, iend) + 4;\n            if (depth==0) goto _storeSequence;\n        }\n\n        /* first search (depth 0) */\n        {   size_t offsetFound = 99999999;\n            size_t const ml2 = searchMax(ctx, ip, iend, &offsetFound, maxSearches, mls);\n            if (ml2 > matchLength)\n                matchLength = ml2, start = ip, offset=offsetFound;\n        }\n\n        if (matchLength < 4) {\n            ip += ((ip-anchor) >> g_searchStrength) + 1;   /* jump faster over incompressible sections */\n            continue;\n        }\n\n        /* let's try to find a better solution */\n        if (depth>=1)\n        while (ip<ilimit) {\n            ip ++;\n            if ((offset) && ((offset_1>0) & (MEM_read32(ip) == MEM_read32(ip - offset_1)))) {\n                size_t const mlRep = ZSTD_count(ip+4, ip+4-offset_1, iend) + 4;\n                int const gain2 = (int)(mlRep * 3);\n                int const gain1 = (int)(matchLength*3 - ZSTD_highbit32((U32)offset+1) + 1);\n                if ((mlRep >= 4) && (gain2 > gain1))\n                    matchLength = mlRep, offset = 0, start = ip;\n            }\n            {   size_t offset2=99999999;\n                size_t const ml2 = searchMax(ctx, ip, iend, &offset2, maxSearches, mls);\n                int const gain2 = (int)(ml2*4 - ZSTD_highbit32((U32)offset2+1));   /* raw approx */\n                int const gain1 = (int)(matchLength*4 - ZSTD_highbit32((U32)offset+1) + 4);\n                if ((ml2 >= 4) && (gain2 > gain1)) {\n                    matchLength = ml2, offset = offset2, start = ip;\n                    continue;   /* search a better one */\n            }   }\n\n            /* let's find an even better one */\n            if ((depth==2) && (ip<ilimit)) {\n                ip ++;\n                if ((offset) && ((offset_1>0) & (MEM_read32(ip) == MEM_read32(ip - offset_1)))) {\n                    size_t const ml2 = ZSTD_count(ip+4, ip+4-offset_1, iend) + 4;\n                    int const gain2 = (int)(ml2 * 4);\n                    int const gain1 = (int)(matchLength*4 - ZSTD_highbit32((U32)offset+1) + 1);\n                    if ((ml2 >= 4) && (gain2 > gain1))\n                        matchLength = ml2, offset = 0, start = ip;\n                }\n                {   size_t offset2=99999999;\n                    size_t const ml2 = searchMax(ctx, ip, iend, &offset2, maxSearches, mls);\n                    int const gain2 = (int)(ml2*4 - ZSTD_highbit32((U32)offset2+1));   /* raw approx */\n                    int const gain1 = (int)(matchLength*4 - ZSTD_highbit32((U32)offset+1) + 7);\n                    if ((ml2 >= 4) && (gain2 > gain1)) {\n                        matchLength = ml2, offset = offset2, start = ip;\n                        continue;\n            }   }   }\n            break;  /* nothing found : store previous solution */\n        }\n\n        /* NOTE:\n         * start[-offset+ZSTD_REP_MOVE-1] is undefined behavior.\n         * (-offset+ZSTD_REP_MOVE-1) is unsigned, and is added to start, which\n         * overflows the pointer, which is undefined behavior.\n         */\n        /* catch up */\n        if (offset) {\n            while ( (start > anchor)\n                 && (start > base+offset-ZSTD_REP_MOVE)\n                 && (start[-1] == (start-offset+ZSTD_REP_MOVE)[-1]) )  /* only search for offset within prefix */\n                { start--; matchLength++; }\n            offset_2 = offset_1; offset_1 = (U32)(offset - ZSTD_REP_MOVE);\n        }\n        /* store sequence */\n_storeSequence:\n        {   size_t const litLength = start - anchor;\n            ZSTD_storeSeq(seqStorePtr, litLength, anchor, (U32)offset, matchLength-MINMATCH);\n            anchor = ip = start + matchLength;\n        }\n\n        /* check immediate repcode */\n        while ( (ip <= ilimit)\n             && ((offset_2>0)\n             & (MEM_read32(ip) == MEM_read32(ip - offset_2)) )) {\n            /* store sequence */\n            matchLength = ZSTD_count(ip+4, ip+4-offset_2, iend) + 4;\n            offset = offset_2; offset_2 = offset_1; offset_1 = (U32)offset; /* swap repcodes */\n            ZSTD_storeSeq(seqStorePtr, 0, anchor, 0, matchLength-MINMATCH);\n            ip += matchLength;\n            anchor = ip;\n            continue;   /* faster when present ... (?) */\n    }   }\n\n    /* Save reps for next block */\n    seqStorePtr->repToConfirm[0] = offset_1 ? offset_1 : savedOffset;\n    seqStorePtr->repToConfirm[1] = offset_2 ? offset_2 : savedOffset;\n\n    /* Last Literals */\n    {   size_t const lastLLSize = iend - anchor;\n        memcpy(seqStorePtr->lit, anchor, lastLLSize);\n        seqStorePtr->lit += lastLLSize;\n    }\n}\n\n\nstatic void ZSTD_compressBlock_btlazy2(ZSTD_CCtx* ctx, const void* src, size_t srcSize)\n{\n    ZSTD_compressBlock_lazy_generic(ctx, src, srcSize, 1, 2);\n}\n\nstatic void ZSTD_compressBlock_lazy2(ZSTD_CCtx* ctx, const void* src, size_t srcSize)\n{\n    ZSTD_compressBlock_lazy_generic(ctx, src, srcSize, 0, 2);\n}\n\nstatic void ZSTD_compressBlock_lazy(ZSTD_CCtx* ctx, const void* src, size_t srcSize)\n{\n    ZSTD_compressBlock_lazy_generic(ctx, src, srcSize, 0, 1);\n}\n\nstatic void ZSTD_compressBlock_greedy(ZSTD_CCtx* ctx, const void* src, size_t srcSize)\n{\n    ZSTD_compressBlock_lazy_generic(ctx, src, srcSize, 0, 0);\n}\n\n\nFORCE_INLINE_TEMPLATE\nvoid ZSTD_compressBlock_lazy_extDict_generic(ZSTD_CCtx* ctx,\n                                     const void* src, size_t srcSize,\n                                     const U32 searchMethod, const U32 depth)\n{\n    seqStore_t* seqStorePtr = &(ctx->seqStore);\n    const BYTE* const istart = (const BYTE*)src;\n    const BYTE* ip = istart;\n    const BYTE* anchor = istart;\n    const BYTE* const iend = istart + srcSize;\n    const BYTE* const ilimit = iend - 8;\n    const BYTE* const base = ctx->base;\n    const U32 dictLimit = ctx->dictLimit;\n    const U32 lowestIndex = ctx->lowLimit;\n    const BYTE* const prefixStart = base + dictLimit;\n    const BYTE* const dictBase = ctx->dictBase;\n    const BYTE* const dictEnd  = dictBase + dictLimit;\n    const BYTE* const dictStart  = dictBase + ctx->lowLimit;\n\n    const U32 maxSearches = 1 << ctx->appliedParams.cParams.searchLog;\n    const U32 mls = ctx->appliedParams.cParams.searchLength;\n\n    typedef size_t (*searchMax_f)(ZSTD_CCtx* zc, const BYTE* ip, const BYTE* iLimit,\n                        size_t* offsetPtr,\n                        U32 maxNbAttempts, U32 matchLengthSearch);\n    searchMax_f searchMax = searchMethod ? ZSTD_BtFindBestMatch_selectMLS_extDict : ZSTD_HcFindBestMatch_extDict_selectMLS;\n\n    U32 offset_1 = seqStorePtr->rep[0], offset_2 = seqStorePtr->rep[1];\n\n    /* init */\n    ctx->nextToUpdate3 = ctx->nextToUpdate;\n    ip += (ip == prefixStart);\n\n    /* Match Loop */\n    while (ip < ilimit) {\n        size_t matchLength=0;\n        size_t offset=0;\n        const BYTE* start=ip+1;\n        U32 current = (U32)(ip-base);\n\n        /* check repCode */\n        {   const U32 repIndex = (U32)(current+1 - offset_1);\n            const BYTE* const repBase = repIndex < dictLimit ? dictBase : base;\n            const BYTE* const repMatch = repBase + repIndex;\n            if (((U32)((dictLimit-1) - repIndex) >= 3) & (repIndex > lowestIndex))   /* intentional overflow */\n            if (MEM_read32(ip+1) == MEM_read32(repMatch)) {\n                /* repcode detected we should take it */\n                const BYTE* const repEnd = repIndex < dictLimit ? dictEnd : iend;\n                matchLength = ZSTD_count_2segments(ip+1+4, repMatch+4, iend, repEnd, prefixStart) + 4;\n                if (depth==0) goto _storeSequence;\n        }   }\n\n        /* first search (depth 0) */\n        {   size_t offsetFound = 99999999;\n            size_t const ml2 = searchMax(ctx, ip, iend, &offsetFound, maxSearches, mls);\n            if (ml2 > matchLength)\n                matchLength = ml2, start = ip, offset=offsetFound;\n        }\n\n         if (matchLength < 4) {\n            ip += ((ip-anchor) >> g_searchStrength) + 1;   /* jump faster over incompressible sections */\n            continue;\n        }\n\n        /* let's try to find a better solution */\n        if (depth>=1)\n        while (ip<ilimit) {\n            ip ++;\n            current++;\n            /* check repCode */\n            if (offset) {\n                const U32 repIndex = (U32)(current - offset_1);\n                const BYTE* const repBase = repIndex < dictLimit ? dictBase : base;\n                const BYTE* const repMatch = repBase + repIndex;\n                if (((U32)((dictLimit-1) - repIndex) >= 3) & (repIndex > lowestIndex))  /* intentional overflow */\n                if (MEM_read32(ip) == MEM_read32(repMatch)) {\n                    /* repcode detected */\n                    const BYTE* const repEnd = repIndex < dictLimit ? dictEnd : iend;\n                    size_t const repLength = ZSTD_count_2segments(ip+4, repMatch+4, iend, repEnd, prefixStart) + 4;\n                    int const gain2 = (int)(repLength * 3);\n                    int const gain1 = (int)(matchLength*3 - ZSTD_highbit32((U32)offset+1) + 1);\n                    if ((repLength >= 4) && (gain2 > gain1))\n                        matchLength = repLength, offset = 0, start = ip;\n            }   }\n\n            /* search match, depth 1 */\n            {   size_t offset2=99999999;\n                size_t const ml2 = searchMax(ctx, ip, iend, &offset2, maxSearches, mls);\n                int const gain2 = (int)(ml2*4 - ZSTD_highbit32((U32)offset2+1));   /* raw approx */\n                int const gain1 = (int)(matchLength*4 - ZSTD_highbit32((U32)offset+1) + 4);\n                if ((ml2 >= 4) && (gain2 > gain1)) {\n                    matchLength = ml2, offset = offset2, start = ip;\n                    continue;   /* search a better one */\n            }   }\n\n            /* let's find an even better one */\n            if ((depth==2) && (ip<ilimit)) {\n                ip ++;\n                current++;\n                /* check repCode */\n                if (offset) {\n                    const U32 repIndex = (U32)(current - offset_1);\n                    const BYTE* const repBase = repIndex < dictLimit ? dictBase : base;\n                    const BYTE* const repMatch = repBase + repIndex;\n                    if (((U32)((dictLimit-1) - repIndex) >= 3) & (repIndex > lowestIndex))  /* intentional overflow */\n                    if (MEM_read32(ip) == MEM_read32(repMatch)) {\n                        /* repcode detected */\n                        const BYTE* const repEnd = repIndex < dictLimit ? dictEnd : iend;\n                        size_t const repLength = ZSTD_count_2segments(ip+4, repMatch+4, iend, repEnd, prefixStart) + 4;\n                        int const gain2 = (int)(repLength * 4);\n                        int const gain1 = (int)(matchLength*4 - ZSTD_highbit32((U32)offset+1) + 1);\n                        if ((repLength >= 4) && (gain2 > gain1))\n                            matchLength = repLength, offset = 0, start = ip;\n                }   }\n\n                /* search match, depth 2 */\n                {   size_t offset2=99999999;\n                    size_t const ml2 = searchMax(ctx, ip, iend, &offset2, maxSearches, mls);\n                    int const gain2 = (int)(ml2*4 - ZSTD_highbit32((U32)offset2+1));   /* raw approx */\n                    int const gain1 = (int)(matchLength*4 - ZSTD_highbit32((U32)offset+1) + 7);\n                    if ((ml2 >= 4) && (gain2 > gain1)) {\n                        matchLength = ml2, offset = offset2, start = ip;\n                        continue;\n            }   }   }\n            break;  /* nothing found : store previous solution */\n        }\n\n        /* catch up */\n        if (offset) {\n            U32 const matchIndex = (U32)((start-base) - (offset - ZSTD_REP_MOVE));\n            const BYTE* match = (matchIndex < dictLimit) ? dictBase + matchIndex : base + matchIndex;\n            const BYTE* const mStart = (matchIndex < dictLimit) ? dictStart : prefixStart;\n            while ((start>anchor) && (match>mStart) && (start[-1] == match[-1])) { start--; match--; matchLength++; }  /* catch up */\n            offset_2 = offset_1; offset_1 = (U32)(offset - ZSTD_REP_MOVE);\n        }\n\n        /* store sequence */\n_storeSequence:\n        {   size_t const litLength = start - anchor;\n            ZSTD_storeSeq(seqStorePtr, litLength, anchor, (U32)offset, matchLength-MINMATCH);\n            anchor = ip = start + matchLength;\n        }\n\n        /* check immediate repcode */\n        while (ip <= ilimit) {\n            const U32 repIndex = (U32)((ip-base) - offset_2);\n            const BYTE* const repBase = repIndex < dictLimit ? dictBase : base;\n            const BYTE* const repMatch = repBase + repIndex;\n            if (((U32)((dictLimit-1) - repIndex) >= 3) & (repIndex > lowestIndex))  /* intentional overflow */\n            if (MEM_read32(ip) == MEM_read32(repMatch)) {\n                /* repcode detected we should take it */\n                const BYTE* const repEnd = repIndex < dictLimit ? dictEnd : iend;\n                matchLength = ZSTD_count_2segments(ip+4, repMatch+4, iend, repEnd, prefixStart) + 4;\n                offset = offset_2; offset_2 = offset_1; offset_1 = (U32)offset;   /* swap offset history */\n                ZSTD_storeSeq(seqStorePtr, 0, anchor, 0, matchLength-MINMATCH);\n                ip += matchLength;\n                anchor = ip;\n                continue;   /* faster when present ... (?) */\n            }\n            break;\n    }   }\n\n    /* Save reps for next block */\n    seqStorePtr->repToConfirm[0] = offset_1; seqStorePtr->repToConfirm[1] = offset_2;\n\n    /* Last Literals */\n    {   size_t const lastLLSize = iend - anchor;\n        memcpy(seqStorePtr->lit, anchor, lastLLSize);\n        seqStorePtr->lit += lastLLSize;\n    }\n}\n\n\nvoid ZSTD_compressBlock_greedy_extDict(ZSTD_CCtx* ctx, const void* src, size_t srcSize)\n{\n    ZSTD_compressBlock_lazy_extDict_generic(ctx, src, srcSize, 0, 0);\n}\n\nstatic void ZSTD_compressBlock_lazy_extDict(ZSTD_CCtx* ctx, const void* src, size_t srcSize)\n{\n    ZSTD_compressBlock_lazy_extDict_generic(ctx, src, srcSize, 0, 1);\n}\n\nstatic void ZSTD_compressBlock_lazy2_extDict(ZSTD_CCtx* ctx, const void* src, size_t srcSize)\n{\n    ZSTD_compressBlock_lazy_extDict_generic(ctx, src, srcSize, 0, 2);\n}\n\nstatic void ZSTD_compressBlock_btlazy2_extDict(ZSTD_CCtx* ctx, const void* src, size_t srcSize)\n{\n    ZSTD_compressBlock_lazy_extDict_generic(ctx, src, srcSize, 1, 2);\n}\n\n\n/* The optimal parser */\n#include \"zstd_opt.h\"\n\nstatic void ZSTD_compressBlock_btopt(ZSTD_CCtx* ctx, const void* src, size_t srcSize)\n{\n#ifdef ZSTD_OPT_H_91842398743\n    ZSTD_compressBlock_opt_generic(ctx, src, srcSize, 0);\n#else\n    (void)ctx; (void)src; (void)srcSize;\n    return;\n#endif\n}\n\nstatic void ZSTD_compressBlock_btultra(ZSTD_CCtx* ctx, const void* src, size_t srcSize)\n{\n#ifdef ZSTD_OPT_H_91842398743\n    ZSTD_compressBlock_opt_generic(ctx, src, srcSize, 1);\n#else\n    (void)ctx; (void)src; (void)srcSize;\n    return;\n#endif\n}\n\nstatic void ZSTD_compressBlock_btopt_extDict(ZSTD_CCtx* ctx, const void* src, size_t srcSize)\n{\n#ifdef ZSTD_OPT_H_91842398743\n    ZSTD_compressBlock_opt_extDict_generic(ctx, src, srcSize, 0);\n#else\n    (void)ctx; (void)src; (void)srcSize;\n    return;\n#endif\n}\n\nstatic void ZSTD_compressBlock_btultra_extDict(ZSTD_CCtx* ctx, const void* src, size_t srcSize)\n{\n#ifdef ZSTD_OPT_H_91842398743\n    ZSTD_compressBlock_opt_extDict_generic(ctx, src, srcSize, 1);\n#else\n    (void)ctx; (void)src; (void)srcSize;\n    return;\n#endif\n}\n\n",
        "res_region": "",
        "lookback": " * assumption : strat is a valid strategy */",
        "lookahead": "/* ZSTD_selectBlockCompressor() :",
        "label": [
            "base",
            "base_empty"
        ],
        "start_line": 1684,
        "end_line": 4707,
        "merge_gen_region": null,
        "confidence": 0.877953052520752,
        "hesitated": true
    }
]