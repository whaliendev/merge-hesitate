[
    {
        "a_contents": "/******************************************************************************\n * Copyright 2022 The Apollo Authors. All Rights Reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *****************************************************************************/\n#include \"modules/perception/lidar/lib/detector/center_point_detection/center_point_detection.h\"\n\n#include <algorithm>\n#include <functional>\n#include <numeric>\n#include <random>\n\n#include <cuda_runtime_api.h>\n\n#include \"cyber/common/file.h\"\n#include \"cyber/common/log.h\"\n#include \"modules/perception/base/object_pool_types.h\"\n#include \"modules/perception/base/point_cloud_util.h\"\n#include \"modules/perception/common/perception_gflags.h\"\n#include \"modules/perception/lidar/common/lidar_timer.h\"\n#include \"modules/perception/lidar/common/pcl_util.h\"\n#include \"modules/perception/lidar/lib/detector/center_point_detection/params.h\"\n\nnamespace apollo {\nnamespace perception {\nnamespace lidar {\n\nusing base::Object;\nusing base::PointD;\nusing base::PointF;\n\n// point cloud range\nCenterPointDetection::CenterPointDetection()\n    : x_min_range_(Params::kMinXRange),\n      x_max_range_(Params::kMaxXRange),\n      y_min_range_(Params::kMinYRange),\n      y_max_range_(Params::kMaxYRange),\n      z_min_range_(Params::kMinZRange),\n      z_max_range_(Params::kMaxZRange) {\n  if (FLAGS_enable_ground_removal) {\n    z_min_range_ =\n        std::max(z_min_range_, static_cast<float>(FLAGS_ground_removal_height));\n  }\n}\n\nbool CenterPointDetection::Init(const LidarDetectorInitOptions &options) {\n  /*\n  num_point_feature\n  */\n  paddle::AnalysisConfig config;\n  config.EnableUseGpu(1000, FLAGS_gpu_id);\n  config.SetModel(FLAGS_center_point_model_file,\n                  FLAGS_center_point_params_file);\n  if (FLAGS_use_trt) {\n    paddle::AnalysisConfig::Precision precision;\n    if (FLAGS_trt_precision == 0) {\n      precision = paddle_infer::PrecisionType::kFloat32;\n    } else if (FLAGS_trt_precision == 1) {\n      precision = paddle_infer::PrecisionType::kHalf;\n    } else {\n      AERROR << \"Tensorrt type can only support 0 or 1, but recieved is\"\n             << FLAGS_trt_precision << \"\\n\";\n      return false;\n    }\n    config.EnableTensorRtEngine(1 << 30, 1, 3, precision, FLAGS_trt_use_static,\n                                false);\n    // todo: solve EnableTunedTensorRtDynamicShape\n    config.CollectShapeRangeInfo(FLAGS_dynamic_shape_file);\n    // config.EnableTunedTensorRtDynamicShape(FLAGS_dynamic_shape_file, true);\n\n    if (FLAGS_trt_use_static) {\n      config.SetOptimCacheDir(FLAGS_trt_static_dir);\n    }\n  }\n  config.SwitchIrOptim(true);\n\n  predictor_ = paddle_infer::CreatePredictor(config);\n  return true;\n}\n\nbool CenterPointDetection::Init(const StageConfig& stage_config) {\n  if (!Initialize(stage_config)) {\n    return false;\n  }\n\n  /*\n  num_point_feature\n  */\n  paddle::AnalysisConfig config;\n  config.EnableUseGpu(1000, FLAGS_gpu_id);\n  config.SetModel(FLAGS_center_point_model_file,\n                  FLAGS_center_point_params_file);\n  if (FLAGS_use_trt) {\n    paddle::AnalysisConfig::Precision precision;\n    if (FLAGS_trt_precision == 0) {\n      precision = paddle_infer::PrecisionType::kFloat32;\n    } else if (FLAGS_trt_precision == 1) {\n      precision = paddle_infer::PrecisionType::kHalf;\n    } else {\n      AERROR << \"Tensorrt type can only support 0 or 1, but recieved is\"\n             << FLAGS_trt_precision << \"\\n\";\n      return false;\n    }\n    config.EnableTensorRtEngine(1 << 30, 1, 3, precision, FLAGS_trt_use_static,\n                                false);\n    // todo: solve EnableTunedTensorRtDynamicShape\n    config.CollectShapeRangeInfo(FLAGS_dynamic_shape_file);\n    // config.EnableTunedTensorRtDynamicShape(FLAGS_dynamic_shape_file, true);\n\n    if (FLAGS_trt_use_static) {\n      config.SetOptimCacheDir(FLAGS_trt_static_dir);\n    }\n  }\n  config.SwitchIrOptim(true);\n\n  predictor_ = paddle_infer::CreatePredictor(config);\n  return true;\n}\n\nbool CenterPointDetection::Process(DataFrame* data_frame) {\n  if (data_frame == nullptr)\n    return false;\n\n  LidarFrame* lidar_frame = data_frame->lidar_frame;\n  if (lidar_frame == nullptr)\n    return false;\n\n  LidarDetectorOptions options;\n  bool res = Detect(options, lidar_frame);\n  return res;\n}\n\nbool CenterPointDetection::Detect(const LidarDetectorOptions &options,\n                                  LidarFrame *frame) {\n  // check input\n  if (frame == nullptr) {\n    AERROR << \"Input null frame ptr.\";\n    return false;\n  }\n  if (frame->cloud == nullptr) {\n    AERROR << \"Input null frame cloud.\";\n    return false;\n  }\n  if (frame->cloud->size() == 0) {\n    AERROR << \"Input none points.\";\n    return false;\n  }\n\n  // record input cloud and lidar frame\n  original_cloud_ = frame->cloud;\n  original_world_cloud_ = frame->world_cloud;\n  lidar_frame_ref_ = frame;\n\n  // check output\n  frame->segmented_objects.clear();\n\n  if (cudaSetDevice(FLAGS_gpu_id) != cudaSuccess) {\n    AERROR << \"Failed to set device to gpu \" << FLAGS_gpu_id;\n    return false;\n  }\n\n  Timer timer;\n\n  int num_points;\n  cur_cloud_ptr_ = std::shared_ptr<base::PointFCloud>(\n      new base::PointFCloud(*original_cloud_));\n\n  // down sample the point cloud through filtering beams\n  if (FLAGS_enable_downsample_beams) {\n    base::PointFCloudPtr downsample_beams_cloud_ptr(new base::PointFCloud());\n    if (DownSamplePointCloudBeams(original_cloud_, downsample_beams_cloud_ptr,\n                                  FLAGS_downsample_beams_factor)) {\n      cur_cloud_ptr_ = downsample_beams_cloud_ptr;\n    } else {\n      AWARN << \"Down-sample beams factor must be >= 1. Cancel down-sampling.\"\n               \" Current factor: \"\n            << FLAGS_downsample_beams_factor;\n    }\n  }\n\n  // down sample the point cloud through filtering voxel grid\n  if (FLAGS_enable_downsample_pointcloud) {\n    pcl::PointCloud<pcl::PointXYZI>::Ptr pcl_cloud_ptr(\n        new pcl::PointCloud<pcl::PointXYZI>());\n    pcl::PointCloud<pcl::PointXYZI>::Ptr filtered_cloud_ptr(\n        new pcl::PointCloud<pcl::PointXYZI>());\n    TransformToPCLXYZI(*cur_cloud_ptr_, pcl_cloud_ptr);\n    DownSampleCloudByVoxelGrid(\n        pcl_cloud_ptr, filtered_cloud_ptr, FLAGS_downsample_voxel_size_x,\n        FLAGS_downsample_voxel_size_y, FLAGS_downsample_voxel_size_z);\n\n    // transform pcl point cloud to apollo point cloud\n    base::PointFCloudPtr downsample_voxel_cloud_ptr(new base::PointFCloud());\n    TransformFromPCLXYZI(filtered_cloud_ptr, downsample_voxel_cloud_ptr);\n    cur_cloud_ptr_ = downsample_voxel_cloud_ptr;\n  }\n  downsample_time_ = timer.toc(true);\n\n  num_points = cur_cloud_ptr_->size();\n  AINFO << \"num points before fusing: \" << num_points;\n\n  // fuse clouds of preceding frames with current cloud\n  cur_cloud_ptr_->mutable_points_timestamp()->assign(cur_cloud_ptr_->size(),\n                                                     0.0);\n  if (FLAGS_enable_fuse_frames && FLAGS_num_fuse_frames > 1) {\n    // before fusing\n    while (!prev_world_clouds_.empty() &&\n           frame->timestamp - prev_world_clouds_.front()->get_timestamp() >\n               FLAGS_fuse_time_interval) {\n      prev_world_clouds_.pop_front();\n    }\n    // transform current cloud to world coordinate and save to a new ptr\n    base::PointDCloudPtr cur_world_cloud_ptr =\n        std::make_shared<base::PointDCloud>();\n    for (size_t i = 0; i < cur_cloud_ptr_->size(); ++i) {\n      auto &pt = cur_cloud_ptr_->at(i);\n      Eigen::Vector3d trans_point(pt.x, pt.y, pt.z);\n      trans_point = lidar_frame_ref_->lidar2world_pose * trans_point;\n      PointD world_point;\n      world_point.x = trans_point(0);\n      world_point.y = trans_point(1);\n      world_point.z = trans_point(2);\n      world_point.intensity = pt.intensity;\n      cur_world_cloud_ptr->push_back(world_point);\n    }\n    cur_world_cloud_ptr->set_timestamp(frame->timestamp);\n\n    // fusing clouds\n    for (auto &prev_world_cloud_ptr : prev_world_clouds_) {\n      num_points += prev_world_cloud_ptr->size();\n    }\n    FuseCloud(cur_cloud_ptr_, prev_world_clouds_);\n\n    // after fusing\n    while (static_cast<int>(prev_world_clouds_.size()) >=\n           FLAGS_num_fuse_frames - 1) {\n      prev_world_clouds_.pop_front();\n    }\n    prev_world_clouds_.emplace_back(cur_world_cloud_ptr);\n  }\n  fuse_time_ = timer.toc(true);\n\n  // shuffle points and cut off\n  if (FLAGS_enable_shuffle_points) {\n    num_points = std::min(num_points, FLAGS_max_num_points);\n    std::vector<int> point_indices = GenerateIndices(0, num_points, true);\n    base::PointFCloudPtr shuffle_cloud_ptr(\n        new base::PointFCloud(*cur_cloud_ptr_, point_indices));\n    cur_cloud_ptr_ = shuffle_cloud_ptr;\n  }\n  shuffle_time_ = timer.toc(true);\n\n  // points_array[x, y, z, i,timestampe, ......]\n  std::vector<float> points_data(num_points * FLAGS_num_point_feature);\n  CloudToArray(cur_cloud_ptr_, points_data.data(), FLAGS_normalizing_factor);\n  cloud_to_array_time_ = timer.toc(true);\n\n  // paddle inference\n  std::vector<float> out_detections;\n  std::vector<int64_t> out_labels;\n  std::vector<float> out_scores;\n  std::vector<float> out_detections_final;\n  std::vector<int64_t> out_labels_final;\n\n  DoInference(points_data, num_points, &out_detections, &out_labels,\n              &out_scores);\n\n  FilterScore(&out_detections, &out_labels, &out_scores, FLAGS_score_threshold,\n               &out_detections_final, &out_labels_final);\n\n  GetObjects(&frame->segmented_objects, frame->lidar2world_pose,\n             &out_detections_final, &out_labels_final);\n  inference_time_ = timer.toc(true);\n\n  AINFO << \"CenterPoint: \"\n        << \"\\n\"\n        << \"down sample: \" << downsample_time_ << \"\\t\"\n        << \"fuse: \" << fuse_time_ << \"\\t\"\n        << \"shuffle: \" << shuffle_time_ << \"\\t\"\n        << \"cloud_to_array: \" << cloud_to_array_time_ << \"\\t\"\n        << \"inference: \" << inference_time_ << \"\\t\";\n  return true;\n}\n\n// normalizing_factor: Normalize intensity range to [0, 1] by this factor\nvoid CenterPointDetection::CloudToArray(const base::PointFCloudPtr &pc_ptr,\n                                        float *out_points_array,\n                                        const float normalizing_factor) {\n  for (size_t i = 0; i < pc_ptr->size(); ++i) {\n    const auto &point = pc_ptr->at(i);\n    float x = point.x;\n    float y = point.y;\n    float z = point.z;\n    float intensity = point.intensity;\n    if (z < z_min_range_ || z > z_max_range_ || y < y_min_range_ ||\n        y > y_max_range_ || x < x_min_range_ || x > x_max_range_) {\n      continue;\n    }\n    out_points_array[i * FLAGS_num_point_feature + 0] = x;\n    out_points_array[i * FLAGS_num_point_feature + 1] = y;\n    out_points_array[i * FLAGS_num_point_feature + 2] = z;\n    out_points_array[i * FLAGS_num_point_feature + 3] =\n        intensity / normalizing_factor;\n    // delta of timestamp between prev and cur frames\n    out_points_array[i * FLAGS_num_point_feature + 4] =\n        static_cast<float>(pc_ptr->points_timestamp(i));\n  }\n}\n\nvoid CenterPointDetection::FuseCloud(\n    const base::PointFCloudPtr &out_cloud_ptr,\n    const std::deque<base::PointDCloudPtr> &fuse_clouds) {\n  for (auto iter = fuse_clouds.rbegin(); iter != fuse_clouds.rend(); ++iter) {\n    double delta_t = lidar_frame_ref_->timestamp - (*iter)->get_timestamp();\n    // transform prev world point cloud to current sensor's coordinates\n    for (size_t i = 0; i < (*iter)->size(); ++i) {\n      auto &point = (*iter)->at(i);\n      Eigen::Vector3d trans_point(point.x, point.y, point.z);\n      trans_point = lidar_frame_ref_->lidar2world_pose.inverse() * trans_point;\n      base::PointF pt;\n      pt.x = static_cast<float>(trans_point(0));\n      pt.y = static_cast<float>(trans_point(1));\n      pt.z = static_cast<float>(trans_point(2));\n      pt.intensity = static_cast<float>(point.intensity);\n      // delta of time between current and prev frame\n      out_cloud_ptr->push_back(pt, delta_t);\n    }\n  }\n}\n\nvoid CenterPointDetection::DoInference(const std::vector<float> &points_data,\n                                       const int in_num_points,\n                                       std::vector<float> *out_detections,\n                                       std::vector<int64_t> *out_labels,\n                                       std::vector<float> *out_scores) {\n  // todo: check gpu_id\n  std::vector<int> points_shape;\n  points_shape.push_back(in_num_points);\n  points_shape.push_back(FLAGS_num_point_feature);\n\n  Run(predictor_.get(), points_shape, points_data, out_detections, out_labels,\n      out_scores);\n}\n\nstd::vector<int> CenterPointDetection::GenerateIndices(int start_index,\n                                                       int size, bool shuffle) {\n  // create a range number array\n  std::vector<int> indices(size);\n  std::iota(indices.begin(), indices.end(), start_index);\n\n  // shuffle the index array\n  if (shuffle) {\n    unsigned seed = 0;\n    std::shuffle(indices.begin(), indices.end(),\n                 std::default_random_engine(seed));\n  }\n  return indices;\n}\n\nvoid CenterPointDetection::Run(paddle_infer::Predictor *predictor,\n                               const std::vector<int> &points_shape,\n                               const std::vector<float> &points_data,\n                               std::vector<float> *box3d_lidar,\n                               std::vector<int64_t> *label_preds,\n                               std::vector<float> *scores) {\n  auto input_names = predictor->GetInputNames();\n  for (const auto &tensor_name : input_names) {\n    auto in_tensor = predictor->GetInputHandle(tensor_name);\n    if (tensor_name == \"data\") {\n      in_tensor->Reshape(points_shape);\n      in_tensor->CopyFromCpu(points_data.data());\n    }\n  }\n  ACHECK(predictor->Run());\n\n  auto output_names = predictor->GetOutputNames();\n  for (size_t i = 0; i != output_names.size(); i++) {\n    auto output = predictor->GetOutputHandle(output_names[i]);\n    std::vector<int> output_shape = output->shape();\n    int out_num = std::accumulate(output_shape.begin(), output_shape.end(), 1,\n                                  std::multiplies<int>());\n    if (i == 0) {\n      box3d_lidar->resize(out_num);\n      output->CopyToCpu(box3d_lidar->data());\n    } else if (i == 1) {\n      label_preds->resize(out_num);\n      output->CopyToCpu(label_preds->data());\n    } else if (i == 2) {\n      scores->resize(out_num);\n      output->CopyToCpu(scores->data());\n    }\n  }\n}\n\nvoid CenterPointDetection::GetObjects(\n    std::vector<std::shared_ptr<Object>> *objects, const Eigen::Affine3d &pose,\n    std::vector<float> *detections, std::vector<int64_t> *labels) {\n  int num_objects = detections->size() / num_output_box_feature_;\n\n  objects->clear();\n  base::ObjectPool::Instance().BatchGet(num_objects, objects);\n\n  for (int i = 0; i < num_objects; ++i) {\n    auto &object = objects->at(i);\n    object->id = i;\n\n    // no velocity\n    float x = detections->at(i * FLAGS_num_output_box_feature + 0);\n    float y = detections->at(i * FLAGS_num_output_box_feature + 1);\n    float z = detections->at(i * FLAGS_num_output_box_feature + 2);\n    float dx = detections->at(i * FLAGS_num_output_box_feature + 3);\n    float dy = detections->at(i * FLAGS_num_output_box_feature + 4);\n    float dz = detections->at(i * FLAGS_num_output_box_feature + 5);\n    float yaw = detections->at(i * FLAGS_num_output_box_feature + 6);\n    // yaw += M_PI / 2;\n    yaw = std::atan2(sinf(yaw), cosf(yaw));\n    yaw = -yaw;\n\n    // directions\n    object->theta = yaw;\n    object->direction[0] = cosf(yaw);\n    object->direction[1] = sinf(yaw);\n    object->direction[2] = 0;\n    object->lidar_supplement.is_orientation_ready = true;\n\n    // compute vertexes of bounding box and transform to world coordinate\n    object->lidar_supplement.num_points_in_roi = 8;\n    object->lidar_supplement.on_use = true;\n    object->lidar_supplement.is_background = false;\n    float roll = 0, pitch = 0;\n    Eigen::Quaternionf quater =\n        Eigen::AngleAxisf(roll, Eigen::Vector3f::UnitX()) *\n        Eigen::AngleAxisf(pitch, Eigen::Vector3f::UnitY()) *\n        Eigen::AngleAxisf(yaw, Eigen::Vector3f::UnitZ());\n    Eigen::Translation3f translation(x, y, z);\n    Eigen::Affine3f affine3f = translation * quater.toRotationMatrix();\n    for (float vx : std::vector<float>{dx / 2, -dx / 2}) {\n      for (float vy : std::vector<float>{dy / 2, -dy / 2}) {\n        for (float vz : std::vector<float>{0, dz}) {\n          Eigen::Vector3f v3f(vx, vy, vz);\n          v3f = affine3f * v3f;\n          PointF point;\n          point.x = v3f.x();\n          point.y = v3f.y();\n          point.z = v3f.z();\n          object->lidar_supplement.cloud.push_back(point);\n\n          Eigen::Vector3d trans_point(point.x, point.y, point.z);\n          trans_point = pose * trans_point;\n          PointD world_point;\n          world_point.x = trans_point(0);\n          world_point.y = trans_point(1);\n          world_point.z = trans_point(2);\n          object->lidar_supplement.cloud_world.push_back(world_point);\n        }\n      }\n    }\n\n    // classification\n    object->lidar_supplement.raw_probs.push_back(std::vector<float>(\n        static_cast<int>(base::ObjectType::MAX_OBJECT_TYPE), 0.f));\n    object->lidar_supplement.raw_classification_methods.push_back(Name());\n    object->sub_type = GetObjectSubType(labels->at(i));\n    object->type = base::kSubType2TypeMap.at(object->sub_type);\n    object->lidar_supplement.raw_probs.back()[static_cast<int>(object->type)] =\n        1.0f;\n    // copy to type\n    object->type_probs.assign(object->lidar_supplement.raw_probs.back().begin(),\n                              object->lidar_supplement.raw_probs.back().end());\n  }\n}\n\nbase::ObjectSubType CenterPointDetection::GetObjectSubType(const int label) {\n  switch (label) {\n    case 0:\n      return base::ObjectSubType::CAR;\n    case 1:\n      return base::ObjectSubType::TRUCK;\n    case 3:\n      return base::ObjectSubType::BUS;\n    case 6:\n      return base::ObjectSubType::MOTORCYCLIST;\n    case 7:\n      return base::ObjectSubType::CYCLIST;\n    case 8:\n      return base::ObjectSubType::PEDESTRIAN;\n    case 9:\n      return base::ObjectSubType::TRAFFICCONE;\n    default:\n      return base::ObjectSubType::UNKNOWN;\n  }\n}\n\nvoid CenterPointDetection::FilterScore(\n    const std::vector<float> *box3d_lidar,\n    const std::vector<int64_t> *label_preds, const std::vector<float> *scores,\n    const float score_threshold, std::vector<float> *box3d_lidar_final,\n    std::vector<int64_t> *label_preds_final) {\n  for (size_t i = 0; i < scores->size(); i++) {\n    if (scores->at(i) > score_threshold) {\n      box3d_lidar_final->insert(\n          box3d_lidar_final->end(),\n          box3d_lidar->begin() + num_output_box_feature_ * i,\n          box3d_lidar->begin() + num_output_box_feature_ * (i + 1));\n      label_preds_final->insert(label_preds_final->end(),\n                                *(label_preds->begin() + i));\n    }\n  }\n}\n\nPERCEPTION_REGISTER_LIDARDETECTOR(CenterPointDetection);\n\n}  // namespace lidar\n}  // namespace perception\n}  // namespace apollo",
        "b_contents": "/******************************************************************************\n * Copyright 2022 The Apollo Authors. All Rights Reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *****************************************************************************/\n#include \"modules/perception/lidar/lib/detector/center_point_detection/center_point_detection.h\"\n\n#include <algorithm>\n#include <functional>\n#include <numeric>\n#include <random>\n\n#include <cuda_runtime_api.h>\n\n#include \"cyber/common/file.h\"\n#include \"cyber/common/log.h\"\n#include \"modules/perception/base/object_pool_types.h\"\n#include \"modules/perception/base/point_cloud_util.h\"\n#include \"modules/perception/common/perception_gflags.h\"\n#include \"modules/perception/lidar/common/lidar_timer.h\"\n#include \"modules/perception/lidar/common/pcl_util.h\"\n#include \"modules/perception/lidar/lib/detector/center_point_detection/params.h\"\n\nnamespace apollo {\nnamespace perception {\nnamespace lidar {\n\nusing base::Object;\nusing base::PointD;\nusing base::PointF;\n\n// point cloud range\nCenterPointDetection::CenterPointDetection()\n    : x_min_range_(Params::kMinXRange),\n      x_max_range_(Params::kMaxXRange),\n      y_min_range_(Params::kMinYRange),\n      y_max_range_(Params::kMaxYRange),\n      z_min_range_(Params::kMinZRange),\n      z_max_range_(Params::kMaxZRange) {\n  if (FLAGS_enable_ground_removal) {\n    z_min_range_ =\n        std::max(z_min_range_, static_cast<float>(FLAGS_ground_removal_height));\n  }\n}\n\nbool CenterPointDetection::Init(const LidarDetectorInitOptions &options) {\n  /*\n  num_point_feature\n  */\n  paddle::AnalysisConfig config;\n  config.EnableUseGpu(1000, FLAGS_gpu_id);\n  config.SetModel(FLAGS_center_point_model_file,\n                  FLAGS_center_point_params_file);\n  if (FLAGS_use_trt) {\n    paddle::AnalysisConfig::Precision precision;\n    if (FLAGS_trt_precision == 0) {\n      precision = paddle_infer::PrecisionType::kFloat32;\n    } else if (FLAGS_trt_precision == 1) {\n      precision = paddle_infer::PrecisionType::kHalf;\n    } else {\n      AERROR << \"Tensorrt type can only support 0 or 1, but recieved is\"\n             << FLAGS_trt_precision << \"\\n\";\n      return false;\n    }\n    config.EnableTensorRtEngine(1 << 30, 1, 3, precision, FLAGS_trt_use_static,\n                                false);\n    // todo: solve EnableTunedTensorRtDynamicShape\n    config.CollectShapeRangeInfo(FLAGS_dynamic_shape_file);\n    // config.EnableTunedTensorRtDynamicShape(FLAGS_dynamic_shape_file, true);\n\n    if (FLAGS_trt_use_static) {\n      config.SetOptimCacheDir(FLAGS_trt_static_dir);\n    }\n  }\n  config.SwitchIrOptim(true);\n\n  predictor_ = paddle_infer::CreatePredictor(config);\n  return true;\n}\n\nbool CenterPointDetection::Detect(const LidarDetectorOptions &options,\n                                  LidarFrame *frame) {\n  // check input\n  if (frame == nullptr) {\n    AERROR << \"Input null frame ptr.\";\n    return false;\n  }\n  if (frame->cloud == nullptr) {\n    AERROR << \"Input null frame cloud.\";\n    return false;\n  }\n  if (frame->cloud->size() == 0) {\n    AERROR << \"Input none points.\";\n    return false;\n  }\n\n  // record input cloud and lidar frame\n  original_cloud_ = frame->cloud;\n  original_world_cloud_ = frame->world_cloud;\n  lidar_frame_ref_ = frame;\n\n  // check output\n  frame->segmented_objects.clear();\n\n  if (cudaSetDevice(FLAGS_gpu_id) != cudaSuccess) {\n    AERROR << \"Failed to set device to gpu \" << FLAGS_gpu_id;\n    return false;\n  }\n\n  Timer timer;\n\n  int num_points;\n  cur_cloud_ptr_ = std::shared_ptr<base::PointFCloud>(\n      new base::PointFCloud(*original_cloud_));\n\n  // down sample the point cloud through filtering beams\n  if (FLAGS_enable_downsample_beams) {\n    base::PointFCloudPtr downsample_beams_cloud_ptr(new base::PointFCloud());\n    if (DownSamplePointCloudBeams(original_cloud_, downsample_beams_cloud_ptr,\n                                  FLAGS_downsample_beams_factor)) {\n      cur_cloud_ptr_ = downsample_beams_cloud_ptr;\n    } else {\n      AWARN << \"Down-sample beams factor must be >= 1. Cancel down-sampling.\"\n               \" Current factor: \"\n            << FLAGS_downsample_beams_factor;\n    }\n  }\n\n  // down sample the point cloud through filtering voxel grid\n  if (FLAGS_enable_downsample_pointcloud) {\n    pcl::PointCloud<pcl::PointXYZI>::Ptr pcl_cloud_ptr(\n        new pcl::PointCloud<pcl::PointXYZI>());\n    pcl::PointCloud<pcl::PointXYZI>::Ptr filtered_cloud_ptr(\n        new pcl::PointCloud<pcl::PointXYZI>());\n    TransformToPCLXYZI(*cur_cloud_ptr_, pcl_cloud_ptr);\n    DownSampleCloudByVoxelGrid(\n        pcl_cloud_ptr, filtered_cloud_ptr, FLAGS_downsample_voxel_size_x,\n        FLAGS_downsample_voxel_size_y, FLAGS_downsample_voxel_size_z);\n\n    // transform pcl point cloud to apollo point cloud\n    base::PointFCloudPtr downsample_voxel_cloud_ptr(new base::PointFCloud());\n    TransformFromPCLXYZI(filtered_cloud_ptr, downsample_voxel_cloud_ptr);\n    cur_cloud_ptr_ = downsample_voxel_cloud_ptr;\n  }\n  downsample_time_ = timer.toc(true);\n\n  num_points = cur_cloud_ptr_->size();\n  AINFO << \"num points before fusing: \" << num_points;\n\n  // fuse clouds of preceding frames with current cloud\n  cur_cloud_ptr_->mutable_points_timestamp()->assign(cur_cloud_ptr_->size(),\n                                                     0.0);\n  if (FLAGS_enable_fuse_frames && FLAGS_num_fuse_frames > 1) {\n    // before fusing\n    while (!prev_world_clouds_.empty() &&\n           frame->timestamp - prev_world_clouds_.front()->get_timestamp() >\n               FLAGS_fuse_time_interval) {\n      prev_world_clouds_.pop_front();\n    }\n    // transform current cloud to world coordinate and save to a new ptr\n    base::PointDCloudPtr cur_world_cloud_ptr =\n        std::make_shared<base::PointDCloud>();\n    for (size_t i = 0; i < cur_cloud_ptr_->size(); ++i) {\n      auto &pt = cur_cloud_ptr_->at(i);\n      Eigen::Vector3d trans_point(pt.x, pt.y, pt.z);\n      trans_point = lidar_frame_ref_->lidar2world_pose * trans_point;\n      PointD world_point;\n      world_point.x = trans_point(0);\n      world_point.y = trans_point(1);\n      world_point.z = trans_point(2);\n      world_point.intensity = pt.intensity;\n      cur_world_cloud_ptr->push_back(world_point);\n    }\n    cur_world_cloud_ptr->set_timestamp(frame->timestamp);\n\n    // fusing clouds\n    for (auto &prev_world_cloud_ptr : prev_world_clouds_) {\n      num_points += prev_world_cloud_ptr->size();\n    }\n    FuseCloud(cur_cloud_ptr_, prev_world_clouds_);\n\n    // after fusing\n    while (static_cast<int>(prev_world_clouds_.size()) >=\n           FLAGS_num_fuse_frames - 1) {\n      prev_world_clouds_.pop_front();\n    }\n    prev_world_clouds_.emplace_back(cur_world_cloud_ptr);\n  }\n  fuse_time_ = timer.toc(true);\n\n  // shuffle points and cut off\n  if (FLAGS_enable_shuffle_points) {\n    num_points = std::min(num_points, FLAGS_max_num_points);\n    std::vector<int> point_indices = GenerateIndices(0, num_points, true);\n    base::PointFCloudPtr shuffle_cloud_ptr(\n        new base::PointFCloud(*cur_cloud_ptr_, point_indices));\n    cur_cloud_ptr_ = shuffle_cloud_ptr;\n  }\n  shuffle_time_ = timer.toc(true);\n\n  // points_array[x, y, z, i,timestampe, ......]\n  std::vector<float> points_data(num_points * FLAGS_num_point_feature);\n  CloudToArray(cur_cloud_ptr_, points_data.data(), FLAGS_normalizing_factor);\n  cloud_to_array_time_ = timer.toc(true);\n\n  // paddle inference\n  std::vector<float> out_detections;\n  std::vector<int64_t> out_labels;\n  std::vector<float> out_scores;\n  std::vector<float> out_detections_final;\n  std::vector<int64_t> out_labels_final;\n\n  DoInference(points_data, num_points, &out_detections, &out_labels,\n              &out_scores);\n\n  FilterScore(&out_detections, &out_labels, &out_scores, FLAGS_score_threshold,\n               &out_detections_final, &out_labels_final);\n\n  GetObjects(&frame->segmented_objects, frame->lidar2world_pose,\n             &out_detections_final, &out_labels_final);\n  inference_time_ = timer.toc(true);\n\n  AINFO << \"CenterPoint: \"\n        << \"\\n\"\n        << \"down sample: \" << downsample_time_ << \"\\t\"\n        << \"fuse: \" << fuse_time_ << \"\\t\"\n        << \"shuffle: \" << shuffle_time_ << \"\\t\"\n        << \"cloud_to_array: \" << cloud_to_array_time_ << \"\\t\"\n        << \"inference: \" << inference_time_ << \"\\t\";\n  return true;\n}\n\n// normalizing_factor: Normalize intensity range to [0, 1] by this factor\nvoid CenterPointDetection::CloudToArray(const base::PointFCloudPtr &pc_ptr,\n                                        float *out_points_array,\n                                        const float normalizing_factor) {\n  for (size_t i = 0; i < pc_ptr->size(); ++i) {\n    const auto &point = pc_ptr->at(i);\n    float x = point.x;\n    float y = point.y;\n    float z = point.z;\n    float intensity = point.intensity;\n    if (z < z_min_range_ || z > z_max_range_ || y < y_min_range_ ||\n        y > y_max_range_ || x < x_min_range_ || x > x_max_range_) {\n      continue;\n    }\n    out_points_array[i * FLAGS_num_point_feature + 0] = x;\n    out_points_array[i * FLAGS_num_point_feature + 1] = y;\n    out_points_array[i * FLAGS_num_point_feature + 2] = z;\n    out_points_array[i * FLAGS_num_point_feature + 3] =\n        intensity / normalizing_factor;\n    // delta of timestamp between prev and cur frames\n    out_points_array[i * FLAGS_num_point_feature + 4] =\n        static_cast<float>(pc_ptr->points_timestamp(i));\n  }\n}\n\nvoid CenterPointDetection::FuseCloud(\n    const base::PointFCloudPtr &out_cloud_ptr,\n    const std::deque<base::PointDCloudPtr> &fuse_clouds) {\n  for (auto iter = fuse_clouds.rbegin(); iter != fuse_clouds.rend(); ++iter) {\n    double delta_t = lidar_frame_ref_->timestamp - (*iter)->get_timestamp();\n    // transform prev world point cloud to current sensor's coordinates\n    for (size_t i = 0; i < (*iter)->size(); ++i) {\n      auto &point = (*iter)->at(i);\n      Eigen::Vector3d trans_point(point.x, point.y, point.z);\n      trans_point = lidar_frame_ref_->lidar2world_pose.inverse() * trans_point;\n      base::PointF pt;\n      pt.x = static_cast<float>(trans_point(0));\n      pt.y = static_cast<float>(trans_point(1));\n      pt.z = static_cast<float>(trans_point(2));\n      pt.intensity = static_cast<float>(point.intensity);\n      // delta of time between current and prev frame\n      out_cloud_ptr->push_back(pt, delta_t);\n    }\n  }\n}\n\nvoid CenterPointDetection::DoInference(const std::vector<float> &points_data,\n                                       const int in_num_points,\n                                       std::vector<float> *out_detections,\n                                       std::vector<int64_t> *out_labels,\n                                       std::vector<float> *out_scores) {\n  // todo: check gpu_id\n  std::vector<int> points_shape;\n  points_shape.push_back(in_num_points);\n  points_shape.push_back(FLAGS_num_point_feature);\n\n  Run(predictor_.get(), points_shape, points_data, out_detections, out_labels,\n      out_scores);\n}\n\nstd::vector<int> CenterPointDetection::GenerateIndices(int start_index,\n                                                       int size, bool shuffle) {\n  // create a range number array\n  std::vector<int> indices(size);\n  std::iota(indices.begin(), indices.end(), start_index);\n\n  // shuffle the index array\n  if (shuffle) {\n    unsigned seed = 0;\n    std::shuffle(indices.begin(), indices.end(),\n                 std::default_random_engine(seed));\n  }\n  return indices;\n}\n\nvoid CenterPointDetection::Run(paddle_infer::Predictor *predictor,\n                               const std::vector<int> &points_shape,\n                               const std::vector<float> &points_data,\n                               std::vector<float> *box3d_lidar,\n                               std::vector<int64_t> *label_preds,\n                               std::vector<float> *scores) {\n  auto input_names = predictor->GetInputNames();\n  for (const auto &tensor_name : input_names) {\n    auto in_tensor = predictor->GetInputHandle(tensor_name);\n    if (tensor_name == \"data\") {\n      in_tensor->Reshape(points_shape);\n      in_tensor->CopyFromCpu(points_data.data());\n    }\n  }\n  ACHECK(predictor->Run());\n\n  auto output_names = predictor->GetOutputNames();\n  for (size_t i = 0; i != output_names.size(); i++) {\n    auto output = predictor->GetOutputHandle(output_names[i]);\n    std::vector<int> output_shape = output->shape();\n    int out_num = std::accumulate(output_shape.begin(), output_shape.end(), 1,\n                                  std::multiplies<int>());\n    if (i == 0) {\n      box3d_lidar->resize(out_num);\n      output->CopyToCpu(box3d_lidar->data());\n    } else if (i == 1) {\n      label_preds->resize(out_num);\n      output->CopyToCpu(label_preds->data());\n    } else if (i == 2) {\n      scores->resize(out_num);\n      output->CopyToCpu(scores->data());\n    }\n  }\n}\n\nvoid CenterPointDetection::GetObjects(\n    std::vector<std::shared_ptr<Object>> *objects, const Eigen::Affine3d &pose,\n    std::vector<float> *detections, std::vector<int64_t> *labels) {\n  int num_objects = detections->size() / num_output_box_feature_;\n\n  objects->clear();\n  base::ObjectPool::Instance().BatchGet(num_objects, objects);\n\n  for (int i = 0; i < num_objects; ++i) {\n    auto &object = objects->at(i);\n    object->id = i;\n\n    // no velocity\n    float x = detections->at(i * FLAGS_num_output_box_feature + 0);\n    float y = detections->at(i * FLAGS_num_output_box_feature + 1);\n    float z = detections->at(i * FLAGS_num_output_box_feature + 2);\n    float dx = detections->at(i * FLAGS_num_output_box_feature + 3);\n    float dy = detections->at(i * FLAGS_num_output_box_feature + 4);\n    float dz = detections->at(i * FLAGS_num_output_box_feature + 5);\n    float yaw = detections->at(i * FLAGS_num_output_box_feature + 6);\n    // yaw += M_PI / 2;\n    yaw = std::atan2(sinf(yaw), cosf(yaw));\n    yaw = -yaw;\n\n    // directions\n    object->theta = yaw;\n    object->direction[0] = cosf(yaw);\n    object->direction[1] = sinf(yaw);\n    object->direction[2] = 0;\n    object->lidar_supplement.is_orientation_ready = true;\n\n    // compute vertexes of bounding box and transform to world coordinate\n    object->lidar_supplement.num_points_in_roi = 8;\n    object->lidar_supplement.on_use = true;\n    object->lidar_supplement.is_background = false;\n    float roll = 0, pitch = 0;\n    Eigen::Quaternionf quater =\n        Eigen::AngleAxisf(roll, Eigen::Vector3f::UnitX()) *\n        Eigen::AngleAxisf(pitch, Eigen::Vector3f::UnitY()) *\n        Eigen::AngleAxisf(yaw, Eigen::Vector3f::UnitZ());\n    Eigen::Translation3f translation(x, y, z);\n    Eigen::Affine3f affine3f = translation * quater.toRotationMatrix();\n    for (float vx : std::vector<float>{dx / 2, -dx / 2}) {\n      for (float vy : std::vector<float>{dy / 2, -dy / 2}) {\n        for (float vz : std::vector<float>{0, dz}) {\n          Eigen::Vector3f v3f(vx, vy, vz);\n          v3f = affine3f * v3f;\n          PointF point;\n          point.x = v3f.x();\n          point.y = v3f.y();\n          point.z = v3f.z();\n          object->lidar_supplement.cloud.push_back(point);\n\n          Eigen::Vector3d trans_point(point.x, point.y, point.z);\n          trans_point = pose * trans_point;\n          PointD world_point;\n          world_point.x = trans_point(0);\n          world_point.y = trans_point(1);\n          world_point.z = trans_point(2);\n          object->lidar_supplement.cloud_world.push_back(world_point);\n        }\n      }\n    }\n\n    // classification\n    object->lidar_supplement.raw_probs.push_back(std::vector<float>(\n        static_cast<int>(base::ObjectType::MAX_OBJECT_TYPE), 0.f));\n    object->lidar_supplement.raw_classification_methods.push_back(Name());\n    object->sub_type = GetObjectSubType(labels->at(i));\n    object->type = base::kSubType2TypeMap.at(object->sub_type);\n    object->lidar_supplement.raw_probs.back()[static_cast<int>(object->type)] =\n        1.0f;\n    // copy to type\n    object->type_probs.assign(object->lidar_supplement.raw_probs.back().begin(),\n                              object->lidar_supplement.raw_probs.back().end());\n  }\n}\n\nbase::ObjectSubType CenterPointDetection::GetObjectSubType(const int label) {\n  switch (label) {\n    case 0:\n      return base::ObjectSubType::CAR;\n    case 1:\n      return base::ObjectSubType::TRUCK;\n    case 3:\n      return base::ObjectSubType::BUS;\n    case 6:\n      return base::ObjectSubType::MOTORCYCLIST;\n    case 7:\n      return base::ObjectSubType::CYCLIST;\n    case 8:\n      return base::ObjectSubType::PEDESTRIAN;\n    case 9:\n      return base::ObjectSubType::TRAFFICCONE;\n    default:\n      return base::ObjectSubType::UNKNOWN;\n  }\n}\n\nvoid CenterPointDetection::FilterScore(\n    const std::vector<float> *box3d_lidar,\n    const std::vector<int64_t> *label_preds, const std::vector<float> *scores,\n    const float score_threshold, std::vector<float> *box3d_lidar_final,\n    std::vector<int64_t> *label_preds_final) {\n  for (size_t i = 0; i < scores->size(); i++) {\n    if (scores->at(i) > score_threshold) {\n      box3d_lidar_final->insert(\n          box3d_lidar_final->end(),\n          box3d_lidar->begin() + num_output_box_feature_ * i,\n          box3d_lidar->begin() + num_output_box_feature_ * (i + 1));\n      label_preds_final->insert(label_preds_final->end(),\n                                *(label_preds->begin() + i));\n    }\n  }\n}\n\nPERCEPTION_REGISTER_LIDARDETECTOR(CenterPointDetection);\n\n}  // namespace lidar\n}  // namespace perception\n}  // namespace apollo",
        "base_contents": "",
        "res_region": "/******************************************************************************\n * Copyright 2022 The Apollo Authors. All Rights Reserved.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n *****************************************************************************/\n#include \"modules/perception/lidar/lib/detector/center_point_detection/center_point_detection.h\"\n\n#include <algorithm>\n#include <functional>\n#include <numeric>\n#include <random>\n\n#include <cuda_runtime_api.h>\n\n#include \"cyber/common/file.h\"\n#include \"cyber/common/log.h\"\n#include \"modules/perception/base/object_pool_types.h\"\n#include \"modules/perception/base/point_cloud_util.h\"\n#include \"modules/perception/common/perception_gflags.h\"\n#include \"modules/perception/lidar/common/lidar_timer.h\"\n#include \"modules/perception/lidar/common/pcl_util.h\"\n#include \"modules/perception/lidar/lib/detector/center_point_detection/params.h\"\n\nnamespace apollo {\nnamespace perception {\nnamespace lidar {\n\nusing base::Object;\nusing base::PointD;\nusing base::PointF;\n\n// point cloud range\nCenterPointDetection::CenterPointDetection()\n    : x_min_range_(Params::kMinXRange),\n      x_max_range_(Params::kMaxXRange),\n      y_min_range_(Params::kMinYRange),\n      y_max_range_(Params::kMaxYRange),\n      z_min_range_(Params::kMinZRange),\n      z_max_range_(Params::kMaxZRange) {\n  if (FLAGS_enable_ground_removal) {\n    z_min_range_ =\n        std::max(z_min_range_, static_cast<float>(FLAGS_ground_removal_height));\n  }\n}\n\nbool CenterPointDetection::Init(const LidarDetectorInitOptions &options) {\n  /*\n  num_point_feature\n  */\n  paddle::AnalysisConfig config;\n  config.EnableUseGpu(1000, FLAGS_gpu_id);\n  config.SetModel(FLAGS_center_point_model_file,\n                  FLAGS_center_point_params_file);\n  if (FLAGS_use_trt) {\n    paddle::AnalysisConfig::Precision precision;\n    if (FLAGS_trt_precision == 0) {\n      precision = paddle_infer::PrecisionType::kFloat32;\n    } else if (FLAGS_trt_precision == 1) {\n      precision = paddle_infer::PrecisionType::kHalf;\n    } else {\n      AERROR << \"Tensorrt type can only support 0 or 1, but recieved is\"\n             << FLAGS_trt_precision << \"\\n\";\n      return false;\n    }\n    config.EnableTensorRtEngine(1 << 30, 1, 3, precision, FLAGS_trt_use_static,\n                                false);\n    // todo: solve EnableTunedTensorRtDynamicShape\n    config.CollectShapeRangeInfo(FLAGS_dynamic_shape_file);\n    // config.EnableTunedTensorRtDynamicShape(FLAGS_dynamic_shape_file, true);\n\n    if (FLAGS_trt_use_static) {\n      config.SetOptimCacheDir(FLAGS_trt_static_dir);\n    }\n  }\n  config.SwitchIrOptim(true);\n\n  predictor_ = paddle_infer::CreatePredictor(config);\n  return true;\n}\n\nbool CenterPointDetection::Init(const StageConfig& stage_config) {\n  if (!Initialize(stage_config)) {\n    return false;\n  }\n\n  /*\n  num_point_feature\n  */\n  paddle::AnalysisConfig config;\n  config.EnableUseGpu(1000, FLAGS_gpu_id);\n  config.SetModel(FLAGS_center_point_model_file,\n                  FLAGS_center_point_params_file);\n  if (FLAGS_use_trt) {\n    paddle::AnalysisConfig::Precision precision;\n    if (FLAGS_trt_precision == 0) {\n      precision = paddle_infer::PrecisionType::kFloat32;\n    } else if (FLAGS_trt_precision == 1) {\n      precision = paddle_infer::PrecisionType::kHalf;\n    } else {\n      AERROR << \"Tensorrt type can only support 0 or 1, but recieved is\"\n             << FLAGS_trt_precision << \"\\n\";\n      return false;\n    }\n    config.EnableTensorRtEngine(1 << 30, 1, 3, precision, FLAGS_trt_use_static,\n                                false);\n    // todo: solve EnableTunedTensorRtDynamicShape\n    config.CollectShapeRangeInfo(FLAGS_dynamic_shape_file);\n    // config.EnableTunedTensorRtDynamicShape(FLAGS_dynamic_shape_file, true);\n\n    if (FLAGS_trt_use_static) {\n      config.SetOptimCacheDir(FLAGS_trt_static_dir);\n    }\n  }\n  config.SwitchIrOptim(true);\n\n  predictor_ = paddle_infer::CreatePredictor(config);\n  return true;\n}\n\nbool CenterPointDetection::Process(DataFrame* data_frame) {\n  if (data_frame == nullptr)\n    return false;\n\n  LidarFrame* lidar_frame = data_frame->lidar_frame;\n  if (lidar_frame == nullptr)\n    return false;\n\n  LidarDetectorOptions options;\n  bool res = Detect(options, lidar_frame);\n  return res;\n}\n\nbool CenterPointDetection::Detect(const LidarDetectorOptions &options,\n                                  LidarFrame *frame) {\n  // check input\n  if (frame == nullptr) {\n    AERROR << \"Input null frame ptr.\";\n    return false;\n  }\n  if (frame->cloud == nullptr) {\n    AERROR << \"Input null frame cloud.\";\n    return false;\n  }\n  if (frame->cloud->size() == 0) {\n    AERROR << \"Input none points.\";\n    return false;\n  }\n\n  // record input cloud and lidar frame\n  original_cloud_ = frame->cloud;\n  original_world_cloud_ = frame->world_cloud;\n  lidar_frame_ref_ = frame;\n\n  // check output\n  frame->segmented_objects.clear();\n\n  if (cudaSetDevice(FLAGS_gpu_id) != cudaSuccess) {\n    AERROR << \"Failed to set device to gpu \" << FLAGS_gpu_id;\n    return false;\n  }\n\n  Timer timer;\n\n  int num_points;\n  cur_cloud_ptr_ = std::shared_ptr<base::PointFCloud>(\n      new base::PointFCloud(*original_cloud_));\n\n  // down sample the point cloud through filtering beams\n  if (FLAGS_enable_downsample_beams) {\n    base::PointFCloudPtr downsample_beams_cloud_ptr(new base::PointFCloud());\n    if (DownSamplePointCloudBeams(original_cloud_, downsample_beams_cloud_ptr,\n                                  FLAGS_downsample_beams_factor)) {\n      cur_cloud_ptr_ = downsample_beams_cloud_ptr;\n    } else {\n      AWARN << \"Down-sample beams factor must be >= 1. Cancel down-sampling.\"\n               \" Current factor: \"\n            << FLAGS_downsample_beams_factor;\n    }\n  }\n\n  // down sample the point cloud through filtering voxel grid\n  if (FLAGS_enable_downsample_pointcloud) {\n    pcl::PointCloud<pcl::PointXYZI>::Ptr pcl_cloud_ptr(\n        new pcl::PointCloud<pcl::PointXYZI>());\n    pcl::PointCloud<pcl::PointXYZI>::Ptr filtered_cloud_ptr(\n        new pcl::PointCloud<pcl::PointXYZI>());\n    TransformToPCLXYZI(*cur_cloud_ptr_, pcl_cloud_ptr);\n    DownSampleCloudByVoxelGrid(\n        pcl_cloud_ptr, filtered_cloud_ptr, FLAGS_downsample_voxel_size_x,\n        FLAGS_downsample_voxel_size_y, FLAGS_downsample_voxel_size_z);\n\n    // transform pcl point cloud to apollo point cloud\n    base::PointFCloudPtr downsample_voxel_cloud_ptr(new base::PointFCloud());\n    TransformFromPCLXYZI(filtered_cloud_ptr, downsample_voxel_cloud_ptr);\n    cur_cloud_ptr_ = downsample_voxel_cloud_ptr;\n  }\n  downsample_time_ = timer.toc(true);\n\n  num_points = cur_cloud_ptr_->size();\n  AINFO << \"num points before fusing: \" << num_points;\n\n  // fuse clouds of preceding frames with current cloud\n  cur_cloud_ptr_->mutable_points_timestamp()->assign(cur_cloud_ptr_->size(),\n                                                     0.0);\n  if (FLAGS_enable_fuse_frames && FLAGS_num_fuse_frames > 1) {\n    // before fusing\n    while (!prev_world_clouds_.empty() &&\n           frame->timestamp - prev_world_clouds_.front()->get_timestamp() >\n               FLAGS_fuse_time_interval) {\n      prev_world_clouds_.pop_front();\n    }\n    // transform current cloud to world coordinate and save to a new ptr\n    base::PointDCloudPtr cur_world_cloud_ptr =\n        std::make_shared<base::PointDCloud>();\n    for (size_t i = 0; i < cur_cloud_ptr_->size(); ++i) {\n      auto &pt = cur_cloud_ptr_->at(i);\n      Eigen::Vector3d trans_point(pt.x, pt.y, pt.z);\n      trans_point = lidar_frame_ref_->lidar2world_pose * trans_point;\n      PointD world_point;\n      world_point.x = trans_point(0);\n      world_point.y = trans_point(1);\n      world_point.z = trans_point(2);\n      world_point.intensity = pt.intensity;\n      cur_world_cloud_ptr->push_back(world_point);\n    }\n    cur_world_cloud_ptr->set_timestamp(frame->timestamp);\n\n    // fusing clouds\n    for (auto &prev_world_cloud_ptr : prev_world_clouds_) {\n      num_points += prev_world_cloud_ptr->size();\n    }\n    FuseCloud(cur_cloud_ptr_, prev_world_clouds_);\n\n    // after fusing\n    while (static_cast<int>(prev_world_clouds_.size()) >=\n           FLAGS_num_fuse_frames - 1) {\n      prev_world_clouds_.pop_front();\n    }\n    prev_world_clouds_.emplace_back(cur_world_cloud_ptr);\n  }\n  fuse_time_ = timer.toc(true);\n\n  // shuffle points and cut off\n  if (FLAGS_enable_shuffle_points) {\n    num_points = std::min(num_points, FLAGS_max_num_points);\n    std::vector<int> point_indices = GenerateIndices(0, num_points, true);\n    base::PointFCloudPtr shuffle_cloud_ptr(\n        new base::PointFCloud(*cur_cloud_ptr_, point_indices));\n    cur_cloud_ptr_ = shuffle_cloud_ptr;\n  }\n  shuffle_time_ = timer.toc(true);\n\n  // points_array[x, y, z, i,timestampe, ......]\n  std::vector<float> points_data(num_points * FLAGS_num_point_feature);\n  CloudToArray(cur_cloud_ptr_, points_data.data(), FLAGS_normalizing_factor);\n  cloud_to_array_time_ = timer.toc(true);\n\n  // paddle inference\n  std::vector<float> out_detections;\n  std::vector<int64_t> out_labels;\n  std::vector<float> out_scores;\n  std::vector<float> out_detections_final;\n  std::vector<int64_t> out_labels_final;\n\n  DoInference(points_data, num_points, &out_detections, &out_labels,\n              &out_scores);\n\n  FilterScore(&out_detections, &out_labels, &out_scores, FLAGS_score_threshold,\n               &out_detections_final, &out_labels_final);\n\n  GetObjects(&frame->segmented_objects, frame->lidar2world_pose,\n             &out_detections_final, &out_labels_final);\n  inference_time_ = timer.toc(true);\n\n  AINFO << \"CenterPoint: \"\n        << \"\\n\"\n        << \"down sample: \" << downsample_time_ << \"\\t\"\n        << \"fuse: \" << fuse_time_ << \"\\t\"\n        << \"shuffle: \" << shuffle_time_ << \"\\t\"\n        << \"cloud_to_array: \" << cloud_to_array_time_ << \"\\t\"\n        << \"inference: \" << inference_time_ << \"\\t\";\n  return true;\n}\n\n// normalizing_factor: Normalize intensity range to [0, 1] by this factor\nvoid CenterPointDetection::CloudToArray(const base::PointFCloudPtr &pc_ptr,\n                                        float *out_points_array,\n                                        const float normalizing_factor) {\n  for (size_t i = 0; i < pc_ptr->size(); ++i) {\n    const auto &point = pc_ptr->at(i);\n    float x = point.x;\n    float y = point.y;\n    float z = point.z;\n    float intensity = point.intensity;\n    if (z < z_min_range_ || z > z_max_range_ || y < y_min_range_ ||\n        y > y_max_range_ || x < x_min_range_ || x > x_max_range_) {\n      continue;\n    }\n    out_points_array[i * FLAGS_num_point_feature + 0] = x;\n    out_points_array[i * FLAGS_num_point_feature + 1] = y;\n    out_points_array[i * FLAGS_num_point_feature + 2] = z;\n    out_points_array[i * FLAGS_num_point_feature + 3] =\n        intensity / normalizing_factor;\n    // delta of timestamp between prev and cur frames\n    out_points_array[i * FLAGS_num_point_feature + 4] =\n        static_cast<float>(pc_ptr->points_timestamp(i));\n  }\n}\n\nvoid CenterPointDetection::FuseCloud(\n    const base::PointFCloudPtr &out_cloud_ptr,\n    const std::deque<base::PointDCloudPtr> &fuse_clouds) {\n  for (auto iter = fuse_clouds.rbegin(); iter != fuse_clouds.rend(); ++iter) {\n    double delta_t = lidar_frame_ref_->timestamp - (*iter)->get_timestamp();\n    // transform prev world point cloud to current sensor's coordinates\n    for (size_t i = 0; i < (*iter)->size(); ++i) {\n      auto &point = (*iter)->at(i);\n      Eigen::Vector3d trans_point(point.x, point.y, point.z);\n      trans_point = lidar_frame_ref_->lidar2world_pose.inverse() * trans_point;\n      base::PointF pt;\n      pt.x = static_cast<float>(trans_point(0));\n      pt.y = static_cast<float>(trans_point(1));\n      pt.z = static_cast<float>(trans_point(2));\n      pt.intensity = static_cast<float>(point.intensity);\n      // delta of time between current and prev frame\n      out_cloud_ptr->push_back(pt, delta_t);\n    }\n  }\n}\n\nvoid CenterPointDetection::DoInference(const std::vector<float> &points_data,\n                                       const int in_num_points,\n                                       std::vector<float> *out_detections,\n                                       std::vector<int64_t> *out_labels,\n                                       std::vector<float> *out_scores) {\n  // todo: check gpu_id\n  std::vector<int> points_shape;\n  points_shape.push_back(in_num_points);\n  points_shape.push_back(FLAGS_num_point_feature);\n\n  Run(predictor_.get(), points_shape, points_data, out_detections, out_labels,\n      out_scores);\n}\n\nstd::vector<int> CenterPointDetection::GenerateIndices(int start_index,\n                                                       int size, bool shuffle) {\n  // create a range number array\n  std::vector<int> indices(size);\n  std::iota(indices.begin(), indices.end(), start_index);\n\n  // shuffle the index array\n  if (shuffle) {\n    unsigned seed = 0;\n    std::shuffle(indices.begin(), indices.end(),\n                 std::default_random_engine(seed));\n  }\n  return indices;\n}\n\nvoid CenterPointDetection::Run(paddle_infer::Predictor *predictor,\n                               const std::vector<int> &points_shape,\n                               const std::vector<float> &points_data,\n                               std::vector<float> *box3d_lidar,\n                               std::vector<int64_t> *label_preds,\n                               std::vector<float> *scores) {\n  auto input_names = predictor->GetInputNames();\n  for (const auto &tensor_name : input_names) {\n    auto in_tensor = predictor->GetInputHandle(tensor_name);\n    if (tensor_name == \"data\") {\n      in_tensor->Reshape(points_shape);\n      in_tensor->CopyFromCpu(points_data.data());\n    }\n  }\n  ACHECK(predictor->Run());\n\n  auto output_names = predictor->GetOutputNames();\n  for (size_t i = 0; i != output_names.size(); i++) {\n    auto output = predictor->GetOutputHandle(output_names[i]);\n    std::vector<int> output_shape = output->shape();\n    int out_num = std::accumulate(output_shape.begin(), output_shape.end(), 1,\n                                  std::multiplies<int>());\n    if (i == 0) {\n      box3d_lidar->resize(out_num);\n      output->CopyToCpu(box3d_lidar->data());\n    } else if (i == 1) {\n      label_preds->resize(out_num);\n      output->CopyToCpu(label_preds->data());\n    } else if (i == 2) {\n      scores->resize(out_num);\n      output->CopyToCpu(scores->data());\n    }\n  }\n}\n\nvoid CenterPointDetection::GetObjects(\n    std::vector<std::shared_ptr<Object>> *objects, const Eigen::Affine3d &pose,\n    std::vector<float> *detections, std::vector<int64_t> *labels) {\n  int num_objects = detections->size() / num_output_box_feature_;\n\n  objects->clear();\n  base::ObjectPool::Instance().BatchGet(num_objects, objects);\n\n  for (int i = 0; i < num_objects; ++i) {\n    auto &object = objects->at(i);\n    object->id = i;\n\n    // no velocity\n    float x = detections->at(i * FLAGS_num_output_box_feature + 0);\n    float y = detections->at(i * FLAGS_num_output_box_feature + 1);\n    float z = detections->at(i * FLAGS_num_output_box_feature + 2);\n    float dx = detections->at(i * FLAGS_num_output_box_feature + 3);\n    float dy = detections->at(i * FLAGS_num_output_box_feature + 4);\n    float dz = detections->at(i * FLAGS_num_output_box_feature + 5);\n    float yaw = detections->at(i * FLAGS_num_output_box_feature + 6);\n    // yaw += M_PI / 2;\n    yaw = std::atan2(sinf(yaw), cosf(yaw));\n    yaw = -yaw;\n\n    // directions\n    object->theta = yaw;\n    object->direction[0] = cosf(yaw);\n    object->direction[1] = sinf(yaw);\n    object->direction[2] = 0;\n    object->lidar_supplement.is_orientation_ready = true;\n\n    // compute vertexes of bounding box and transform to world coordinate\n    object->lidar_supplement.num_points_in_roi = 8;\n    object->lidar_supplement.on_use = true;\n    object->lidar_supplement.is_background = false;\n    float roll = 0, pitch = 0;\n    Eigen::Quaternionf quater =\n        Eigen::AngleAxisf(roll, Eigen::Vector3f::UnitX()) *\n        Eigen::AngleAxisf(pitch, Eigen::Vector3f::UnitY()) *\n        Eigen::AngleAxisf(yaw, Eigen::Vector3f::UnitZ());\n    Eigen::Translation3f translation(x, y, z);\n    Eigen::Affine3f affine3f = translation * quater.toRotationMatrix();\n    for (float vx : std::vector<float>{dx / 2, -dx / 2}) {\n      for (float vy : std::vector<float>{dy / 2, -dy / 2}) {\n        for (float vz : std::vector<float>{0, dz}) {\n          Eigen::Vector3f v3f(vx, vy, vz);\n          v3f = affine3f * v3f;\n          PointF point;\n          point.x = v3f.x();\n          point.y = v3f.y();\n          point.z = v3f.z();\n          object->lidar_supplement.cloud.push_back(point);\n\n          Eigen::Vector3d trans_point(point.x, point.y, point.z);\n          trans_point = pose * trans_point;\n          PointD world_point;\n          world_point.x = trans_point(0);\n          world_point.y = trans_point(1);\n          world_point.z = trans_point(2);\n          object->lidar_supplement.cloud_world.push_back(world_point);\n        }\n      }\n    }\n\n    // classification\n    object->lidar_supplement.raw_probs.push_back(std::vector<float>(\n        static_cast<int>(base::ObjectType::MAX_OBJECT_TYPE), 0.f));\n    object->lidar_supplement.raw_classification_methods.push_back(Name());\n    object->sub_type = GetObjectSubType(labels->at(i));\n    object->type = base::kSubType2TypeMap.at(object->sub_type);\n    object->lidar_supplement.raw_probs.back()[static_cast<int>(object->type)] =\n        1.0f;\n    // copy to type\n    object->type_probs.assign(object->lidar_supplement.raw_probs.back().begin(),\n                              object->lidar_supplement.raw_probs.back().end());\n  }\n}\n\nbase::ObjectSubType CenterPointDetection::GetObjectSubType(const int label) {\n  switch (label) {\n    case 0:\n      return base::ObjectSubType::CAR;\n    case 1:\n      return base::ObjectSubType::TRUCK;\n    case 3:\n      return base::ObjectSubType::BUS;\n    case 6:\n      return base::ObjectSubType::MOTORCYCLIST;\n    case 7:\n      return base::ObjectSubType::CYCLIST;\n    case 8:\n      return base::ObjectSubType::PEDESTRIAN;\n    case 9:\n      return base::ObjectSubType::TRAFFICCONE;\n    default:\n      return base::ObjectSubType::UNKNOWN;\n  }\n}\n\nvoid CenterPointDetection::FilterScore(\n    const std::vector<float> *box3d_lidar,\n    const std::vector<int64_t> *label_preds, const std::vector<float> *scores,\n    const float score_threshold, std::vector<float> *box3d_lidar_final,\n    std::vector<int64_t> *label_preds_final) {\n  for (size_t i = 0; i < scores->size(); i++) {\n    if (scores->at(i) > score_threshold) {\n      box3d_lidar_final->insert(\n          box3d_lidar_final->end(),\n          box3d_lidar->begin() + num_output_box_feature_ * i,\n          box3d_lidar->begin() + num_output_box_feature_ * (i + 1));\n      label_preds_final->insert(label_preds_final->end(),\n                                *(label_preds->begin() + i));\n    }\n  }\n}\n\nPERCEPTION_REGISTER_LIDARDETECTOR(CenterPointDetection);\n\n}  // namespace lidar\n}  // namespace perception\n}  // namespace apollo",
        "lookback": " * Copyright 2022 The Apollo Authors. All Rights Reserved.",
        "lookahead": "",
        "label": [
            "ours",
            "one_side_deletion"
        ],
        "start_line": 0,
        "end_line": 1001,
        "merge_gen_region": " Copyright022 Alo. Rights. Rights. * Licensed the Apache, 20theLicense *\"); you not this except in with the. * may a of License * may a of License * * httpwwwapacheorglicensesLICENSE20 * Unless by law agreed in,, * to writing software distributed the is on \" IS BASIS * WARRANTIES CONDITIONS ANY, either or. See License the for specific governing and limitations the License *****************************************************************************/includemodulesper/arlibdetectorcenterpointdetectioncenterpointdetectionh # <>includefunctional # <>includenumeric # <random # <cudaruntimeapih # \"cy////.\"includecy/commonlogh # \"/per//___.\"includemodulesper//___.\"includemodulesper//__.\"includemodulesper//__.\"includemodulesper//artimerh # \"modulesper/arcommonpclutilh # \"/ceptionlid////__.\"includemodulesper/ar////.\"includemodulesper/arlibdetectorcenterpointdetectionparamsh namespacepol { per { lar using::D using::Point; point rangePoint::::Point::Point() xminrangeParams::",
        "confidence": 0.9161060452461243,
        "hesitated": false,
        "resolved": false
    }
]