[
    {
        "a_contents": "    : ModelInference(config), device_(torch::kCPU) {}\n\nbool TrajectoryImitationLibtorchInference::LoadCONVRNNModel() {\n  // run a fake inference at init time as first inference is relative slow\n  torch::Tensor input_feature_tensor = torch::zeros({1, 12, 200, 200});\n  torch::Tensor initial_point_tensor = torch::zeros({1, 1, 200, 200});\n  torch::Tensor initial_box_tensor = torch::zeros({1, 1, 200, 200});\n  std::vector<torch::jit::IValue> torch_inputs;\n  at::Tensor tensor_tmp;\n  torch_inputs.push_back(\n      c10::ivalue::Tuple::create({std::move(input_feature_tensor.to(device_)),\n                                  std::move(initial_point_tensor.to(device_)),\n                                  std::move(initial_box_tensor.to(device_))}));\n  try {\n    auto torch_output_tensor =\n        model_.forward(torch_inputs).toTensor().to(torch::kCPU);\n  } catch (const c10::Error& e) {\n    AERROR << \"Fail to do initial inference on CONV_RNN Model\";\n    return false;\n  }\n  return true;\n}\n\nbool TrajectoryImitationLibtorchInference::LoadCNNModel() {\n  // run a fake inference at init time as first inference is relative slow\n  torch::Tensor input_feature_tensor = torch::zeros({1, 12, 200, 200});\n  std::vector<torch::jit::IValue> torch_inputs;\n  torch_inputs.push_back(input_feature_tensor.to(device_));\n  try {\n    auto torch_output_tensor =\n        model_.forward(torch_inputs).toTensor().to(torch::kCPU);\n  } catch (const c10::Error& e) {\n    AERROR << \"Fail to do initial inference on CNN Model\";\n    return false;\n  }\n  return true;\n}",
        "b_contents": "    : ModelInference(config), device_(torch::kCPU) {}",
        "base_contents": "    : ModelInference(config), device_(torch::kCPU) {\n}\n\nbool TrajectoryImitationLibtorchInference::LoadModel() {\n  if (config_.use_cuda() && torch::cuda::is_available()) {\n    ADEBUG << \"CUDA is available\";\n    device_ = torch::Device(torch::kCUDA);\n  }\n  model_ = torch::jit::load(config_.model_file(), device_);\n  torch::set_num_threads(1);",
        "res_region": "    : ModelInference(config), device_(torch::kCPU) {}\n\nbool TrajectoryImitationLibtorchInference::LoadCONVRNNModel() {\n  // run a fake inference at init time as first inference is relative slow\n  torch::Tensor input_feature_tensor = torch::zeros({1, 12, 200, 200});\n  torch::Tensor past_points_tensor = torch::zeros({1, 10, 4});\n  torch::Tensor past_points_step_tensor = torch::zeros({1, 10, 4});\n  std::vector<torch::jit::IValue> torch_inputs;\n  torch_inputs.push_back(c10::ivalue::Tuple::create(\n      {std::move(input_feature_tensor.to(device_)),\n       std::move(past_points_tensor.to(device_)),\n       std::move(past_points_step_tensor.to(device_))}));\n  try {\n    auto torch_output_tensor =\n        model_.forward(torch_inputs).toTensor().to(torch::kCPU);\n  } catch (const c10::Error& e) {\n    AERROR << \"Fail to do initial inference on CONV_RNN Model\";\n    return false;\n  }\n  return true;\n}\n\nbool TrajectoryImitationLibtorchInference::LoadCNNModel() {\n  // run a fake inference at init time as first inference is relative slow\n  torch::Tensor input_feature_tensor = torch::zeros({1, 12, 200, 200});\n  std::vector<torch::jit::IValue> torch_inputs;\n  torch_inputs.push_back(input_feature_tensor.to(device_));\n  try {\n    auto torch_output_tensor =\n        model_.forward(torch_inputs).toTensor().to(torch::kCPU);\n  } catch (const c10::Error& e) {\n    AERROR << \"Fail to do initial inference on CNN Model\";\n    return false;\n  }\n  return true;\n}",
        "lookback": "",
        "lookahead": "",
        "label": [
            "newcode",
            "complex_conflict"
        ],
        "start_line": 33,
        "end_line": 84,
        "merge_gen_region": null,
        "confidence": 0.6736571192741394,
        "hesitated": true
    },
    {
        "a_contents": "bool TrajectoryImitationLibtorchInference::LoadCNNLSTMModel() {",
        "b_contents": "bool TrajectoryImitationLibtorchInference::LoadCONVRNNModel() {",
        "base_contents": "",
        "res_region": "",
        "lookback": "bool TrajectoryImitationLibtorchInference::LoadCNNLSTMModel() {",
        "lookahead": "",
        "label": [
            "theirs",
            "one_side_deletion"
        ],
        "start_line": 86,
        "end_line": 91,
        "merge_gen_region": null,
        "confidence": 0.8754960894584656,
        "hesitated": true
    },
    {
        "a_contents": "       std::move(past_points_tensor.to(device_)),\n       std::move(past_points_step_tensor.to(device_))}));\n  try {\n    auto torch_output_tensor =\n        model_.forward(torch_inputs).toTensor().to(torch::kCPU);\n  } catch (const c10::Error& e) {\n    AERROR << \"Fail to do initial inference on CNN_LSTM Model\";\n    return false;\n  }",
        "b_contents": "       std::move(initial_point_tensor.to(device_)),\n       std::move(initial_box_tensor.to(device_))},\n      c10::TupleType::create(\n          std::vector<c10::TypePtr>(3, c10::TensorType::create()))));\n  try {\n    auto torch_output_tensor =\n        model_.forward(torch_inputs).toTensor().to(torch::kCPU);\n  } catch (const c10::Error& e) {\n    AERROR << \"Fail to do initial inference on CONV_RNN Model\";\n    return false;\n  }",
        "base_contents": "       std::move(initial_point_tensor.to(device_)),\n       std::move(initial_box_tensor.to(device_))},\n      c10::TupleType::create(\n          std::vector<c10::TypePtr>(3, c10::TensorType::create()))));\n  auto torch_output_tensor =\n      model_.forward(torch_inputs).toTensor().to(torch::kCPU);",
        "res_region": "",
        "lookback": "bool TrajectoryImitationLibtorchInference::LoadCNNLSTMModel() {",
        "lookahead": "",
        "label": [
            "deletion",
            "complex_conflict"
        ],
        "start_line": 99,
        "end_line": 128,
        "merge_gen_region": "::::(stdmovepBaseBase(::&&&&%%&&&%%&&&%%%&&%%%%&&%%%%%&&%%%%%&&%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%",
        "confidence": 0.9250378608703613,
        "hesitated": false,
        "resolved": false
    },
    {
        "a_contents": "bool TrajectoryImitationLibtorchInference::LoadModel() {\n  if (config_.use_cuda() && torch::cuda::is_available()) {\n    ADEBUG << \"CUDA is available\";\n    device_ = torch::Device(torch::kCUDA);\n  }\n  try {\n    model_ = torch::jit::load(config_.model_file(), device_);\n  } catch (const c10::Error& e) {\n    AERROR << \"Failed to load model on to device\";\n    return false;\n  }\n  torch::set_num_threads(1);\n  switch (config_.model_type()) {\n    case LearningModelInferenceTaskConfig::CONV_RNN: {\n      if (!LoadCONVRNNModel()) {\n        return false;\n      }\n      break;\n    }\n    case LearningModelInferenceTaskConfig::CNN: {\n      if (!LoadCNNModel()) {\n        return false;\n      }\n      break;\n    }\n    case LearningModelInferenceTaskConfig::CNN_LSTM: {\n      if (!LoadCNNLSTMModel()) {\n        return false;\n      }\n      break;\n    }\n    default: {\n      AERROR << \"Configured model type not defined and implemented\";\n      break;\n    }\n  }\n  return true;\n}\n\nbool TrajectoryImitationLibtorchInference::DoCONVRNNMODELInference(",
        "b_contents": "bool TrajectoryImitationLibtorchInference::LoadCNNModel() {\n  // run a fake inference at init time as first inference is relative slow\n  torch::Tensor input_feature_tensor = torch::zeros({1, 12, 200, 200});\n  std::vector<torch::jit::IValue> torch_inputs;\n  torch_inputs.push_back(input_feature_tensor.to(device_));\n  try {\n    auto torch_output_tensor =\n        model_.forward(torch_inputs).toTensor().to(torch::kCPU);\n  } catch (const c10::Error& e) {\n    AERROR << \"Fail to do initial inference on CNN Model\";\n    return false;\n  }\n  return true;\n}\n\nbool TrajectoryImitationLibtorchInference::LoadCNNLSTMModel() {\n  // run a fake inference at init time as first inference is relative slow\n  torch::Tensor input_feature_tensor = torch::zeros({1, 12, 200, 200});\n  torch::Tensor past_points_tensor = torch::zeros({1, 10, 4});\n  torch::Tensor past_points_step_tensor = torch::zeros({1, 10, 4});\n  std::vector<torch::jit::IValue> torch_inputs;\n  torch_inputs.push_back(c10::ivalue::Tuple::create(\n      {std::move(input_feature_tensor.to(device_)),\n       std::move(past_points_tensor.to(device_)),\n       std::move(past_points_step_tensor.to(device_))},\n      c10::TupleType::create(\n          std::vector<c10::TypePtr>(3, c10::TensorType::create()))));\n  try {\n    auto torch_output_tensor =\n        model_.forward(torch_inputs).toTensor().to(torch::kCPU);\n  } catch (const c10::Error& e) {\n    AERROR << \"Fail to do initial inference on CNN_LSTM Model\";\n    return false;\n  }\n  return true;\n}\n\nbool TrajectoryImitationLibtorchInference::LoadModel() {\n  if (config_.use_cuda() && torch::cuda::is_available()) {\n    ADEBUG << \"CUDA is available\";\n    device_ = torch::Device(torch::kCUDA);\n  }\n  try {\n    model_ = torch::jit::load(config_.model_file(), device_);\n  } catch (const c10::Error& e) {\n    AERROR << \"Failed to load model on to device\";\n    return false;\n  }\n  torch::set_num_threads(1);\n  switch (config_.model_type()) {\n    case LearningModelInferenceTaskConfig::CONV_RNN: {\n      if (!LoadCONVRNNModel()) {\n        return false;\n      }\n      break;\n    }\n    case LearningModelInferenceTaskConfig::CNN: {\n      if (!LoadCNNModel()) {\n        return false;\n      }\n      break;\n    }\n    case LearningModelInferenceTaskConfig::CNN_LSTM: {\n      if (!LoadCNNLSTMModel()) {\n        return false;\n      }\n      break;\n    }\n    default: {\n      AERROR << \"Configured model type not defined and implemented\";\n      break;\n    }\n  }\n  return true;\n}\n\nbool TrajectoryImitationLibtorchInference::DoCONVRNNMODELInference(",
        "base_contents": "bool TrajectoryImitationLibtorchInference::DoInference(",
        "res_region": "\nbool TrajectoryImitationLibtorchInference::LoadCNNLSTMModel() {\n  // run a fake inference at init time as first inference is relative slow\n  torch::Tensor input_feature_tensor = torch::zeros({1, 12, 200, 200});\n  torch::Tensor past_points_tensor = torch::zeros({1, 10, 4});\n  torch::Tensor past_points_step_tensor = torch::zeros({1, 10, 4});\n  std::vector<torch::jit::IValue> torch_inputs;\n  torch_inputs.push_back(c10::ivalue::Tuple::create(\n      {std::move(input_feature_tensor.to(device_)),\n       std::move(past_points_tensor.to(device_)),\n       std::move(past_points_step_tensor.to(device_))}));\n  try {\n    auto torch_output_tensor =\n        model_.forward(torch_inputs).toTensor().to(torch::kCPU);\n  } catch (const c10::Error& e) {\n    AERROR << \"Fail to do initial inference on CNN_LSTM Model\";\n    return false;\n  }\n  return true;\n}\n\nbool TrajectoryImitationLibtorchInference::LoadModel() {\n  if (config_.use_cuda() && torch::cuda::is_available()) {\n    ADEBUG << \"CUDA is available\";\n    device_ = torch::Device(torch::kCUDA);\n  }\n  try {\n    model_ = torch::jit::load(config_.model_file(), device_);\n  } catch (const c10::Error& e) {\n    AERROR << \"Failed to load model on to device\";\n    return false;\n  }\n  torch::set_num_threads(1);\n  switch (config_.model_type()) {\n    case LearningModelInferenceTaskConfig::CONV_RNN: {\n      if (!LoadCONVRNNModel()) {\n        return false;\n      }\n      break;\n    }\n    case LearningModelInferenceTaskConfig::CNN: {\n      if (!LoadCNNModel()) {\n        return false;\n      }\n      break;\n    }\n    case LearningModelInferenceTaskConfig::CNN_LSTM: {\n      if (!LoadCNNLSTMModel()) {\n        return false;\n      }\n      break;\n    }\n    default: {\n      AERROR << \"Configured model type not defined and implemented\";\n      break;\n    }\n  }\n  return true;\n}\n\nbool TrajectoryImitationLibtorchInference::DoCONVRNNMODELInference(",
        "lookback": "bool TrajectoryImitationLibtorchInference::LoadCNNLSTMModel() {",
        "lookahead": "    LearningDataFrame* learning_data_frame) {",
        "label": [
            "newcode",
            "complex_conflict"
        ],
        "start_line": 132,
        "end_line": 253,
        "merge_gen_region": " TraectorymitLibchferenceLoadNLModel { TraectorymitationtorIn::Model() ifconfigusecuda &&chcudacudaisavailable {DEBUG \" is\"; device =chDevicetor::CUDA); try model =chjitloadconfigmodelfile device } catchconst c::Error e {ERROR \" to load on device\"; false } tor::setnumthreads1); (_.modeltype { LningModelferenceConfigCONVRN: { (!CONNN()) return; break } LningInferenceConfigCN: ifLoadN()) { false }; break } LningInferenceConfigCN_STM: ifLoadNLModel { false }; default { are <<Config model not and\";; } return; } TraectorymitLibchferenceLoadNModel { run fakeference at time firstference is slowchTensor___ =chzeros1 12 200 200 200 200 tor:: pastpointstensor tor::jitI>chinputs tor_._).Tensorto(chk); catchconst c::Error e {ERROR \" to initialference onNN\"; false } true } TraectorymitLibchferenceLoadNLModel { run fakeference at time firstference is slowchTensor___ =chzeros1 12 200 200 tor:: past",
        "confidence": 0.9101014733314514,
        "hesitated": false,
        "resolved": false
    },
    {
        "a_contents": "  torch_inputs.push_back(\n      c10::ivalue::Tuple::create({std::move(input_feature_tensor.to(device_)),\n                                  std::move(initial_point_tensor.to(device_)),\n                                  std::move(initial_box_tensor.to(device_))}));\n  at::Tensor torch_output_tensor =\n      model_.forward(torch_inputs).toTensor().to(torch::kCPU);\n\n  auto inference_end_time = std::chrono::system_clock::now();\n  std::chrono::duration<double> inference_diff =\n      inference_end_time - inference_start_time;\n  ADEBUG << \"trajectory imitation model inference used time: \"\n         << inference_diff.count() * 1000 << \" ms.\";\n\n  const auto& cur_traj_point =\n      learning_data_frame->adc_trajectory_point(past_points_size - 1);\n  const double cur_time_sec = cur_traj_point.timestamp_sec();\n  const auto& cur_path_point = cur_traj_point.trajectory_point().path_point();\n  const double cur_x = cur_path_point.x();\n  const double cur_y = cur_path_point.y();\n  const double cur_heading = cur_path_point.theta();\n  ADEBUG << \"cur_x[\" << cur_x << \"], cur_y[\" << cur_y << \"], cur_heading[\"\n         << cur_heading << \"], cur_v[\" << cur_traj_point.trajectory_point().v()\n         << \"]\";\n\n  learning_data_frame->mutable_output()->clear_adc_future_trajectory_point();\n  const double delta_t = config_.trajectory_delta_t();\n  auto torch_output = torch_output_tensor.accessor<float, 3>();\n  for (int i = 0; i < torch_output_tensor.size(1); ++i) {\n    const double dx = static_cast<double>(torch_output[0][i][0]);\n    const double dy = static_cast<double>(torch_output[0][i][1]);\n    const double dtheta = static_cast<double>(torch_output[0][i][2]);\n    const double v = static_cast<double>(torch_output[0][i][3]);\n    ADEBUG << \"dx[\" << dx << \"], dy[\" << dy << \"], dtheta[\" << dtheta << \"], v[\"\n           << v << \"]\";\n    const double time_sec = cur_time_sec + delta_t * (i + 1);\n    apollo::common::math::Vec2d offset(dx, dy);\n    apollo::common::math::Vec2d rotated_offset = offset.rotate(cur_heading);\n    const double x = cur_x + rotated_offset.x();\n    const double y = cur_y + rotated_offset.y();\n    const double heading =\n        apollo::common::math::NormalizeAngle(dtheta + cur_heading);",
        "b_contents": "  torch_inputs.push_back(c10::ivalue::Tuple::create(\n      {std::move(input_feature_tensor.to(device_)),\n       std::move(initial_point_tensor.to(device_)),\n       std::move(initial_box_tensor.to(device_))},\n      c10::TupleType::create(\n          std::vector<c10::TypePtr>(3, c10::TensorType::create()))));\n  at::Tensor torch_output_tensor =\n      model_.forward(torch_inputs).toTensor().to(torch::kCPU);",
        "base_contents": "  torch_inputs.push_back(c10::ivalue::Tuple::create(\n      {std::move(input_feature_tensor.to(device_)),\n       std::move(initial_point_tensor.to(device_)),\n       std::move(initial_box_tensor.to(device_))},\n      c10::TupleType::create(\n          std::vector<c10::TypePtr>(3, c10::TensorType::create()))));",
        "res_region": "  torch_inputs.push_back(c10::ivalue::Tuple::create(\n      {std::move(input_feature_tensor.to(device_)),\n       std::move(initial_point_tensor.to(device_)),\n       std::move(initial_box_tensor.to(device_))}));\n  at::Tensor torch_output_tensor =\n      model_.forward(torch_inputs).toTensor().to(torch::kCPU);\n",
        "lookback": "      {std::move(input_feature_tensor.to(device_)),",
        "lookahead": "  auto inference_end_time = std::chrono::system_clock::now();",
        "label": [
            "newcode",
            "complex_conflict"
        ],
        "start_line": 324,
        "end_line": 382,
        "merge_gen_region": null,
        "confidence": 0.8302732110023499,
        "hesitated": true
    },
    {
        "a_contents": "    auto* traj_point = learning_data_frame->mutable_output()\n                           ->add_adc_future_trajectory_point();\n    traj_point->set_timestamp_sec(time_sec);\n    traj_point->mutable_trajectory_point()->mutable_path_point()->set_x(x);\n    traj_point->mutable_trajectory_point()->mutable_path_point()->set_y(y);\n    traj_point->mutable_trajectory_point()->mutable_path_point()->set_theta(\n        heading);\n    traj_point->mutable_trajectory_point()->set_v(v);\n    traj_point->mutable_trajectory_point()->set_relative_time(delta_t *\n                                                              (i + 1));\n  }\n\n  return true;\n}\n\nbool TrajectoryImitationLibtorchInference::DoCNNMODELInference(\n    LearningDataFrame* learning_data_frame) {\n  const int past_points_size = learning_data_frame->adc_trajectory_point_size();\n  if (past_points_size == 0) {\n    AERROR << \"No current trajectory point status\";\n    return false;\n  }\n\n  auto input_renderering_start_time = std::chrono::system_clock::now();\n\n  cv::Mat input_feature;\n  if (!BirdviewImgFeatureRenderer::Instance()->RenderMultiChannelEnv(\n          *learning_data_frame, &input_feature)) {\n    AERROR << \"Render multi-channel input image failed\";\n    return false;\n  }\n\n  auto input_renderering_end_time = std::chrono::system_clock::now();\n  std::chrono::duration<double> rendering_diff =\n      input_renderering_end_time - input_renderering_start_time;\n  ADEBUG << \"trajectory imitation model input renderer used time: \"\n         << rendering_diff.count() * 1000 << \" ms.\";\n\n  auto input_preprocessing_start_time = std::chrono::system_clock::now();\n\n  cv::Mat input_feature_float;\n  input_feature.convertTo(input_feature_float, CV_32F, 1.0 / 255);\n  torch::Tensor input_feature_tensor =\n      torch::from_blob(input_feature_float.data,\n                       {1, input_feature_float.rows, input_feature_float.cols,\n                        input_feature_float.channels()});\n  input_feature_tensor = input_feature_tensor.permute({0, 3, 1, 2});\n  for (int i = 0; i < input_feature_float.channels(); ++i) {\n    input_feature_tensor[0][i] = input_feature_tensor[0][i].sub(0.5).div(0.5);\n  }\n\n  auto input_preprocessing_end_time = std::chrono::system_clock::now();\n  std::chrono::duration<double> preprocessing_diff =\n      input_preprocessing_end_time - input_preprocessing_start_time;\n  ADEBUG << \"trajectory imitation model input preprocessing used time: \"\n         << preprocessing_diff.count() * 1000 << \" ms.\";",
        "b_contents": "  auto inference_end_time = std::chrono::system_clock::now();\n  std::chrono::duration<double> inference_diff =\n      inference_end_time - inference_start_time;\n  ADEBUG << \"trajectory imitation model inference used time: \"\n         << inference_diff.count() * 1000 << \" ms.\";\n\n  const auto& cur_traj_point =\n      learning_data_frame->adc_trajectory_point(past_points_size - 1);\n  const double cur_time_sec = cur_traj_point.timestamp_sec();\n  const auto& cur_path_point = cur_traj_point.trajectory_point().path_point();\n  const double cur_x = cur_path_point.x();\n  const double cur_y = cur_path_point.y();\n  const double cur_heading = cur_path_point.theta();\n  ADEBUG << \"cur_x[\" << cur_x << \"], cur_y[\" << cur_y << \"], cur_heading[\"\n         << cur_heading << \"], cur_v[\" << cur_traj_point.trajectory_point().v()\n         << \"]\";\n\n  learning_data_frame->mutable_output()->clear_adc_future_trajectory_point();\n  const double delta_t = config_.trajectory_delta_t();\n  auto torch_output = torch_output_tensor.accessor<float, 3>();\n  for (int i = 0; i < torch_output_tensor.size(1); ++i) {\n    const double dx = static_cast<double>(torch_output[0][i][0]);\n    const double dy = static_cast<double>(torch_output[0][i][1]);\n    const double dtheta = static_cast<double>(torch_output[0][i][2]);\n    const double v = static_cast<double>(torch_output[0][i][3]);\n    ADEBUG << \"dx[\" << dx << \"], dy[\" << dy << \"], dtheta[\" << dtheta << \"], v[\"\n           << v << \"]\";\n    const double time_sec = cur_time_sec + delta_t * (i + 1);\n    apollo::common::math::Vec2d offset(dx, dy);\n    apollo::common::math::Vec2d rotated_offset = offset.rotate(cur_heading);\n    const double x = cur_x + rotated_offset.x();\n    const double y = cur_y + rotated_offset.y();\n    const double heading =\n        apollo::common::math::NormalizeAngle(dtheta + cur_heading);\n\n    auto* traj_point = learning_data_frame->mutable_output()\n                           ->add_adc_future_trajectory_point();\n    traj_point->set_timestamp_sec(time_sec);\n    traj_point->mutable_trajectory_point()->mutable_path_point()->set_x(x);\n    traj_point->mutable_trajectory_point()->mutable_path_point()->set_y(y);\n    traj_point->mutable_trajectory_point()->mutable_path_point()->set_theta(\n        heading);\n    traj_point->mutable_trajectory_point()->set_v(v);\n    traj_point->mutable_trajectory_point()->set_relative_time(delta_t *\n                                                              (i + 1));\n  }\n\n  return true;\n}\n\nbool TrajectoryImitationLibtorchInference::DoCNNMODELInference(\n    LearningDataFrame* learning_data_frame) {\n  const int past_points_size = learning_data_frame->adc_trajectory_point_size();\n  if (past_points_size == 0) {\n    AERROR << \"No current trajectory point status\";\n    return false;\n  }\n\n  auto input_renderering_start_time = std::chrono::system_clock::now();\n\n  cv::Mat input_feature;\n  if (!BirdviewImgFeatureRenderer::Instance()->RenderMultiChannelEnv(\n          *learning_data_frame, &input_feature)) {\n    AERROR << \"Render multi-channel input image failed\";\n    return false;\n  }\n\n  auto input_renderering_end_time = std::chrono::system_clock::now();\n  std::chrono::duration<double> rendering_diff =\n      input_renderering_end_time - input_renderering_start_time;\n  ADEBUG << \"trajectory imitation model input renderer used time: \"\n         << rendering_diff.count() * 1000 << \" ms.\";\n\n  auto input_preprocessing_start_time = std::chrono::system_clock::now();\n\n  cv::Mat input_feature_float;\n  input_feature.convertTo(input_feature_float, CV_32F, 1.0 / 255);\n  torch::Tensor input_feature_tensor =\n      torch::from_blob(input_feature_float.data,\n                       {1, input_feature_float.rows, input_feature_float.cols,\n                        input_feature_float.channels()});\n  input_feature_tensor = input_feature_tensor.permute({0, 3, 1, 2});\n  for (int i = 0; i < input_feature_float.channels(); ++i) {\n    input_feature_tensor[0][i] = input_feature_tensor[0][i].sub(0.5).div(0.5);\n  }\n\n  auto input_preprocessing_end_time = std::chrono::system_clock::now();\n  std::chrono::duration<double> preprocessing_diff =\n      input_preprocessing_end_time - input_preprocessing_start_time;\n  ADEBUG << \"trajectory imitation model input preprocessing used time: \"\n         << preprocessing_diff.count() * 1000 << \" ms.\";\n\n  auto inference_start_time = std::chrono::system_clock::now();\n\n  std::vector<torch::jit::IValue> torch_inputs;\n  torch_inputs.push_back(input_feature_tensor.to(device_));\n  at::Tensor torch_output_tensor =\n      model_.forward(torch_inputs).toTensor().to(torch::kCPU);\n\n  auto inference_end_time = std::chrono::system_clock::now();\n  std::chrono::duration<double> inference_diff =\n      inference_end_time - inference_start_time;\n  ADEBUG << \"trajectory imitation model inference used time: \"\n         << inference_diff.count() * 1000 << \" ms.\";\n\n  const auto& cur_traj_point =\n      learning_data_frame->adc_trajectory_point(past_points_size - 1);\n  const double cur_time_sec = cur_traj_point.timestamp_sec();\n  const auto& cur_path_point = cur_traj_point.trajectory_point().path_point();\n  const double cur_x = cur_path_point.x();\n  const double cur_y = cur_path_point.y();\n  const double cur_heading = cur_path_point.theta();\n  ADEBUG << \"cur_x[\" << cur_x << \"], cur_y[\" << cur_y << \"], cur_heading[\"\n         << cur_heading << \"], cur_v[\" << cur_traj_point.trajectory_point().v()\n         << \"]\";\n\n  learning_data_frame->mutable_output()->clear_adc_future_trajectory_point();\n  const double delta_t = config_.trajectory_delta_t();\n  auto torch_output = torch_output_tensor.accessor<float, 3>();\n  for (int i = 0; i < torch_output_tensor.size(1); ++i) {\n    const double dx = static_cast<double>(torch_output[0][i][0]);\n    const double dy = static_cast<double>(torch_output[0][i][1]);\n    const double dtheta = static_cast<double>(torch_output[0][i][2]);\n    const double v = static_cast<double>(torch_output[0][i][3]);\n    ADEBUG << \"dx[\" << dx << \"], dy[\" << dy << \"], dtheta[\" << dtheta << \"], v[\"\n           << v << \"]\";\n    const double time_sec = cur_time_sec + delta_t * (i + 1);\n    apollo::common::math::Vec2d offset(dx, dy);\n    apollo::common::math::Vec2d rotated_offset = offset.rotate(cur_heading);\n    const double x = cur_x + rotated_offset.x();\n    const double y = cur_y + rotated_offset.y();\n    const double heading =\n        apollo::common::math::NormalizeAngle(dtheta + cur_heading);\n\n    auto* traj_point = learning_data_frame->mutable_output()\n                           ->add_adc_future_trajectory_point();\n    traj_point->set_timestamp_sec(time_sec);\n    traj_point->mutable_trajectory_point()->mutable_path_point()->set_x(x);\n    traj_point->mutable_trajectory_point()->mutable_path_point()->set_y(y);\n    traj_point->mutable_trajectory_point()->mutable_path_point()->set_theta(\n        heading);\n    traj_point->mutable_trajectory_point()->set_v(v);\n    traj_point->mutable_trajectory_point()->set_relative_time(delta_t *\n                                                              (i + 1));\n  }\n\n  return true;\n}\n\nbool TrajectoryImitationLibtorchInference::DoCNNLSTMMODELInference(\n    LearningDataFrame* learning_data_frame) {\n  const int past_points_size = learning_data_frame->adc_trajectory_point_size();\n  if (past_points_size == 0) {\n    AERROR << \"No current trajectory point status\";\n    return false;\n  }\n\n  auto input_renderering_start_time = std::chrono::system_clock::now();\n\n  cv::Mat input_feature;\n  if (!BirdviewImgFeatureRenderer::Instance()->RenderMultiChannelEnv(\n          *learning_data_frame, &input_feature)) {\n    AERROR << \"Render multi-channel input image failed\";\n    return false;\n  }\n\n  auto input_renderering_end_time = std::chrono::system_clock::now();\n  std::chrono::duration<double> rendering_diff =\n      input_renderering_end_time - input_renderering_start_time;\n  ADEBUG << \"trajectory imitation model input renderer used time: \"\n         << rendering_diff.count() * 1000 << \" ms.\";\n\n  auto input_preprocessing_start_time = std::chrono::system_clock::now();\n\n  cv::Mat input_feature_float;\n  input_feature.convertTo(input_feature_float, CV_32F, 1.0 / 255);\n  torch::Tensor input_feature_tensor =\n      torch::from_blob(input_feature_float.data,\n                       {1, input_feature_float.rows, input_feature_float.cols,\n                        input_feature_float.channels()});\n  input_feature_tensor = input_feature_tensor.permute({0, 3, 1, 2});\n  for (int i = 0; i < input_feature_float.channels(); ++i) {\n    input_feature_tensor[0][i] = input_feature_tensor[0][i].sub(0.5).div(0.5);\n  }\n\n  const auto& current_traj_point = learning_data_frame->adc_trajectory_point(\n      learning_data_frame->adc_trajectory_point_size() - 1);\n  const auto& current_path_point =\n      current_traj_point.trajectory_point().path_point();\n  const double current_x = current_path_point.x();\n  const double current_y = current_path_point.y();\n  const double current_heading = current_path_point.theta();\n  // TODO(Jinyun): move to conf\n  const int past_history_size = 10;\n  torch::Tensor past_points_tensor = torch::zeros({1, past_history_size, 4});\n  int i = 0;\n  for (i = 0; i < learning_data_frame->adc_trajectory_point_size() &&\n              i < past_history_size;\n       ++i) {\n    const auto& trajectory_point =\n        learning_data_frame\n            ->adc_trajectory_point(\n                learning_data_frame->adc_trajectory_point_size() - 1 - i)\n            .trajectory_point();\n    const auto& path_point = trajectory_point.path_point();\n    const double x = path_point.x();\n    const double y = path_point.y();\n    const auto& relative_coords = util::WorldCoordToObjCoord(\n        std::make_pair(x, y), std::make_pair(current_x, current_y),\n        current_heading);\n    // TODO(Jinyun): to be validated. Unnormalized angle difference, aligning\n    // with offline training setups\n    const double heading_diff = path_point.theta() - current_heading;\n    const double v = trajectory_point.v();\n    past_points_tensor[0][past_history_size - 1 - i][0] = relative_coords.first;\n    past_points_tensor[0][past_history_size - 1 - i][1] =\n        relative_coords.second;\n    past_points_tensor[0][past_history_size - 1 - i][2] = heading_diff;\n    past_points_tensor[0][past_history_size - 1 - i][3] = v;\n  }\n  // Strong assumption here is the ADC always tracks past points except when the\n  // ADC starts from standstill\n  while (i < past_history_size) {\n    for (int j = 0; j < 4; ++j) {\n      past_points_tensor[0][past_history_size - 1 - i][j] =\n          past_points_tensor[0][past_history_size - i][j];\n    }\n    ++i;\n  }\n  torch::Tensor past_points_step_tensor =\n      torch::zeros({1, past_history_size, 4});\n  for (int i = 1; i < past_history_size; ++i) {\n    for (int j = 0; j < 4; ++j) {\n      past_points_step_tensor[0][i][j] =\n          past_points_tensor[0][i][j] - past_points_tensor[0][i - 1][j];\n    }\n  }\n\n  auto input_preprocessing_end_time = std::chrono::system_clock::now();\n  std::chrono::duration<double> preprocessing_diff =\n      input_preprocessing_end_time - input_preprocessing_start_time;\n  ADEBUG << \"trajectory imitation model input preprocessing used time: \"\n         << preprocessing_diff.count() * 1000 << \" ms.\";",
        "base_contents": "  auto input_prepration_end_time = std::chrono::system_clock::now();\n  std::chrono::duration<double> prepration_diff =\n      input_prepration_end_time - input_prepration_start_time;\n  ADEBUG << \"trajectory imitation model input prepration used time: \"\n         << prepration_diff.count() * 1000 << \" ms.\";",
        "res_region": "  auto inference_end_time = std::chrono::system_clock::now();\n  std::chrono::duration<double> inference_diff =\n      inference_end_time - inference_start_time;\n  ADEBUG << \"trajectory imitation model inference used time: \"\n         << inference_diff.count() * 1000 << \" ms.\";\n\n  const auto& cur_traj_point =\n      learning_data_frame->adc_trajectory_point(past_points_size - 1);\n  const double cur_time_sec = cur_traj_point.timestamp_sec();\n  const auto& cur_path_point = cur_traj_point.trajectory_point().path_point();\n  const double cur_x = cur_path_point.x();\n  const double cur_y = cur_path_point.y();\n  const double cur_heading = cur_path_point.theta();\n  ADEBUG << \"cur_x[\" << cur_x << \"], cur_y[\" << cur_y << \"], cur_heading[\"\n         << cur_heading << \"], cur_v[\" << cur_traj_point.trajectory_point().v()\n         << \"]\";\n\n  learning_data_frame->mutable_output()->clear_adc_future_trajectory_point();\n  const double delta_t = config_.trajectory_delta_t();\n  auto torch_output = torch_output_tensor.accessor<float, 3>();\n  for (int i = 0; i < torch_output_tensor.size(1); ++i) {\n    const double dx = static_cast<double>(torch_output[0][i][0]);\n    const double dy = static_cast<double>(torch_output[0][i][1]);\n    const double dtheta = static_cast<double>(torch_output[0][i][2]);\n    const double v = static_cast<double>(torch_output[0][i][3]);\n    ADEBUG << \"dx[\" << dx << \"], dy[\" << dy << \"], dtheta[\" << dtheta << \"], v[\"\n           << v << \"]\";\n    const double time_sec = cur_time_sec + delta_t * (i + 1);\n    apollo::common::math::Vec2d offset(dx, dy);\n    apollo::common::math::Vec2d rotated_offset = offset.rotate(cur_heading);\n    const double x = cur_x + rotated_offset.x();\n    const double y = cur_y + rotated_offset.y();\n    const double heading =\n        apollo::common::math::NormalizeAngle(dtheta + cur_heading);\n\n    auto* traj_point = learning_data_frame->mutable_output()\n                           ->add_adc_future_trajectory_point();\n    traj_point->set_timestamp_sec(time_sec);\n    traj_point->mutable_trajectory_point()->mutable_path_point()->set_x(x);\n    traj_point->mutable_trajectory_point()->mutable_path_point()->set_y(y);\n    traj_point->mutable_trajectory_point()->mutable_path_point()->set_theta(\n        heading);\n    traj_point->mutable_trajectory_point()->set_v(v);\n    traj_point->mutable_trajectory_point()->set_relative_time(delta_t *\n                                                              (i + 1));\n  }\n\n  return true;\n}\n\nbool TrajectoryImitationLibtorchInference::DoCNNMODELInference(\n    LearningDataFrame* learning_data_frame) {\n  const int past_points_size = learning_data_frame->adc_trajectory_point_size();\n  if (past_points_size == 0) {\n    AERROR << \"No current trajectory point status\";\n    return false;\n  }\n\n  auto input_renderering_start_time = std::chrono::system_clock::now();\n\n  cv::Mat input_feature;\n  if (!BirdviewImgFeatureRenderer::Instance()->RenderMultiChannelEnv(\n          *learning_data_frame, &input_feature)) {\n    AERROR << \"Render multi-channel input image failed\";\n    return false;\n  }\n\n  auto input_renderering_end_time = std::chrono::system_clock::now();\n  std::chrono::duration<double> rendering_diff =\n      input_renderering_end_time - input_renderering_start_time;\n  ADEBUG << \"trajectory imitation model input renderer used time: \"\n         << rendering_diff.count() * 1000 << \" ms.\";\n\n  auto input_preprocessing_start_time = std::chrono::system_clock::now();\n\n  cv::Mat input_feature_float;\n  input_feature.convertTo(input_feature_float, CV_32F, 1.0 / 255);\n  torch::Tensor input_feature_tensor =\n      torch::from_blob(input_feature_float.data,\n                       {1, input_feature_float.rows, input_feature_float.cols,\n                        input_feature_float.channels()});\n  input_feature_tensor = input_feature_tensor.permute({0, 3, 1, 2});\n  for (int i = 0; i < input_feature_float.channels(); ++i) {\n    input_feature_tensor[0][i] = input_feature_tensor[0][i].sub(0.5).div(0.5);\n  }\n\n  auto input_preprocessing_end_time = std::chrono::system_clock::now();\n  std::chrono::duration<double> preprocessing_diff =\n      input_preprocessing_end_time - input_preprocessing_start_time;\n  ADEBUG << \"trajectory imitation model input preprocessing used time: \"\n         << preprocessing_diff.count() * 1000 << \" ms.\";\n\n  auto inference_start_time = std::chrono::system_clock::now();\n\n  std::vector<torch::jit::IValue> torch_inputs;\n  torch_inputs.push_back(input_feature_tensor.to(device_));\n  at::Tensor torch_output_tensor =\n      model_.forward(torch_inputs).toTensor().to(torch::kCPU);\n\n  auto inference_end_time = std::chrono::system_clock::now();\n  std::chrono::duration<double> inference_diff =\n      inference_end_time - inference_start_time;\n  ADEBUG << \"trajectory imitation model inference used time: \"\n         << inference_diff.count() * 1000 << \" ms.\";\n\n  const auto& cur_traj_point =\n      learning_data_frame->adc_trajectory_point(past_points_size - 1);\n  const double cur_time_sec = cur_traj_point.timestamp_sec();\n  const auto& cur_path_point = cur_traj_point.trajectory_point().path_point();\n  const double cur_x = cur_path_point.x();\n  const double cur_y = cur_path_point.y();\n  const double cur_heading = cur_path_point.theta();\n  ADEBUG << \"cur_x[\" << cur_x << \"], cur_y[\" << cur_y << \"], cur_heading[\"\n         << cur_heading << \"], cur_v[\" << cur_traj_point.trajectory_point().v()\n         << \"]\";\n\n  learning_data_frame->mutable_output()->clear_adc_future_trajectory_point();\n  const double delta_t = config_.trajectory_delta_t();\n  auto torch_output = torch_output_tensor.accessor<float, 3>();\n  for (int i = 0; i < torch_output_tensor.size(1); ++i) {\n    const double dx = static_cast<double>(torch_output[0][i][0]);\n    const double dy = static_cast<double>(torch_output[0][i][1]);\n    const double dtheta = static_cast<double>(torch_output[0][i][2]);\n    const double v = static_cast<double>(torch_output[0][i][3]);\n    ADEBUG << \"dx[\" << dx << \"], dy[\" << dy << \"], dtheta[\" << dtheta << \"], v[\"\n           << v << \"]\";\n    const double time_sec = cur_time_sec + delta_t * (i + 1);\n    apollo::common::math::Vec2d offset(dx, dy);\n    apollo::common::math::Vec2d rotated_offset = offset.rotate(cur_heading);\n    const double x = cur_x + rotated_offset.x();\n    const double y = cur_y + rotated_offset.y();\n    const double heading =\n        apollo::common::math::NormalizeAngle(dtheta + cur_heading);\n\n    auto* traj_point = learning_data_frame->mutable_output()\n                           ->add_adc_future_trajectory_point();\n    traj_point->set_timestamp_sec(time_sec);\n    traj_point->mutable_trajectory_point()->mutable_path_point()->set_x(x);\n    traj_point->mutable_trajectory_point()->mutable_path_point()->set_y(y);\n    traj_point->mutable_trajectory_point()->mutable_path_point()->set_theta(\n        heading);\n    traj_point->mutable_trajectory_point()->set_v(v);\n    traj_point->mutable_trajectory_point()->set_relative_time(delta_t *\n                                                              (i + 1));\n  }\n\n  return true;\n}\n\nbool TrajectoryImitationLibtorchInference::DoCNNLSTMMODELInference(\n    LearningDataFrame* learning_data_frame) {\n  const int past_points_size = learning_data_frame->adc_trajectory_point_size();\n  if (past_points_size == 0) {\n    AERROR << \"No current trajectory point status\";\n    return false;\n  }\n\n  auto input_renderering_start_time = std::chrono::system_clock::now();\n\n  cv::Mat input_feature;\n  if (!BirdviewImgFeatureRenderer::Instance()->RenderMultiChannelEnv(\n          *learning_data_frame, &input_feature)) {\n    AERROR << \"Render multi-channel input image failed\";\n    return false;\n  }\n\n  auto input_renderering_end_time = std::chrono::system_clock::now();\n  std::chrono::duration<double> rendering_diff =\n      input_renderering_end_time - input_renderering_start_time;\n  ADEBUG << \"trajectory imitation model input renderer used time: \"\n         << rendering_diff.count() * 1000 << \" ms.\";\n\n  auto input_preprocessing_start_time = std::chrono::system_clock::now();\n\n  cv::Mat input_feature_float;\n  input_feature.convertTo(input_feature_float, CV_32F, 1.0 / 255);\n  torch::Tensor input_feature_tensor =\n      torch::from_blob(input_feature_float.data,\n                       {1, input_feature_float.rows, input_feature_float.cols,\n                        input_feature_float.channels()});\n  input_feature_tensor = input_feature_tensor.permute({0, 3, 1, 2});\n  for (int i = 0; i < input_feature_float.channels(); ++i) {\n    input_feature_tensor[0][i] = input_feature_tensor[0][i].sub(0.5).div(0.5);\n  }\n\n  const auto& current_traj_point = learning_data_frame->adc_trajectory_point(\n      learning_data_frame->adc_trajectory_point_size() - 1);\n  const auto& current_path_point =\n      current_traj_point.trajectory_point().path_point();\n  const double current_x = current_path_point.x();\n  const double current_y = current_path_point.y();\n  const double current_heading = current_path_point.theta();\n  // TODO(Jinyun): move to conf\n  const int past_history_size = 10;\n  torch::Tensor past_points_tensor = torch::zeros({1, past_history_size, 4});\n  int i = 0;\n  for (i = 0; i < learning_data_frame->adc_trajectory_point_size() &&\n              i < past_history_size;\n       ++i) {\n    const auto& trajectory_point =\n        learning_data_frame\n            ->adc_trajectory_point(\n                learning_data_frame->adc_trajectory_point_size() - 1 - i)\n            .trajectory_point();\n    const auto& path_point = trajectory_point.path_point();\n    const double x = path_point.x();\n    const double y = path_point.y();\n    const auto& relative_coords = util::WorldCoordToObjCoord(\n        std::make_pair(x, y), std::make_pair(current_x, current_y),\n        current_heading);\n    // TODO(Jinyun): to be validated. Unnormalized angle difference, aligning\n    // with offline training setups\n    const double heading_diff = path_point.theta() - current_heading;\n    const double v = trajectory_point.v();\n    past_points_tensor[0][past_history_size - 1 - i][0] = relative_coords.first;\n    past_points_tensor[0][past_history_size - 1 - i][1] =\n        relative_coords.second;\n    past_points_tensor[0][past_history_size - 1 - i][2] = heading_diff;\n    past_points_tensor[0][past_history_size - 1 - i][3] = v;\n  }\n  // Strong assumption here is the ADC always tracks past points except when the\n  // ADC starts from standstill\n  while (i < past_history_size) {\n    for (int j = 0; j < 4; ++j) {\n      past_points_tensor[0][past_history_size - 1 - i][j] =\n          past_points_tensor[0][past_history_size - i][j];\n    }\n    ++i;\n  }\n  torch::Tensor past_points_step_tensor =\n      torch::zeros({1, past_history_size, 4});\n  for (int i = 1; i < past_history_size; ++i) {\n    for (int j = 0; j < 4; ++j) {\n      past_points_step_tensor[0][i][j] =\n          past_points_tensor[0][i][j] - past_points_tensor[0][i - 1][j];\n    }\n  }\n\n  auto input_preprocessing_end_time = std::chrono::system_clock::now();\n  std::chrono::duration<double> preprocessing_diff =\n      input_preprocessing_end_time - input_preprocessing_start_time;\n  ADEBUG << \"trajectory imitation model input preprocessing used time: \"\n         << preprocessing_diff.count() * 1000 << \" ms.\";",
        "lookback": "  std::chrono::duration<double> inference_diff =",
        "lookahead": "",
        "label": [
            "base",
            "complex_conflict"
        ],
        "start_line": 384,
        "end_line": 691,
        "merge_gen_region": null,
        "confidence": 0.809474766254425,
        "hesitated": true
    },
    {
        "a_contents": "\n  std::vector<torch::jit::IValue> torch_inputs;\n  torch_inputs.push_back(input_feature_tensor.to(device_));",
        "b_contents": "\n  std::vector<torch::jit::IValue> torch_inputs;\n  torch_inputs.push_back(c10::ivalue::Tuple::create(\n      {std::move(input_feature_tensor.to(device_)),\n       std::move(past_points_tensor.to(device_)),\n       std::move(past_points_step_tensor.to(device_))},\n      c10::TupleType::create(\n          std::vector<c10::TypePtr>(3, c10::TensorType::create()))));",
        "base_contents": "",
        "res_region": "",
        "lookback": "  auto inference_start_time = std::chrono::system_clock::now();",
        "lookahead": "",
        "label": [
            "theirs",
            "one_side_deletion"
        ],
        "start_line": 694,
        "end_line": 708,
        "merge_gen_region": "::<stdvectorstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstdstd",
        "confidence": 0.8822704553604126,
        "hesitated": false,
        "resolved": false
    },
    {
        "a_contents": "bool TrajectoryImitationLibtorchInference::DoCNNLSTMMODELInference(\n    LearningDataFrame* learning_data_frame) {\n  const int past_points_size = learning_data_frame->adc_trajectory_point_size();\n  if (past_points_size == 0) {\n    AERROR << \"No current trajectory point status\";\n    return false;\n  }\n\n  auto input_renderering_start_time = std::chrono::system_clock::now();\n\n  cv::Mat input_feature;\n  if (!BirdviewImgFeatureRenderer::Instance()->RenderMultiChannelEnv(\n          *learning_data_frame, &input_feature)) {\n    AERROR << \"Render multi-channel input image failed\";\n    return false;\n  }\n\n  auto input_renderering_end_time = std::chrono::system_clock::now();\n  std::chrono::duration<double> rendering_diff =\n      input_renderering_end_time - input_renderering_start_time;\n  ADEBUG << \"trajectory imitation model input renderer used time: \"\n         << rendering_diff.count() * 1000 << \" ms.\";\n\n  auto input_preprocessing_start_time = std::chrono::system_clock::now();\n\n  cv::Mat input_feature_float;\n  input_feature.convertTo(input_feature_float, CV_32F, 1.0 / 255);\n  torch::Tensor input_feature_tensor =\n      torch::from_blob(input_feature_float.data,\n                       {1, input_feature_float.rows, input_feature_float.cols,\n                        input_feature_float.channels()});\n  input_feature_tensor = input_feature_tensor.permute({0, 3, 1, 2});\n  for (int i = 0; i < input_feature_float.channels(); ++i) {\n    input_feature_tensor[0][i] = input_feature_tensor[0][i].sub(0.5).div(0.5);\n  }\n\n  const auto& current_traj_point = learning_data_frame->adc_trajectory_point(\n      learning_data_frame->adc_trajectory_point_size() - 1);\n  const auto& current_path_point =\n      current_traj_point.trajectory_point().path_point();\n  const double current_x = current_path_point.x();\n  const double current_y = current_path_point.y();\n  const double current_heading = current_path_point.theta();\n  // TODO(Jinyun): move to conf\n  const int past_history_size = 10;\n  torch::Tensor past_points_tensor = torch::zeros({1, past_history_size, 4});\n  int i = 0;\n  for (i = 0; i < learning_data_frame->adc_trajectory_point_size() &&\n              i < past_history_size;\n       ++i) {\n    const auto& trajectory_point =\n        learning_data_frame\n            ->adc_trajectory_point(\n                learning_data_frame->adc_trajectory_point_size() - 1 - i)\n            .trajectory_point();\n    const auto& path_point = trajectory_point.path_point();\n    const double x = path_point.x();\n    const double y = path_point.y();\n    const auto& relative_coords = util::WorldCoordToObjCoord(\n        std::make_pair(x, y), std::make_pair(current_x, current_y),\n        current_heading);\n    // TODO(Jinyun): to be validated. Unnormalized angle difference, aligning\n    // with offline training setups\n    const double heading_diff = path_point.theta() - current_heading;\n    const double v = trajectory_point.v();\n    past_points_tensor[0][past_history_size - 1 - i][0] = relative_coords.first;\n    past_points_tensor[0][past_history_size - 1 - i][1] =\n        relative_coords.second;\n    past_points_tensor[0][past_history_size - 1 - i][2] = heading_diff;\n    past_points_tensor[0][past_history_size - 1 - i][3] = v;\n  }\n  // Strong assumption here is the ADC always tracks past points except when the\n  // ADC starts from standstill\n  while (i < past_history_size) {\n    for (int j = 0; j < 4; ++j) {\n      past_points_tensor[0][past_history_size - 1 - i][j] =\n          past_points_tensor[0][past_history_size - i][j];\n    }\n    ++i;\n  }\n  torch::Tensor past_points_step_tensor =\n      torch::zeros({1, past_history_size, 4});\n  for (int i = 1; i < past_history_size; ++i) {\n    for (int j = 0; j < 4; ++j) {\n      past_points_step_tensor[0][i][j] =\n          past_points_tensor[0][i][j] - past_points_tensor[0][i - 1][j];\n    }\n  }\n\n  auto input_preprocessing_end_time = std::chrono::system_clock::now();\n  std::chrono::duration<double> preprocessing_diff =\n      input_preprocessing_end_time - input_preprocessing_start_time;\n  ADEBUG << \"trajectory imitation model input preprocessing used time: \"\n         << preprocessing_diff.count() * 1000 << \" ms.\";\n\n  auto inference_start_time = std::chrono::system_clock::now();\n\n  std::vector<torch::jit::IValue> torch_inputs;\n  torch_inputs.push_back(c10::ivalue::Tuple::create(\n      {std::move(input_feature_tensor.to(device_)),\n       std::move(past_points_tensor.to(device_)),\n       std::move(past_points_step_tensor.to(device_))}));\n  at::Tensor torch_output_tensor =\n      model_.forward(torch_inputs).toTensor().to(torch::kCPU);\n\n  auto inference_end_time = std::chrono::system_clock::now();\n  std::chrono::duration<double> inference_diff =\n      inference_end_time - inference_start_time;\n  ADEBUG << \"trajectory imitation model inference used time: \"\n         << inference_diff.count() * 1000 << \" ms.\";\n\n  const auto& cur_traj_point =\n      learning_data_frame->adc_trajectory_point(past_points_size - 1);\n  const double cur_time_sec = cur_traj_point.timestamp_sec();\n  const auto& cur_path_point = cur_traj_point.trajectory_point().path_point();\n  const double cur_x = cur_path_point.x();\n  const double cur_y = cur_path_point.y();\n  const double cur_heading = cur_path_point.theta();\n  ADEBUG << \"cur_x[\" << cur_x << \"], cur_y[\" << cur_y << \"], cur_heading[\"\n         << cur_heading << \"], cur_v[\" << cur_traj_point.trajectory_point().v()\n         << \"]\";\n\n  learning_data_frame->mutable_output()->clear_adc_future_trajectory_point();\n  const double delta_t = config_.trajectory_delta_t();\n  auto torch_output = torch_output_tensor.accessor<float, 3>();\n  for (int i = 0; i < torch_output_tensor.size(1); ++i) {\n    const double dx = static_cast<double>(torch_output[0][i][0]);\n    const double dy = static_cast<double>(torch_output[0][i][1]);\n    const double dtheta = static_cast<double>(torch_output[0][i][2]);\n    const double v = static_cast<double>(torch_output[0][i][3]);\n    ADEBUG << \"dx[\" << dx << \"], dy[\" << dy << \"], dtheta[\" << dtheta << \"], v[\"\n           << v << \"]\";\n    const double time_sec = cur_time_sec + delta_t * (i + 1);\n    apollo::common::math::Vec2d offset(dx, dy);\n    apollo::common::math::Vec2d rotated_offset = offset.rotate(cur_heading);\n    const double x = cur_x + rotated_offset.x();\n    const double y = cur_y + rotated_offset.y();\n    const double heading =\n        apollo::common::math::NormalizeAngle(dtheta + cur_heading);\n\n    auto* traj_point = learning_data_frame->mutable_output()\n                           ->add_adc_future_trajectory_point();\n    traj_point->set_timestamp_sec(time_sec);\n    traj_point->mutable_trajectory_point()->mutable_path_point()->set_x(x);\n    traj_point->mutable_trajectory_point()->mutable_path_point()->set_y(y);\n    traj_point->mutable_trajectory_point()->mutable_path_point()->set_theta(\n        heading);\n    traj_point->mutable_trajectory_point()->set_v(v);\n    traj_point->mutable_trajectory_point()->set_relative_time(delta_t *\n                                                              (i + 1));\n  }\n\n  return true;\n}\n\nbool TrajectoryImitationLibtorchInference::DoInference(\n    LearningDataFrame* learning_data_frame) {\n  switch (config_.model_type()) {\n    case LearningModelInferenceTaskConfig::CONV_RNN: {\n      if (!DoCONVRNNMODELInference(learning_data_frame)) {\n        return false;\n      }\n      break;\n    }\n    case LearningModelInferenceTaskConfig::CNN: {\n      if (!DoCNNMODELInference(learning_data_frame)) {\n        return false;\n      }\n      break;\n    }\n    case LearningModelInferenceTaskConfig::CNN_LSTM: {\n      if (!DoCNNLSTMMODELInference(learning_data_frame)) {\n        return false;\n      }\n      break;\n    }\n    default: {\n      AERROR << \"Configured model type not defined and implemented\";\n      break;\n    }\n  }\n  return true;\n}",
        "b_contents": "bool TrajectoryImitationLibtorchInference::DoInference(\n    LearningDataFrame* learning_data_frame) {\n  switch (config_.model_type()) {\n    case LearningModelInferenceTaskConfig::CONV_RNN: {\n      if (!DoCONVRNNMODELInference(learning_data_frame)) {\n        return false;\n      }\n      break;\n    }\n    case LearningModelInferenceTaskConfig::CNN: {\n      if (!DoCNNMODELInference(learning_data_frame)) {\n        return false;\n      }\n      break;\n    }\n    case LearningModelInferenceTaskConfig::CNN_LSTM: {\n      if (!DoCNNLSTMMODELInference(learning_data_frame)) {\n        return false;\n      }\n      break;\n    }\n    default: {\n      AERROR << \"Configured model type not defined and implemented\";\n      break;\n    }\n  }\n  return true;\n}",
        "base_contents": "",
        "res_region": "\n  auto inference_start_time = std::chrono::system_clock::now();\n\n  std::vector<torch::jit::IValue> torch_inputs;\n  torch_inputs.push_back(c10::ivalue::Tuple::create(\n      {std::move(input_feature_tensor.to(device_)),\n       std::move(past_points_tensor.to(device_)),\n       std::move(past_points_step_tensor.to(device_))}));\n  at::Tensor torch_output_tensor =\n      model_.forward(torch_inputs).toTensor().to(torch::kCPU);\n\n  auto inference_end_time = std::chrono::system_clock::now();\n  std::chrono::duration<double> inference_diff =\n      inference_end_time - inference_start_time;\n  ADEBUG << \"trajectory imitation model inference used time: \"\n         << inference_diff.count() * 1000 << \" ms.\";\n\n  const auto& cur_traj_point =\n      learning_data_frame->adc_trajectory_point(past_points_size - 1);\n  const double cur_time_sec = cur_traj_point.timestamp_sec();\n  const auto& cur_path_point = cur_traj_point.trajectory_point().path_point();\n  const double cur_x = cur_path_point.x();\n  const double cur_y = cur_path_point.y();\n  const double cur_heading = cur_path_point.theta();\n  ADEBUG << \"cur_x[\" << cur_x << \"], cur_y[\" << cur_y << \"], cur_heading[\"\n         << cur_heading << \"], cur_v[\" << cur_traj_point.trajectory_point().v()\n         << \"]\";\n\n  learning_data_frame->mutable_output()->clear_adc_future_trajectory_point();\n  const double delta_t = config_.trajectory_delta_t();\n  auto torch_output = torch_output_tensor.accessor<float, 3>();\n  for (int i = 0; i < torch_output_tensor.size(1); ++i) {\n    const double dx = static_cast<double>(torch_output[0][i][0]);\n    const double dy = static_cast<double>(torch_output[0][i][1]);\n    const double dtheta = static_cast<double>(torch_output[0][i][2]);\n    const double v = static_cast<double>(torch_output[0][i][3]);\n    ADEBUG << \"dx[\" << dx << \"], dy[\" << dy << \"], dtheta[\" << dtheta << \"], v[\"\n           << v << \"]\";\n    const double time_sec = cur_time_sec + delta_t * (i + 1);\n    apollo::common::math::Vec2d offset(dx, dy);\n    apollo::common::math::Vec2d rotated_offset = offset.rotate(cur_heading);\n    const double x = cur_x + rotated_offset.x();\n    const double y = cur_y + rotated_offset.y();\n    const double heading =\n        apollo::common::math::NormalizeAngle(dtheta + cur_heading);\n\n    auto* traj_point = learning_data_frame->mutable_output()\n                           ->add_adc_future_trajectory_point();\n    traj_point->set_timestamp_sec(time_sec);\n    traj_point->mutable_trajectory_point()->mutable_path_point()->set_x(x);\n    traj_point->mutable_trajectory_point()->mutable_path_point()->set_y(y);\n    traj_point->mutable_trajectory_point()->mutable_path_point()->set_theta(\n        heading);\n    traj_point->mutable_trajectory_point()->set_v(v);\n    traj_point->mutable_trajectory_point()->set_relative_time(delta_t *\n                                                              (i + 1));\n  }\n\n  return true;\n}\n\nbool TrajectoryImitationLibtorchInference::DoInference(\n    LearningDataFrame* learning_data_frame) {\n  switch (config_.model_type()) {\n    case LearningModelInferenceTaskConfig::CONV_RNN: {\n      if (!DoCONVRNNMODELInference(learning_data_frame)) {\n        return false;\n      }\n      break;\n    }\n    case LearningModelInferenceTaskConfig::CNN: {\n      if (!DoCNNMODELInference(learning_data_frame)) {\n        return false;\n      }\n      break;\n    }\n    case LearningModelInferenceTaskConfig::CNN_LSTM: {\n      if (!DoCNNLSTMMODELInference(learning_data_frame)) {\n        return false;\n      }\n      break;\n    }\n    default: {\n      AERROR << \"Configured model type not defined and implemented\";\n      break;\n    }\n  }\n  return true;\n}",
        "lookback": "  auto inference_start_time = std::chrono::system_clock::now();",
        "lookahead": "}  // namespace planning",
        "label": [
            "interleave",
            "one_side_deletion"
        ],
        "start_line": 762,
        "end_line": 976,
        "merge_gen_region": " inferencestarttime std::::::_::::(); TraectorymitationtorchferenceDoNLMODELference LningData*arningdataframe { const pastpointssize le___frameadctraectorypointpointsize ifp__ == 0 {ERROR \" current tr point\"; false } inputrenderer_ingstarttime stdchronosystemclocknow cvMat_; (!irdImgRenderer::()->MultiEnv *ningdataframe &_)) A <<Render- input failed return; auto_ingendtime stdchronosystemclocknow stdchronodurationdouble renderingdiff inputrenderer_ingendtime input_ingstarttime ADEBUG \"jectorymitation input renderer time \" renderingdiffcount * << ms auto_processingstarttime stdchronosystemclocknow();:::: inputfeaturefloat inputfeatureconvert(__,_F 10 255);chTensor___ =ch::__(__,1 inputfeaturefloatrows inputfeaturefloatcols inputfeaturefloatchannels});___ =____.ute0 3 1 2 forint =; <___.(); ++) inputfeaturetensor0][] =__[][].(.).05div05 } auto currenttra_ =zipdataframe",
        "confidence": 0.9156309366226196,
        "hesitated": false,
        "resolved": false
    }
]