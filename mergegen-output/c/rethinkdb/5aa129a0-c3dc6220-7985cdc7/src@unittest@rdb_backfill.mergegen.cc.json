[
    {
        "a_contents": "TPTEST(RDBBackfill, FillPreItemQueue) {\n    /* Force the pre-item queue to fill up, but make the other two queues unlimited. */\n    backfill_test_config_t cfg;\n    cfg.backfill.pre_item_queue_mem_size = 1;\n    cfg.backfill.pre_item_chunk_mem_size = 1;\n    /* Since the pre-item queue will stall a lot, this backfill will be much slower than\n    normal; so we backfill fewer items to speed up the test. */\n    cfg.num_initial_writes = 100;\n    cfg.num_step_writes = 100;\n    run_backfill_test(cfg);\n}\n\nTPTEST(RDBBackfill, FillWriteQueue) {\n    /* Force the write queue to fill up so that the backfill repeatedly switches between\n    streaming and backfilling modes. */\n    backfill_test_config_t cfg;\n    cfg.backfill.write_queue_count = 3;\n    run_backfill_test(cfg);",
        "b_contents": "    /* Start sending operations to the broadcaster */\n    std::map<std::string, std::string> inserter_state;\n    test_inserter_t inserter(\n        std::bind(&write_to_broadcaster, 0, broadcaster->get(),\n                  ph::_1, ph::_2, ph::_3, ph::_4),\n        std::function<std::string(const std::string &, order_token_t, signal_t *)>(),\n        &mc_key_gen,\n        order_source,\n        \"rdb_backfill run_partial_backfill_test inserter\",\n        &inserter_state);\n    nap(10000);\n\n    /* Set up a second mirror */\n    test_store_t store2(io_backender, order_source, ctx);\n    cond_t interruptor;\n    listener_t listener2(\n        base_path_t(\".\"),\n        io_backender,\n        cluster->get_mailbox_manager(),\n        generate_uuid(),\n        &backfill_throttler,\n        broadcaster_metadata_view,\n        branch_history_manager,\n        &store2.store,\n        replier_business_card_variable.get_watchable(),\n        &get_global_perfmon_collection(),\n        &interruptor,\n        order_source,\n        nullptr);\n\n    EXPECT_FALSE((*initial_listener)->get_broadcaster_lost_signal()->is_pulsed());\n    EXPECT_FALSE(listener2.get_broadcaster_lost_signal()->is_pulsed());\n\n    nap(10000);\n\n    /* Stop the inserter, then let any lingering writes finish */\n    inserter.stop();\n    /* Let any lingering writes finish */\n    // TODO: 100 seconds?\n    nap(100000);\n\n    for (std::map<std::string, std::string>::iterator it = inserter_state.begin();\n            it != inserter_state.end(); it++) {\n        rapidjson::Document sindex_key_json;\n        sindex_key_json.Parse(it->second.c_str());\n        auto sindex_key_literal = ql::to_datum(sindex_key_json,\n                                               ql::configured_limits_t(),\n                                               reql_version_t::LATEST);\n        read_t read = make_sindex_read(sindex_key_literal, id);\n        fake_fifo_enforcement_t enforce;\n        fifo_enforcer_sink_t::exit_read_t exiter(&enforce.sink, enforce.source.enter_read());\n        cond_t non_interruptor;\n        read_response_t response;\n        broadcaster->get()->read(read, &response, &exiter, order_source->check_in(\"unittest::(rdb)run_partial_backfill_test\").with_read_mode(), &non_interruptor);\n        rget_read_response_t get_result = boost::get<rget_read_response_t>(response.response);\n        auto groups = boost::get<ql::grouped_t<ql::stream_t> >(&get_result.result);\n        ASSERT_TRUE(groups != NULL);\n        ASSERT_EQ(1, groups->size());\n        // Order doesn't matter because groups->size() is 1.\n        auto result_stream = &groups->begin()->second;\n        ASSERT_EQ(1u, result_stream->size());\n        EXPECT_EQ(generate_document(0, it->second), result_stream->at(0).data);\n    }",
        "base_contents": "    /* Start sending operations to the broadcaster */\n    std::map<std::string, std::string> inserter_state;\n    test_inserter_t inserter(\n        std::bind(&write_to_broadcaster, 0, broadcaster->get(),\n                  ph::_1, ph::_2, ph::_3, ph::_4),\n        std::function<std::string(const std::string &, order_token_t, signal_t *)>(),\n        &mc_key_gen,\n        order_source,\n        \"rdb_backfill run_partial_backfill_test inserter\",\n        &inserter_state);\n    nap(10000);\n\n    /* Set up a second mirror */\n    test_store_t store2(io_backender, order_source, ctx);\n    cond_t interruptor;\n    listener_t listener2(\n        base_path_t(\".\"),\n        io_backender,\n        cluster->get_mailbox_manager(),\n        generate_uuid(),\n        &backfill_throttler,\n        broadcaster_metadata_view,\n        branch_history_manager,\n        &store2.store,\n        replier_business_card_variable.get_watchable(),\n        &get_global_perfmon_collection(),\n        &interruptor,\n        order_source,\n        nullptr);\n\n    EXPECT_FALSE((*initial_listener)->get_broadcaster_lost_signal()->is_pulsed());\n    EXPECT_FALSE(listener2.get_broadcaster_lost_signal()->is_pulsed());\n\n    nap(10000);\n\n    /* Stop the inserter, then let any lingering writes finish */\n    inserter.stop();\n    /* Let any lingering writes finish */\n    // TODO: 100 seconds?\n    nap(100000);\n\n    for (std::map<std::string, std::string>::iterator it = inserter_state.begin();\n            it != inserter_state.end(); it++) {\n        rapidjson::Document sindex_key_json;\n        sindex_key_json.Parse(it->second.c_str());\n        auto sindex_key_literal = ql::to_datum(sindex_key_json,\n                                               ql::configured_limits_t(),\n                                               reql_version_t::LATEST);\n        read_t read = make_sindex_read(sindex_key_literal, id);\n        fake_fifo_enforcement_t enforce;\n        fifo_enforcer_sink_t::exit_read_t exiter(&enforce.sink, enforce.source.enter_read());\n        cond_t non_interruptor;\n        read_response_t response;\n        broadcaster->get()->read(read, &response, &exiter, order_source->check_in(\"unittest::(rdb)run_partial_backfill_test\").with_read_mode(), &non_interruptor);\n        rget_read_response_t get_result = boost::get<rget_read_response_t>(response.response);\n        auto groups = boost::get<ql::grouped_t<ql::stream_t> >(&get_result.result);\n        ASSERT_TRUE(groups != NULL);\n        ASSERT_EQ(1, groups->size());\n        // Order doesn't matter because groups->size() is 1.\n        auto result_stream = &groups->begin(ql::grouped::order_doesnt_matter_t())->second;\n        ASSERT_EQ(1u, result_stream->size());\n        EXPECT_EQ(generate_document(0, it->second), result_stream->at(0).data);\n    }",
        "res_region": "TPTEST(RDBBackfill, FillPreItemQueue) {\n    /* Force the pre-item queue to fill up, but make the other two queues unlimited. */\n    backfill_test_config_t cfg;\n    cfg.backfill.pre_item_queue_mem_size = 1;\n    cfg.backfill.pre_item_chunk_mem_size = 1;\n    /* Since the pre-item queue will stall a lot, this backfill will be much slower than\n    normal; so we backfill fewer items to speed up the test. */\n    cfg.num_initial_writes = 100;\n    cfg.num_step_writes = 100;\n    run_backfill_test(cfg);\n}\n\nTPTEST(RDBBackfill, FillWriteQueue) {\n    /* Force the write queue to fill up so that the backfill repeatedly switches between\n    streaming and backfilling modes. */\n    backfill_test_config_t cfg;\n    cfg.backfill.write_queue_count = 3;\n    run_backfill_test(cfg);",
        "lookback": "    /* Force the pre-item queue to fill up, but make the other two queues unlimited. */",
        "lookahead": "}",
        "label": [
            "ours",
            "complex_conflict"
        ],
        "start_line": 424,
        "end_line": 571,
        "merge_gen_region": null,
        "confidence": 0.87387615442276,
        "hesitated": true
    }
]