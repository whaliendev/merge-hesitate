[
    {
        "a_contents": "#include <unistd.h>\n#include <fstream>\n#include <numeric>\n\n#ifndef __BIONIC__\n#ifndef MREMAP_DONTUNMAP\n#define MREMAP_DONTUNMAP 4\n#endif\n#ifndef __NR_userfaultfd\n#if defined(__x86_64__)\n#define __NR_userfaultfd 323\n#elif defined(__i386__)\n#define __NR_userfaultfd 374\n#elif defined(__aarch64__)\n#define __NR_userfaultfd 282\n#elif defined(__arm__)\n#define __NR_userfaultfd 388\n#else\n#error \"__NR_userfaultfd undefined\"\n#endif\n#endif  // __NR_userfaultfd\n#endif  // __BIONIC__",
        "b_contents": "#include <unistd.h>\n#include <fstream>\n#include <numeric>",
        "base_contents": "",
        "res_region": "#include <unistd.h>\n#include <fstream>\n#include <numeric>\n\n#ifndef __BIONIC__\n#ifndef MREMAP_DONTUNMAP\n#define MREMAP_DONTUNMAP 4\n#endif\n#ifndef __NR_userfaultfd\n#if defined(__x86_64__)\n#define __NR_userfaultfd 323\n#elif defined(__i386__)\n#define __NR_userfaultfd 374\n#elif defined(__aarch64__)\n#define __NR_userfaultfd 282\n#elif defined(__arm__)\n#define __NR_userfaultfd 388\n#else\n#error \"__NR_userfaultfd undefined\"\n#endif\n#endif  // __NR_userfaultfd\n#endif  // __BIONIC__",
        "lookback": "#include <fstream>",
        "lookahead": "",
        "label": [
            "ours",
            "one_side_deletion"
        ],
        "start_line": 37,
        "end_line": 65,
        "merge_gen_region": null,
        "confidence": 0.41251450777053833,
        "hesitated": true
    },
    {
        "a_contents": "// Turn off kCheckLocks when profiling the GC as it slows down the GC",
        "b_contents": "#ifndef __BIONIC__\n#ifndef MREMAP_DONTUNMAP\n#define MREMAP_DONTUNMAP 4\n#endif\n#ifndef __NR_userfaultfd\n#if defined(__x86_64__)\n#define __NR_userfaultfd 323\n#elif defined(__i386__)\n#define __NR_userfaultfd 374\n#elif defined(__aarch64__)\n#define __NR_userfaultfd 282\n#elif defined(__arm__)\n#define __NR_userfaultfd 388\n#else\n#error \"__NR_userfaultfd undefined\"\n#endif\n#endif  // __NR_userfaultfd\n#endif  // __BIONIC__\n// Turn of kCheckLocks when profiling the GC as it slows down the GC",
        "base_contents": "#ifndef MREMAP_DONTUNMAP\n#define MREMAP_DONTUNMAP 4\n#endif\n// Turn of kCheckLocks when profiling the GC as it slows down the GC",
        "res_region": "// Turn off kCheckLocks when profiling the GC as it slows down the GC",
        "lookback": "// significantly.",
        "lookahead": "// significantly.",
        "label": [
            "ours",
            "complex_conflict"
        ],
        "start_line": 124,
        "end_line": 151,
        "merge_gen_region": null,
        "confidence": 0.39869874715805054,
        "hesitated": true
    },
    {
        "a_contents": "\nbool MarkCompact::CreateUserfaultfd(bool post_fork) {\n  if (post_fork || uffd_ == -1) {\n    // Don't use O_NONBLOCK as we rely on read waiting on uffd_ if there isn't\n    // any read event available. We don't use poll.\n    if (gKernelHasFaultRetry) {\n      uffd_ = syscall(__NR_userfaultfd, O_CLOEXEC | UFFD_USER_MODE_ONLY);\n      // On non-android devices we may not have the kernel patches that restrict\n      // userfaultfd to user mode. But that is not a security concern as we are\n      // on host. Therefore, attempt one more time without UFFD_USER_MODE_ONLY.\n      if (!kIsTargetAndroid && UNLIKELY(uffd_ == -1 && errno == EINVAL)) {\n        uffd_ = syscall(__NR_userfaultfd, O_CLOEXEC);\n      }\n      if (UNLIKELY(uffd_ == -1)) {\n        uffd_ = kFallbackMode;\n        LOG(WARNING) << \"Userfaultfd isn't supported (reason: \" << strerror(errno)\n                     << \") and therefore falling back to stop-the-world compaction.\";\n      } else {\n        DCHECK_GE(uffd_, 0);\n        // Get/update the features that we want in userfaultfd\n        struct uffdio_api api = {.api = UFFD_API, .features = 0};\n        CHECK_EQ(ioctl(uffd_, UFFDIO_API, &api), 0)\n              << \"ioctl_userfaultfd: API: \" << strerror(errno);\n      }\n    } else {\n      // Without fault-retry feature in the kernel we can't terminate concurrent\n      // compaction. So fallback to stop-the-world compaction.\n      uffd_ = kFallbackMode;\n    }\n  }\n  uffd_initialized_ = !post_fork || uffd_ == kFallbackMode;\n  return uffd_ >= 0;\n}",
        "b_contents": "\nbool MarkCompact::CreateUserfaultfd(bool post_fork) {\n  if (post_fork || uffd_ == -1) {\n    // Don't use O_NONBLOCK as we rely on read waiting on uffd_ if there isn't\n    // any read event available. We don't use poll.\n    uffd_ = syscall(__NR_userfaultfd, O_CLOEXEC | UFFD_USER_MODE_ONLY);\n#ifndef ART_TARGET\n    // On host we may not have the kernel patches that restrict userfaultfd to\n    // user mode. But that is not a security concern as we are on host.\n    // Therefore, attempt one more time without UFFD_USER_MODE_ONLY.\n    if (UNLIKELY(uffd_ == -1 && errno == EINVAL)) {\n      uffd_ = syscall(__NR_userfaultfd, O_CLOEXEC);\n    }\n#endif\n    if (UNLIKELY(uffd_ == -1)) {\n      uffd_ = kFallbackMode;\n      LOG(WARNING) << \"Userfaultfd isn't supported (reason: \" << strerror(errno)\n                   << \") and therefore falling back to stop-the-world compaction.\";\n    } else {\n      DCHECK_GE(uffd_, 0);\n      // Get/update the features that we want in userfaultfd\n      struct uffdio_api api = {.api = UFFD_API, .features = 0};\n      CHECK_EQ(ioctl(uffd_, UFFDIO_API, &api), 0) << \"ioctl_userfaultfd: API: \" << strerror(errno);\n    }\n  }\n  uffd_initialized_ = !post_fork || uffd_ == kFallbackMode;\n  return uffd_ >= 0;\n}",
        "base_contents": "static constexpr bool kConcurrentCompaction = false;",
        "res_region": "\nbool MarkCompact::CreateUserfaultfd(bool post_fork) {\n  if (post_fork || uffd_ == -1) {\n    // Don't use O_NONBLOCK as we rely on read waiting on uffd_ if there isn't\n    // any read event available. We don't use poll.\n    if (gKernelHasFaultRetry) {\n      uffd_ = syscall(__NR_userfaultfd, O_CLOEXEC | UFFD_USER_MODE_ONLY);\n      // On non-android devices we may not have the kernel patches that restrict\n      // userfaultfd to user mode. But that is not a security concern as we are\n      // on host. Therefore, attempt one more time without UFFD_USER_MODE_ONLY.\n      if (!kIsTargetAndroid && UNLIKELY(uffd_ == -1 && errno == EINVAL)) {\n        uffd_ = syscall(__NR_userfaultfd, O_CLOEXEC);\n      }\n      if (UNLIKELY(uffd_ == -1)) {\n        uffd_ = kFallbackMode;\n        LOG(WARNING) << \"Userfaultfd isn't supported (reason: \" << strerror(errno)\n                     << \") and therefore falling back to stop-the-world compaction.\";\n      } else {\n        DCHECK_GE(uffd_, 0);\n        // Get/update the features that we want in userfaultfd\n        struct uffdio_api api = {.api = UFFD_API, .features = 0};\n        CHECK_EQ(ioctl(uffd_, UFFDIO_API, &api), 0)\n              << \"ioctl_userfaultfd: API: \" << strerror(errno);\n      }\n    } else {\n      // Without fault-retry feature in the kernel we can't terminate concurrent\n      // compaction. So fallback to stop-the-world compaction.\n      uffd_ = kFallbackMode;\n    }\n  }\n  uffd_initialized_ = !post_fork || uffd_ == kFallbackMode;\n  return uffd_ >= 0;\n}",
        "lookback": "bool MarkCompact::CreateUserfaultfd(bool post_fork) {",
        "lookahead": "",
        "label": [
            "ours",
            "complex_conflict"
        ],
        "start_line": 155,
        "end_line": 220,
        "merge_gen_region": null,
        "confidence": 0.6571964621543884,
        "hesitated": true
    },
    {
        "a_contents": "  UpdateNonMovingSpace();\n  // fallback mode\n  if (uffd_ == kFallbackMode) {\n    CompactMovingSpace</*kFallback*/true>();\n",
        "b_contents": "  UpdateNonMovingSpace();\n  // fallback mode\n  if (uffd_ == kFallbackMode) {\n    CompactMovingSpace</*kFallback*/true>();\n  } else {\n    // We must start worker threads before resuming mutators to avoid deadlocks.\n    heap_->GetThreadPool()->StartWorkers(thread_running_gc_);\n  }\n  stack_end_ = nullptr;\n}\n\nvoid MarkCompact::KernelPreparation() {\n  TimingLogger::ScopedTiming t(__FUNCTION__, GetTimings());\n  // TODO: Create mapping's at 2MB aligned addresses to benefit from optimized\n  // mremap.\n  size_t size = bump_pointer_space_->Capacity();\n  uint8_t* begin = bump_pointer_space_->Begin();\n  void* ret = mremap(begin,\n                     size,\n                     size,\n                     MREMAP_MAYMOVE | MREMAP_FIXED | MREMAP_DONTUNMAP,\n                     from_space_begin_);\n  CHECK_EQ(ret, static_cast<void*>(from_space_begin_))\n         << \"mremap to move pages from moving space to from-space failed: \" << strerror(errno)\n         << \". moving-space-addr=\" << reinterpret_cast<void*>(begin)\n         << \" size=\" << size;\n\n  DCHECK_EQ(mprotect(from_space_begin_, size, PROT_READ), 0)\n         << \"mprotect failed: \" << strerror(errno);\n\n  if (uffd_ >= 0) {\n    // Userfaultfd registration\n    struct uffdio_register uffd_register;\n    uffd_register.range.start = reinterpret_cast<uintptr_t>(begin);\n    uffd_register.range.len = size;\n    uffd_register.mode = UFFDIO_REGISTER_MODE_MISSING;\n    CHECK_EQ(ioctl(uffd_, UFFDIO_REGISTER, &uffd_register), 0)\n          << \"ioctl_userfaultfd: register moving-space: \" << strerror(errno);\n  }\n}\n\nvoid MarkCompact::ConcurrentCompaction(uint8_t* page) {\n  struct uffd_msg msg;\n  uint8_t* unused_space_begin = bump_pointer_space_->Begin()\n                                + (moving_first_objs_count_ + black_page_count_) * kPageSize;\n  DCHECK(IsAligned<kPageSize>(unused_space_begin));\n  auto zeropage_ioctl = [this] (void* addr) {\n                          struct uffdio_zeropage uffd_zeropage;\n                          DCHECK(IsAligned<kPageSize>(addr));\n                          uffd_zeropage.range.start = reinterpret_cast<uintptr_t>(addr);\n                          uffd_zeropage.range.len = kPageSize;\n                          uffd_zeropage.mode = 0;\n                          CHECK_EQ(ioctl(uffd_, UFFDIO_ZEROPAGE, &uffd_zeropage), 0)\n                                << \"ioctl: zeropage: \" << strerror(errno);\n                          DCHECK_EQ(uffd_zeropage.zeropage, static_cast<ssize_t>(kPageSize));\n                        };\n\n  auto copy_ioctl = [this] (void* fault_page, void* src) {\n                          struct uffdio_copy uffd_copy;\n                          uffd_copy.src = reinterpret_cast<uintptr_t>(src);\n                          uffd_copy.dst = reinterpret_cast<uintptr_t>(fault_page);\n                          uffd_copy.len = kPageSize;\n                          uffd_copy.mode = 0;\n                          CHECK_EQ(ioctl(uffd_, UFFDIO_COPY, &uffd_copy), 0)\n                                << \"ioctl: copy: \" << strerror(errno);\n                          DCHECK_EQ(uffd_copy.copy, static_cast<ssize_t>(kPageSize));\n                    };\n\n  while (true) {\n    ssize_t nread = read(uffd_, &msg, sizeof(msg));\n    CHECK_GT(nread, 0);\n    CHECK_EQ(msg.event, UFFD_EVENT_PAGEFAULT);\n    DCHECK_EQ(nread, static_cast<ssize_t>(sizeof(msg)));\n    uint8_t* fault_addr = reinterpret_cast<uint8_t*>(msg.arg.pagefault.address);\n    if (fault_addr == conc_compaction_termination_page_) {\n      // The counter doesn't need to be updated atomically as only one thread\n      // would wake up against the gc-thread's load to this fault_addr. In fact,\n      // the other threads would wake up serially because every exiting thread\n      // will wake up gc-thread, which would retry load but again would find the\n      // page missing. Also, the value will be flushed to caches due to the ioctl\n      // syscall below.\n      uint8_t ret = thread_pool_counter_--;\n      // Only the last thread should map the zeropage so that the gc-thread can\n      // proceed.\n      if (ret == 1) {\n        zeropage_ioctl(fault_addr);\n      } else {\n        struct uffdio_range uffd_range;\n        uffd_range.start = msg.arg.pagefault.address;\n        uffd_range.len = kPageSize;\n        CHECK_EQ(ioctl(uffd_, UFFDIO_WAKE, &uffd_range), 0)\n              << \"ioctl: wake: \" << strerror(errno);\n      }\n      break;\n    }\n    DCHECK(bump_pointer_space_->HasAddress(reinterpret_cast<mirror::Object*>(fault_addr)));\n    uint8_t* fault_page = AlignDown(fault_addr, kPageSize);\n    if (fault_addr >= unused_space_begin) {\n      zeropage_ioctl(fault_page);\n      continue;\n    }\n    size_t page_idx = (fault_page - bump_pointer_space_->Begin()) / kPageSize;\n    PageState state = moving_pages_status_[page_idx].load(std::memory_order_relaxed);\n    if (state == PageState::kUncompacted) {\n      // Relaxed memory-order is fine as the subsequent ioctl syscall guarantees\n      // status to be flushed before this thread attempts to copy/zeropage the\n      // fault_page.\n      state = moving_pages_status_[page_idx].exchange(PageState::kCompacting,\n                                                      std::memory_order_relaxed);\n    }\n    if (state == PageState::kCompacting) {\n      // Somebody else took (or taking) care of the page, so nothing to do.\n      continue;\n    }\n\n    if (fault_page < post_compact_end_) {\n      // The page has to be compacted.\n      CompactPage(first_objs_moving_space_[page_idx].AsMirrorPtr(),\n                  pre_compact_offset_moving_space_[page_idx],\n                  page);\n      copy_ioctl(fault_page, page);\n    } else {\n      // The page either has to be slid, or if it's an empty page then a\n      // zeropage needs to be mapped.\n      mirror::Object* first_obj = first_objs_moving_space_[page_idx].AsMirrorPtr();\n      if (first_obj != nullptr) {\n        DCHECK_GT(pre_compact_offset_moving_space_[page_idx], 0u);\n        uint8_t* pre_compact_page = black_allocations_begin_ + (fault_page - post_compact_end_);\n        DCHECK(IsAligned<kPageSize>(pre_compact_page));\n        SlideBlackPage(first_obj,\n                       page_idx,\n                       pre_compact_page,\n                       page);\n        copy_ioctl(fault_page, page);\n      } else {\n        zeropage_ioctl(fault_page);\n      }\n    }\n  }\n}\n\n#pragma clang diagnostic push\n#pragma clang diagnostic ignored \"-Wframe-larger-than=\"\nvoid MarkCompact::CompactionPhase() {\n  TimingLogger::ScopedTiming t(__FUNCTION__, GetTimings());\n  {\n    // TODO: Calculate freed objects and update that as well.",
        "base_contents": "  if (!kConcurrentCompaction) {\n    UpdateNonMovingSpace();\n    CompactMovingSpace();\n  }\n\n  stack_end_ = nullptr;\n  {\n    // TODO: Calculate freed objects and update that as well.",
        "res_region": "  UpdateNonMovingSpace();\n  // fallback mode\n  if (uffd_ == kFallbackMode) {\n    CompactMovingSpace</*kFallback*/true>();\n",
        "lookback": "  // fallback mode",
        "lookahead": "    int32_t freed_bytes = black_objs_slide_diff_;",
        "label": [
            "ours",
            "complex_conflict"
        ],
        "start_line": 1856,
        "end_line": 2019,
        "merge_gen_region": null,
        "confidence": 0.794169008731842,
        "hesitated": true
    },
    {
        "a_contents": "void MarkCompact::ConcurrentCompaction(uint8_t* page) {\n  struct uffd_msg msg;\n  uint8_t* unused_space_begin = bump_pointer_space_->Begin()\n                                + (moving_first_objs_count_ + black_page_count_) * kPageSize;\n  DCHECK(IsAligned<kPageSize>(unused_space_begin));\n  auto zeropage_ioctl = [this] (void* addr, bool tolerate_eexist) {\n                          struct uffdio_zeropage uffd_zeropage;\n                          DCHECK(IsAligned<kPageSize>(addr));\n                          uffd_zeropage.range.start = reinterpret_cast<uintptr_t>(addr);\n                          uffd_zeropage.range.len = kPageSize;\n                          uffd_zeropage.mode = 0;\n                          int ret = ioctl(uffd_, UFFDIO_ZEROPAGE, &uffd_zeropage);\n                          CHECK(ret == 0 || (tolerate_eexist && ret == -1 && errno == EEXIST))\n                              << \"ioctl: zeropage: \" << strerror(errno);\n                          DCHECK_EQ(uffd_zeropage.zeropage, static_cast<ssize_t>(kPageSize));\n                        };\n\n  auto copy_ioctl = [this] (void* fault_page, void* src) {\n                          struct uffdio_copy uffd_copy;\n                          uffd_copy.src = reinterpret_cast<uintptr_t>(src);\n                          uffd_copy.dst = reinterpret_cast<uintptr_t>(fault_page);\n                          uffd_copy.len = kPageSize;\n                          uffd_copy.mode = 0;\n                          CHECK_EQ(ioctl(uffd_, UFFDIO_COPY, &uffd_copy), 0)\n                                << \"ioctl: copy: \" << strerror(errno);\n                          DCHECK_EQ(uffd_copy.copy, static_cast<ssize_t>(kPageSize));\n                    };\n\n  while (true) {\n    ssize_t nread = read(uffd_, &msg, sizeof(msg));\n    CHECK_GT(nread, 0);\n    CHECK_EQ(msg.event, UFFD_EVENT_PAGEFAULT);\n    DCHECK_EQ(nread, static_cast<ssize_t>(sizeof(msg)));\n    uint8_t* fault_addr = reinterpret_cast<uint8_t*>(msg.arg.pagefault.address);\n    if (fault_addr == conc_compaction_termination_page_) {\n      // The counter doesn't need to be updated atomically as only one thread\n      // would wake up against the gc-thread's load to this fault_addr. In fact,\n      // the other threads would wake up serially because every exiting thread\n      // will wake up gc-thread, which would retry load but again would find the\n      // page missing. Also, the value will be flushed to caches due to the ioctl\n      // syscall below.\n      uint8_t ret = thread_pool_counter_--;\n      // Only the last thread should map the zeropage so that the gc-thread can\n      // proceed.\n      if (ret == 1) {\n        zeropage_ioctl(fault_addr, /*tolerate_eexist*/ false);\n      } else {\n        struct uffdio_range uffd_range;\n        uffd_range.start = msg.arg.pagefault.address;\n        uffd_range.len = kPageSize;\n        CHECK_EQ(ioctl(uffd_, UFFDIO_WAKE, &uffd_range), 0)\n              << \"ioctl: wake: \" << strerror(errno);\n      }\n      break;\n    }\n    DCHECK(bump_pointer_space_->HasAddress(reinterpret_cast<mirror::Object*>(fault_addr)));\n    uint8_t* fault_page = AlignDown(fault_addr, kPageSize);\n    if (fault_addr >= unused_space_begin) {\n      // There is a race which allows more than one thread to install a\n      // zero-page. But we can tolerate that. So absorb the EEXIST returned by\n      // the ioctl and move on.\n      zeropage_ioctl(fault_page, /*tolerate_eexist*/ true);\n      continue;\n    }\n    size_t page_idx = (fault_page - bump_pointer_space_->Begin()) / kPageSize;\n    PageState state = moving_pages_status_[page_idx].load(std::memory_order_relaxed);\n    if (state == PageState::kUncompacted) {\n      // Relaxed memory-order is fine as the subsequent ioctl syscall guarantees\n      // status to be flushed before this thread attempts to copy/zeropage the\n      // fault_page.\n      state = moving_pages_status_[page_idx].exchange(PageState::kCompacting,\n                                                      std::memory_order_relaxed);\n    }\n    if (state == PageState::kCompacting) {\n      // Somebody else took (or taking) care of the page, so nothing to do.\n      continue;\n    }\n\n    if (fault_page < post_compact_end_) {\n      // The page has to be compacted.\n      CompactPage(first_objs_moving_space_[page_idx].AsMirrorPtr(),\n                  pre_compact_offset_moving_space_[page_idx],\n                  page);\n      copy_ioctl(fault_page, page);\n    } else {\n      // The page either has to be slid, or if it's an empty page then a\n      // zeropage needs to be mapped.\n      mirror::Object* first_obj = first_objs_moving_space_[page_idx].AsMirrorPtr();\n      if (first_obj != nullptr) {\n        DCHECK_GT(pre_compact_offset_moving_space_[page_idx], 0u);\n        uint8_t* pre_compact_page = black_allocations_begin_ + (fault_page - post_compact_end_);\n        DCHECK(IsAligned<kPageSize>(pre_compact_page));\n        SlideBlackPage(first_obj,\n                       page_idx,\n                       pre_compact_page,\n                       page);\n        copy_ioctl(fault_page, page);\n      } else {\n        // We should never have a case where two workers are trying to install a\n        // zeropage in this range as we synchronize using\n        // moving_pages_status_[page_idx].\n        zeropage_ioctl(fault_page, /*tolerate_eexist*/ false);\n      }\n    }\n  }\n}\n\n#pragma clang diagnostic push\n#pragma clang diagnostic ignored \"-Wframe-larger-than=\"\nvoid MarkCompact::CompactionPhase() {\n  TimingLogger::ScopedTiming t(__FUNCTION__, GetTimings());\n  {\n    int32_t freed_bytes = black_objs_slide_diff_;\n    bump_pointer_space_->RecordFree(freed_objects_, freed_bytes);\n    RecordFree(ObjectBytePair(freed_objects_, freed_bytes));\n  }\n\n  if (kObjPtrPoisoning) {\n    CompactMovingSpace</*kFallback*/false>(compaction_buffers_map_.Begin());\n    // madvise the page so that we can get userfaults on it. We don't need to\n    // do this when not using poisoning as in that case the address location is\n    // untouched during compaction.\n    ZeroAndReleasePages(conc_compaction_termination_page_, kPageSize);\n  } else {\n    uint8_t buf[kPageSize];\n    CompactMovingSpace</*kFallback*/false>(buf);\n  }\n\n  // The following triggers 'special' userfaults. When received by the\n  // thread-pool workers, they will exit out of the compaction task. This fault\n  // happens because we madvise info_map_ above and it is at least kPageSize in length.\n  DCHECK(IsAligned<kPageSize>(conc_compaction_termination_page_));\n  CHECK_EQ(*reinterpret_cast<volatile uint8_t*>(conc_compaction_termination_page_), 0);\n  DCHECK_EQ(thread_pool_counter_, 0);\n\n  struct uffdio_range unregister_range;\n  unregister_range.start = reinterpret_cast<uintptr_t>(bump_pointer_space_->Begin());\n  unregister_range.len = bump_pointer_space_->Capacity();\n  CHECK_EQ(ioctl(uffd_, UFFDIO_UNREGISTER, &unregister_range), 0)\n        << \"ioctl_userfaultfd: unregister moving-space: \" << strerror(errno);\n\n  // When poisoning ObjPtr, we are forced to use buffers for page compaction in\n  // lower 4GB. Now that the usage is done, madvise them. But skip the first\n  // page, which is used by the gc-thread for the next iteration. Otherwise, we\n  // get into a deadlock due to userfault on it in the next iteration. This page\n  // is not consuming any physical memory because we already madvised it above\n  // and then we triggered a read userfault, which maps a special zero-page.\n  if (kObjPtrPoisoning) {\n    ZeroAndReleasePages(compaction_buffers_map_.Begin() + kPageSize,\n                        compaction_buffers_map_.Size() - kPageSize);\n  } else {\n    ZeroAndReleasePages(conc_compaction_termination_page_, kPageSize);\n  }\n  heap_->GetThreadPool()->StopWorkers(thread_running_gc_);",
        "b_contents": "  if (kObjPtrPoisoning) {\n    CompactMovingSpace</*kFallback*/false>(compaction_buffers_map_.Begin());\n    // madvise the page so that we can get userfaults on it. We don't need to\n    // do this when not using poisoning as in that case the address location is\n    // untouched during compaction.\n    ZeroAndReleasePages(conc_compaction_termination_page_, kPageSize);\n  } else {\n    uint8_t buf[kPageSize];\n    CompactMovingSpace</*kFallback*/false>(buf);\n  }\n\n  // The following triggers 'special' userfaults. When received by the\n  // thread-pool workers, they will exit out of the compaction task. This fault\n  // happens because we madvise info_map_ above and it is at least kPageSize in length.\n  DCHECK(IsAligned<kPageSize>(conc_compaction_termination_page_));\n  CHECK_EQ(*reinterpret_cast<volatile uint8_t*>(conc_compaction_termination_page_), 0);\n  DCHECK_EQ(thread_pool_counter_, 0);\n\n  struct uffdio_range unregister_range;\n  unregister_range.start = reinterpret_cast<uintptr_t>(bump_pointer_space_->Begin());\n  unregister_range.len = bump_pointer_space_->Capacity();\n  CHECK_EQ(ioctl(uffd_, UFFDIO_UNREGISTER, &unregister_range), 0)\n        << \"ioctl_userfaultfd: unregister moving-space: \" << strerror(errno);\n\n  // When poisoning ObjPtr, we are forced to use buffers for page compaction in\n  // lower 4GB. Now that the usage is done, madvise them. But skip the first\n  // page, which is used by the gc-thread for the next iteration. Otherwise, we\n  // get into a deadlock due to userfault on it in the next iteration. This page\n  // is not consuming any physical memory because we already madvised it above\n  // and then we triggered a read userfault, which maps a special zero-page.\n  if (kObjPtrPoisoning) {\n    ZeroAndReleasePages(compaction_buffers_map_.Begin() + kPageSize,\n                        compaction_buffers_map_.Size() - kPageSize);\n  } else {\n    ZeroAndReleasePages(conc_compaction_termination_page_, kPageSize);\n  }\n  heap_->GetThreadPool()->StopWorkers(thread_running_gc_);",
        "base_contents": "void MarkCompact::CompactionPhase() {\n  // TODO: This is the concurrent compaction phase.\n  TimingLogger::ScopedTiming t(__FUNCTION__, GetTimings());\n  UNIMPLEMENTED(FATAL) << \"Unreachable\";",
        "res_region": "void MarkCompact::ConcurrentCompaction(uint8_t* page) {\n  struct uffd_msg msg;\n  uint8_t* unused_space_begin = bump_pointer_space_->Begin()\n                                + (moving_first_objs_count_ + black_page_count_) * kPageSize;\n  DCHECK(IsAligned<kPageSize>(unused_space_begin));\n  auto zeropage_ioctl = [this] (void* addr, bool tolerate_eexist) {\n                          struct uffdio_zeropage uffd_zeropage;\n                          DCHECK(IsAligned<kPageSize>(addr));\n                          uffd_zeropage.range.start = reinterpret_cast<uintptr_t>(addr);\n                          uffd_zeropage.range.len = kPageSize;\n                          uffd_zeropage.mode = 0;\n                          int ret = ioctl(uffd_, UFFDIO_ZEROPAGE, &uffd_zeropage);\n                          CHECK(ret == 0 || (tolerate_eexist && ret == -1 && errno == EEXIST))\n                              << \"ioctl: zeropage: \" << strerror(errno);\n                          DCHECK_EQ(uffd_zeropage.zeropage, static_cast<ssize_t>(kPageSize));\n                        };\n\n  auto copy_ioctl = [this] (void* fault_page, void* src) {\n                          struct uffdio_copy uffd_copy;\n                          uffd_copy.src = reinterpret_cast<uintptr_t>(src);\n                          uffd_copy.dst = reinterpret_cast<uintptr_t>(fault_page);\n                          uffd_copy.len = kPageSize;\n                          uffd_copy.mode = 0;\n                          CHECK_EQ(ioctl(uffd_, UFFDIO_COPY, &uffd_copy), 0)\n                                << \"ioctl: copy: \" << strerror(errno);\n                          DCHECK_EQ(uffd_copy.copy, static_cast<ssize_t>(kPageSize));\n                    };\n\n  while (true) {\n    ssize_t nread = read(uffd_, &msg, sizeof(msg));\n    CHECK_GT(nread, 0);\n    CHECK_EQ(msg.event, UFFD_EVENT_PAGEFAULT);\n    DCHECK_EQ(nread, static_cast<ssize_t>(sizeof(msg)));\n    uint8_t* fault_addr = reinterpret_cast<uint8_t*>(msg.arg.pagefault.address);\n    if (fault_addr == conc_compaction_termination_page_) {\n      // The counter doesn't need to be updated atomically as only one thread\n      // would wake up against the gc-thread's load to this fault_addr. In fact,\n      // the other threads would wake up serially because every exiting thread\n      // will wake up gc-thread, which would retry load but again would find the\n      // page missing. Also, the value will be flushed to caches due to the ioctl\n      // syscall below.\n      uint8_t ret = thread_pool_counter_--;\n      // Only the last thread should map the zeropage so that the gc-thread can\n      // proceed.\n      if (ret == 1) {\n        zeropage_ioctl(fault_addr, /*tolerate_eexist*/ false);\n      } else {\n        struct uffdio_range uffd_range;\n        uffd_range.start = msg.arg.pagefault.address;\n        uffd_range.len = kPageSize;\n        CHECK_EQ(ioctl(uffd_, UFFDIO_WAKE, &uffd_range), 0)\n              << \"ioctl: wake: \" << strerror(errno);\n      }\n      break;\n    }\n    DCHECK(bump_pointer_space_->HasAddress(reinterpret_cast<mirror::Object*>(fault_addr)));\n    uint8_t* fault_page = AlignDown(fault_addr, kPageSize);\n    if (fault_addr >= unused_space_begin) {\n      // There is a race which allows more than one thread to install a\n      // zero-page. But we can tolerate that. So absorb the EEXIST returned by\n      // the ioctl and move on.\n      zeropage_ioctl(fault_page, /*tolerate_eexist*/ true);\n      continue;\n    }\n    size_t page_idx = (fault_page - bump_pointer_space_->Begin()) / kPageSize;\n    PageState state = moving_pages_status_[page_idx].load(std::memory_order_relaxed);\n    if (state == PageState::kUncompacted) {\n      // Relaxed memory-order is fine as the subsequent ioctl syscall guarantees\n      // status to be flushed before this thread attempts to copy/zeropage the\n      // fault_page.\n      state = moving_pages_status_[page_idx].exchange(PageState::kCompacting,\n                                                      std::memory_order_relaxed);\n    }\n    if (state == PageState::kCompacting) {\n      // Somebody else took (or taking) care of the page, so nothing to do.\n      continue;\n    }\n\n    if (fault_page < post_compact_end_) {\n      // The page has to be compacted.\n      CompactPage(first_objs_moving_space_[page_idx].AsMirrorPtr(),\n                  pre_compact_offset_moving_space_[page_idx],\n                  page);\n      copy_ioctl(fault_page, page);\n    } else {\n      // The page either has to be slid, or if it's an empty page then a\n      // zeropage needs to be mapped.\n      mirror::Object* first_obj = first_objs_moving_space_[page_idx].AsMirrorPtr();\n      if (first_obj != nullptr) {\n        DCHECK_GT(pre_compact_offset_moving_space_[page_idx], 0u);\n        uint8_t* pre_compact_page = black_allocations_begin_ + (fault_page - post_compact_end_);\n        DCHECK(IsAligned<kPageSize>(pre_compact_page));\n        SlideBlackPage(first_obj,\n                       page_idx,\n                       pre_compact_page,\n                       page);\n        copy_ioctl(fault_page, page);\n      } else {\n        // We should never have a case where two workers are trying to install a\n        // zeropage in this range as we synchronize using\n        // moving_pages_status_[page_idx].\n        zeropage_ioctl(fault_page, /*tolerate_eexist*/ false);\n      }\n    }\n  }\n}\n\n#pragma clang diagnostic push\n#pragma clang diagnostic ignored \"-Wframe-larger-than=\"\nvoid MarkCompact::CompactionPhase() {\n  TimingLogger::ScopedTiming t(__FUNCTION__, GetTimings());\n  {\n    int32_t freed_bytes = black_objs_slide_diff_;\n    bump_pointer_space_->RecordFree(freed_objects_, freed_bytes);\n    RecordFree(ObjectBytePair(freed_objects_, freed_bytes));\n  }\n\n  if (kObjPtrPoisoning) {\n    CompactMovingSpace</*kFallback*/false>(compaction_buffers_map_.Begin());\n    // madvise the page so that we can get userfaults on it. We don't need to\n    // do this when not using poisoning as in that case the address location is\n    // untouched during compaction.\n    ZeroAndReleasePages(conc_compaction_termination_page_, kPageSize);\n  } else {\n    uint8_t buf[kPageSize];\n    CompactMovingSpace</*kFallback*/false>(buf);\n  }\n\n  // The following triggers 'special' userfaults. When received by the\n  // thread-pool workers, they will exit out of the compaction task. This fault\n  // happens because we madvise info_map_ above and it is at least kPageSize in length.\n  DCHECK(IsAligned<kPageSize>(conc_compaction_termination_page_));\n  CHECK_EQ(*reinterpret_cast<volatile uint8_t*>(conc_compaction_termination_page_), 0);\n  DCHECK_EQ(thread_pool_counter_, 0);\n\n  struct uffdio_range unregister_range;\n  unregister_range.start = reinterpret_cast<uintptr_t>(bump_pointer_space_->Begin());\n  unregister_range.len = bump_pointer_space_->Capacity();\n  CHECK_EQ(ioctl(uffd_, UFFDIO_UNREGISTER, &unregister_range), 0)\n        << \"ioctl_userfaultfd: unregister moving-space: \" << strerror(errno);\n\n  // When poisoning ObjPtr, we are forced to use buffers for page compaction in\n  // lower 4GB. Now that the usage is done, madvise them. But skip the first\n  // page, which is used by the gc-thread for the next iteration. Otherwise, we\n  // get into a deadlock due to userfault on it in the next iteration. This page\n  // is not consuming any physical memory because we already madvised it above\n  // and then we triggered a read userfault, which maps a special zero-page.\n  if (kObjPtrPoisoning) {\n    ZeroAndReleasePages(compaction_buffers_map_.Begin() + kPageSize,\n                        compaction_buffers_map_.Size() - kPageSize);\n  } else {\n    ZeroAndReleasePages(conc_compaction_termination_page_, kPageSize);\n  }\n  heap_->GetThreadPool()->StopWorkers(thread_running_gc_);",
        "lookback": "  struct uffd_msg msg;",
        "lookahead": "}",
        "label": [
            "ours",
            "complex_conflict"
        ],
        "start_line": 2067,
        "end_line": 2265,
        "merge_gen_region": null,
        "confidence": 0.6516983509063721,
        "hesitated": true
    }
]