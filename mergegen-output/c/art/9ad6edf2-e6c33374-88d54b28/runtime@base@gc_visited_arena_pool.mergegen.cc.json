[
    {
        "a_contents": "  if (single_obj_arena) {\n    // We have only one object in this arena and it is expected to consume the\n    // entire arena.\n    bytes_allocated_ = size;\n  } else {\n    DCHECK_ALIGNED(size, kPageSize);\n    DCHECK_ALIGNED(start, kPageSize);\n    size_t arr_size = size / kPageSize;\n    first_obj_array_.reset(new uint8_t*[arr_size]);\n    std::fill_n(first_obj_array_.get(), arr_size, nullptr);\n  }\n}\n\nvoid TrackedArena::ReleasePages(uint8_t* begin, size_t size, bool pre_zygote_fork) {\n  DCHECK_ALIGNED(begin, kPageSize);\n  // Userfaultfd GC uses MAP_SHARED mappings for linear-alloc and therefore\n  // MADV_DONTNEED will not free the pages from page cache. Therefore use\n  // MADV_REMOVE instead, which is meant for this purpose.\n  // Arenas allocated pre-zygote fork are private anonymous and hence must be\n  // released using MADV_DONTNEED.\n  if (!gUseUserfaultfd || pre_zygote_fork ||\n      (madvise(begin, size, MADV_REMOVE) == -1 && errno == EINVAL)) {\n    // MADV_REMOVE fails if invoked on anonymous mapping, which could happen\n    // if the arena is released before userfaultfd-GC starts using memfd. So\n    // use MADV_DONTNEED.\n    ZeroAndReleaseMemory(begin, size);\n  }",
        "b_contents": "  if (single_obj_arena) {\n    // We have only one object in this arena and it is expected to consume the\n    // entire arena.\n    bytes_allocated_ = size;\n  } else {\n    DCHECK_ALIGNED(size, kPageSize);\n    DCHECK_ALIGNED(start, kPageSize);\n    size_t arr_size = size / kPageSize;\n    first_obj_array_.reset(new uint8_t*[arr_size]);\n    std::fill_n(first_obj_array_.get(), arr_size, nullptr);\n  }\n}\n\nvoid TrackedArena::ReleasePages(uint8_t* begin, size_t size, bool pre_zygote_fork) {\n  DCHECK_ALIGNED(begin, kPageSize);\n  // Userfaultfd GC uses MAP_SHARED mappings for linear-alloc and therefore\n  // MADV_DONTNEED will not free the pages from page cache. Therefore use\n  // MADV_REMOVE instead, which is meant for this purpose.\n  // Arenas allocated pre-zygote fork are private anonymous and hence must be\n  // released using MADV_DONTNEED.\n  if (!gUseUserfaultfd || pre_zygote_fork ||\n      (madvise(begin, size, MADV_REMOVE) == -1 && errno == EINVAL)) {\n    // MADV_REMOVE fails if invoked on anonymous mapping, which could happen\n    // if the arena is released before userfaultfd-GC starts using memfd. So\n    // use MADV_DONTNEED.\n    ZeroAndReleasePages(begin, size);\n  }",
        "base_contents": "  size_t arr_size = size / kPageSize;\n  first_obj_array_.reset(new uint8_t*[arr_size]);\n  std::fill_n(first_obj_array_.get(), arr_size, nullptr);",
        "res_region": "  if (single_obj_arena) {\n    // We have only one object in this arena and it is expected to consume the\n    // entire arena.\n    bytes_allocated_ = size;\n  } else {\n    DCHECK_ALIGNED(size, kPageSize);\n    DCHECK_ALIGNED(start, kPageSize);\n    size_t arr_size = size / kPageSize;\n    first_obj_array_.reset(new uint8_t*[arr_size]);\n    std::fill_n(first_obj_array_.get(), arr_size, nullptr);\n  }\n}\n\nvoid TrackedArena::ReleasePages(uint8_t* begin, size_t size, bool pre_zygote_fork) {\n  DCHECK_ALIGNED(begin, kPageSize);\n  // Userfaultfd GC uses MAP_SHARED mappings for linear-alloc and therefore\n  // MADV_DONTNEED will not free the pages from page cache. Therefore use\n  // MADV_REMOVE instead, which is meant for this purpose.\n  // Arenas allocated pre-zygote fork are private anonymous and hence must be\n  // released using MADV_DONTNEED.\n  if (!gUseUserfaultfd || pre_zygote_fork ||\n      (madvise(begin, size, MADV_REMOVE) == -1 && errno == EINVAL)) {\n    // MADV_REMOVE fails if invoked on anonymous mapping, which could happen\n    // if the arena is released before userfaultfd-GC starts using memfd. So\n    // use MADV_DONTNEED.\n    ZeroAndReleaseMemory(begin, size);\n  }",
        "lookback": "    // We have only one object in this arena and it is expected to consume the",
        "lookahead": "}",
        "label": [
            "ours",
            "complex_conflict"
        ],
        "start_line": 38,
        "end_line": 98,
        "merge_gen_region": null,
        "confidence": 0.8475086688995361,
        "hesitated": true
    },
    {
        "a_contents": "uint8_t* GcVisitedArenaPool::AllocSingleObjArena(size_t size) {\n  WriterMutexLock wmu(Thread::Current(), lock_);\n  Arena* arena;\n  DCHECK(gUseUserfaultfd);\n  // To minimize private dirty, all class and intern table allocations are\n  // done outside LinearAlloc range so they are untouched during GC.\n  if (pre_zygote_fork_) {\n    uint8_t* begin = static_cast<uint8_t*>(malloc(size));\n    auto insert_result = allocated_arenas_.insert(\n        new TrackedArena(begin, size, /*pre_zygote_fork=*/true, /*single_obj_arena=*/true));\n    arena = *insert_result.first;\n  } else {\n    arena = AllocArena(size, /*need_first_obj_arr=*/true);\n  }\n  return arena->Begin();\n}\n\nvoid GcVisitedArenaPool::FreeSingleObjArena(uint8_t* addr) {\n  Thread* self = Thread::Current();\n  size_t size;\n  bool zygote_arena;\n  {\n    TrackedArena temp_arena(addr);\n    WriterMutexLock wmu(self, lock_);\n    auto iter = allocated_arenas_.find(&temp_arena);\n    DCHECK(iter != allocated_arenas_.end());\n    TrackedArena* arena = *iter;\n    size = arena->Size();\n    zygote_arena = arena->IsPreZygoteForkArena();\n    DCHECK_EQ(arena->Begin(), addr);\n    DCHECK(arena->IsSingleObjectArena());\n    allocated_arenas_.erase(iter);\n    if (defer_arena_freeing_) {\n      arena->SetupForDeferredDeletion(unused_arenas_);\n      unused_arenas_ = arena;\n    } else {\n      delete arena;\n    }\n  }\n  // Refer to the comment in FreeArenaChain() for why the pages are released\n  // after deleting the arena.\n  if (zygote_arena) {\n    free(addr);\n  } else {\n    TrackedArena::ReleasePages(addr, size, /*pre_zygote_fork=*/false);\n    WriterMutexLock wmu(self, lock_);\n    FreeRangeLocked(addr, size);\n  }\n}\n\nArena* GcVisitedArenaPool::AllocArena(size_t size, bool single_obj_arena) {",
        "b_contents": "uint8_t* GcVisitedArenaPool::AllocSingleObjArena(size_t size) {\n  WriterMutexLock wmu(Thread::Current(), lock_);\n  Arena* arena;\n  DCHECK(gUseUserfaultfd);\n  // To minimize private dirty, all class and intern table allocations are\n  // done outside LinearAlloc range so they are untouched during GC.\n  if (pre_zygote_fork_) {\n    uint8_t* begin = static_cast<uint8_t*>(malloc(size));\n    auto insert_result = allocated_arenas_.insert(\n        new TrackedArena(begin, size, /*pre_zygote_fork=*/true, /*single_obj_arena=*/true));\n    arena = *insert_result.first;\n  } else {\n    arena = AllocArena(size, /*single_obj_arena=*/true);\n  }\n  return arena->Begin();\n}\n\nvoid GcVisitedArenaPool::FreeSingleObjArena(uint8_t* addr) {\n  Thread* self = Thread::Current();\n  size_t size;\n  bool zygote_arena;\n  {\n    TrackedArena temp_arena(addr);\n    WriterMutexLock wmu(self, lock_);\n    auto iter = allocated_arenas_.find(&temp_arena);\n    DCHECK(iter != allocated_arenas_.end());\n    TrackedArena* arena = *iter;\n    size = arena->Size();\n    zygote_arena = arena->IsPreZygoteForkArena();\n    DCHECK_EQ(arena->Begin(), addr);\n    DCHECK(arena->IsSingleObjectArena());\n    allocated_arenas_.erase(iter);\n    if (defer_arena_freeing_) {\n      arena->SetupForDeferredDeletion(unused_arenas_);\n      unused_arenas_ = arena;\n    } else {\n      delete arena;\n    }\n  }\n  // Refer to the comment in FreeArenaChain() for why the pages are released\n  // after deleting the arena.\n  if (zygote_arena) {\n    free(addr);\n  } else {\n    TrackedArena::ReleasePages(addr, size, /*pre_zygote_fork=*/false);\n    WriterMutexLock wmu(self, lock_);\n    FreeRangeLocked(addr, size);\n  }\n}\n\nArena* GcVisitedArenaPool::AllocArena(size_t size, bool single_obj_arena) {",
        "base_contents": "Arena* GcVisitedArenaPool::AllocArena(size_t size) {",
        "res_region": "uint8_t* GcVisitedArenaPool::AllocSingleObjArena(size_t size) {\n  WriterMutexLock wmu(Thread::Current(), lock_);\n  Arena* arena;\n  DCHECK(gUseUserfaultfd);\n  // To minimize private dirty, all class and intern table allocations are\n  // done outside LinearAlloc range so they are untouched during GC.\n  if (pre_zygote_fork_) {\n    uint8_t* begin = static_cast<uint8_t*>(malloc(size));\n    auto insert_result = allocated_arenas_.insert(\n        new TrackedArena(begin, size, /*pre_zygote_fork=*/true, /*single_obj_arena=*/true));\n    arena = *insert_result.first;\n  } else {\n    arena = AllocArena(size, /*need_first_obj_arr=*/true);\n  }\n  return arena->Begin();\n}\n\nvoid GcVisitedArenaPool::FreeSingleObjArena(uint8_t* addr) {\n  Thread* self = Thread::Current();\n  size_t size;\n  bool zygote_arena;\n  {\n    TrackedArena temp_arena(addr);\n    WriterMutexLock wmu(self, lock_);\n    auto iter = allocated_arenas_.find(&temp_arena);\n    DCHECK(iter != allocated_arenas_.end());\n    TrackedArena* arena = *iter;\n    size = arena->Size();\n    zygote_arena = arena->IsPreZygoteForkArena();\n    DCHECK_EQ(arena->Begin(), addr);\n    DCHECK(arena->IsSingleObjectArena());\n    allocated_arenas_.erase(iter);\n    if (defer_arena_freeing_) {\n      arena->SetupForDeferredDeletion(unused_arenas_);\n      unused_arenas_ = arena;\n    } else {\n      delete arena;\n    }\n  }\n  // Refer to the comment in FreeArenaChain() for why the pages are released\n  // after deleting the arena.\n  if (zygote_arena) {\n    free(addr);\n  } else {\n    TrackedArena::ReleasePages(addr, size, /*pre_zygote_fork=*/false);\n    WriterMutexLock wmu(self, lock_);\n    FreeRangeLocked(addr, size);\n  }\n}\n\nArena* GcVisitedArenaPool::AllocArena(size_t size, bool single_obj_arena) {",
        "lookback": "  WriterMutexLock wmu(Thread::Current(), lock_);",
        "lookahead": "  // Return only page aligned sizes so that madvise can be leveraged.",
        "label": [
            "ours",
            "complex_conflict"
        ],
        "start_line": 202,
        "end_line": 308,
        "merge_gen_region": null,
        "confidence": 0.3454506993293762,
        "hesitated": true
    }
]