[
    {
        "a_contents": "\n  Arena* AllocArena(size_t size, bool need_first_obj_arr) REQUIRES(lock_);\n  // Use by arena allocator.\n  Arena* AllocArena(size_t size) override REQUIRES(!lock_) {\n    WriterMutexLock wmu(Thread::Current(), lock_);\n    return AllocArena(size, /*need_first_obj_arr=*/false);\n  }\n  void FreeArenaChain(Arena* first) override REQUIRES(!lock_);\n  size_t GetBytesAllocated() const override REQUIRES(!lock_);",
        "b_contents": "\n  Arena* AllocArena(size_t size, bool need_first_obj_arr) REQUIRES(lock_);\n  // Use by arena allocator.\n  Arena* AllocArena(size_t size) override REQUIRES(!lock_) {\n    WriterMutexLock wmu(Thread::Current(), lock_);\n    return AllocArena(size, /*single_obj_arena=*/false);\n  }\n  void FreeArenaChain(Arena* first) override REQUIRES(!lock_);\n  size_t GetBytesAllocated() const override REQUIRES(!lock_);",
        "base_contents": "  Arena* AllocArena(size_t size) override;\n  void FreeArenaChain(Arena* first) override;\n  size_t GetBytesAllocated() const override;",
        "res_region": "\n  Arena* AllocArena(size_t size, bool need_first_obj_arr) REQUIRES(lock_);\n  // Use by arena allocator.\n  Arena* AllocArena(size_t size) override REQUIRES(!lock_) {\n    WriterMutexLock wmu(Thread::Current(), lock_);\n    return AllocArena(size, /*need_first_obj_arr=*/false);\n  }\n  void FreeArenaChain(Arena* first) override REQUIRES(!lock_);\n  size_t GetBytesAllocated() const override REQUIRES(!lock_);",
        "lookback": "  Arena* AllocArena(size_t size, bool need_first_obj_arr) REQUIRES(lock_);",
        "lookahead": "  void ReclaimMemory() override {}",
        "label": [
            "ours",
            "complex_conflict"
        ],
        "start_line": 147,
        "end_line": 171,
        "merge_gen_region": "*TypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeGTypeTypeGTypeTypeGTypeTypeTypeGTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeTypeType",
        "confidence": 0.8978469371795654,
        "hesitated": false,
        "resolved": false
    },
    {
        "a_contents": "// Allocator for class-table and intern-table hash-sets. It enables updating the\n// roots concurrently page-by-page.\ntemplate <class T, AllocatorTag kTag>\nclass GcRootArenaAllocator : public TrackingAllocator<T, kTag> {\n public:\n  using value_type = typename TrackingAllocator<T, kTag>::value_type;\n  using size_type = typename TrackingAllocator<T, kTag>::size_type;\n  using difference_type = typename TrackingAllocator<T, kTag>::difference_type;\n  using pointer = typename TrackingAllocator<T, kTag>::pointer;\n  using const_pointer = typename TrackingAllocator<T, kTag>::const_pointer;\n  using reference = typename TrackingAllocator<T, kTag>::reference;\n  using const_reference = typename TrackingAllocator<T, kTag>::const_reference;\n\n  // Used internally by STL data structures.\n  template <class U>\n  explicit GcRootArenaAllocator(\n      [[maybe_unused]] const GcRootArenaAllocator<U, kTag>& alloc) noexcept {}\n  // Used internally by STL data structures.\n  GcRootArenaAllocator() noexcept : TrackingAllocator<T, kTag>() {}\n\n  // Enables an allocator for objects of one type to allocate storage for objects of another type.\n  // Used internally by STL data structures.\n  template <class U>\n  struct rebind {\n    using other = GcRootArenaAllocator<U, kTag>;\n  };\n\n  pointer allocate(size_type n, [[maybe_unused]] const_pointer hint = nullptr) {\n    if (!gUseUserfaultfd) {\n      return TrackingAllocator<T, kTag>::allocate(n);\n    }\n    size_t size = n * sizeof(T);\n    GcVisitedArenaPool* pool =\n        down_cast<GcVisitedArenaPool*>(Runtime::Current()->GetLinearAllocArenaPool());\n    return reinterpret_cast<pointer>(pool->AllocSingleObjArena(size));\n  }\n\n  template <typename PT>\n  void deallocate(PT p, size_type n) {\n    if (!gUseUserfaultfd) {\n      TrackingAllocator<T, kTag>::deallocate(p, n);\n      return;\n    }\n    GcVisitedArenaPool* pool =\n        down_cast<GcVisitedArenaPool*>(Runtime::Current()->GetLinearAllocArenaPool());\n    pool->FreeSingleObjArena(reinterpret_cast<uint8_t*>(p));\n  }\n};\n",
        "b_contents": "// Allocator for class-table and intern-table hash-sets. It enables updating the\n// roots concurrently page-by-page.\ntemplate <class T, AllocatorTag kTag>\nclass GcRootArenaAllocator : public TrackingAllocator<T, kTag> {\n public:\n  using value_type = typename TrackingAllocator<T, kTag>::value_type;\n  using size_type = typename TrackingAllocator<T, kTag>::size_type;\n  using difference_type = typename TrackingAllocator<T, kTag>::difference_type;\n  using pointer = typename TrackingAllocator<T, kTag>::pointer;\n  using const_pointer = typename TrackingAllocator<T, kTag>::const_pointer;\n  using reference = typename TrackingAllocator<T, kTag>::reference;\n  using const_reference = typename TrackingAllocator<T, kTag>::const_reference;\n\n  // Used internally by STL data structures.\n  template <class U>\n  explicit GcRootArenaAllocator(\n      [[maybe_unused]] const GcRootArenaAllocator<U, kTag>& alloc) noexcept {}\n  // Used internally by STL data structures.\n  GcRootArenaAllocator() noexcept : TrackingAllocator<T, kTag>() {}\n\n  // Enables an allocator for objects of one type to allocate storage for objects of another type.\n  // Used internally by STL data structures.\n  template <class U>\n  struct rebind {\n    using other = GcRootArenaAllocator<U, kTag>;\n  };\n\n  pointer allocate(size_type n, [[maybe_unused]] const_pointer hint = 0) {\n    if (!gUseUserfaultfd) {\n      return TrackingAllocator<T, kTag>::allocate(n);\n    }\n    size_t size = n * sizeof(T);\n    GcVisitedArenaPool* pool =\n        down_cast<GcVisitedArenaPool*>(Runtime::Current()->GetLinearAllocArenaPool());\n    return reinterpret_cast<pointer>(pool->AllocSingleObjArena(size));\n  }\n\n  template <typename PT>\n  void deallocate(PT p, size_type n) {\n    if (!gUseUserfaultfd) {\n      TrackingAllocator<T, kTag>::deallocate(p, n);\n      return;\n    }\n    GcVisitedArenaPool* pool =\n        down_cast<GcVisitedArenaPool*>(Runtime::Current()->GetLinearAllocArenaPool());\n    pool->FreeSingleObjArena(reinterpret_cast<uint8_t*>(p));\n  }\n};\n",
        "base_contents": "",
        "res_region": "// Allocator for class-table and intern-table hash-sets. It enables updating the\n// roots concurrently page-by-page.\ntemplate <class T, AllocatorTag kTag>\nclass GcRootArenaAllocator : public TrackingAllocator<T, kTag> {\n public:\n  using value_type = typename TrackingAllocator<T, kTag>::value_type;\n  using size_type = typename TrackingAllocator<T, kTag>::size_type;\n  using difference_type = typename TrackingAllocator<T, kTag>::difference_type;\n  using pointer = typename TrackingAllocator<T, kTag>::pointer;\n  using const_pointer = typename TrackingAllocator<T, kTag>::const_pointer;\n  using reference = typename TrackingAllocator<T, kTag>::reference;\n  using const_reference = typename TrackingAllocator<T, kTag>::const_reference;\n\n  // Used internally by STL data structures.\n  template <class U>\n  explicit GcRootArenaAllocator(\n      [[maybe_unused]] const GcRootArenaAllocator<U, kTag>& alloc) noexcept {}\n  // Used internally by STL data structures.\n  GcRootArenaAllocator() noexcept : TrackingAllocator<T, kTag>() {}\n\n  // Enables an allocator for objects of one type to allocate storage for objects of another type.\n  // Used internally by STL data structures.\n  template <class U>\n  struct rebind {\n    using other = GcRootArenaAllocator<U, kTag>;\n  };\n\n  pointer allocate(size_type n, [[maybe_unused]] const_pointer hint = nullptr) {\n    if (!gUseUserfaultfd) {\n      return TrackingAllocator<T, kTag>::allocate(n);\n    }\n    size_t size = n * sizeof(T);\n    GcVisitedArenaPool* pool =\n        down_cast<GcVisitedArenaPool*>(Runtime::Current()->GetLinearAllocArenaPool());\n    return reinterpret_cast<pointer>(pool->AllocSingleObjArena(size));\n  }\n\n  template <typename PT>\n  void deallocate(PT p, size_type n) {\n    if (!gUseUserfaultfd) {\n      TrackingAllocator<T, kTag>::deallocate(p, n);\n      return;\n    }\n    GcVisitedArenaPool* pool =\n        down_cast<GcVisitedArenaPool*>(Runtime::Current()->GetLinearAllocArenaPool());\n    pool->FreeSingleObjArena(reinterpret_cast<uint8_t*>(p));\n  }\n};\n",
        "lookback": "// roots concurrently page-by-page.",
        "lookahead": "}  // namespace art",
        "label": [
            "ours",
            "one_side_deletion"
        ],
        "start_line": 304,
        "end_line": 405,
        "merge_gen_region": null,
        "confidence": 0.7924284338951111,
        "hesitated": true
    }
]