[
    {
        "a_contents": "      compaction_score_(vset->NumberLevels()),\n      compaction_level_(vset->NumberLevels()),\n      version_number_(version_number) {\n}",
        "b_contents": "      compaction_score_(num_levels_),\n      compaction_level_(num_levels_),\n      version_number_(version_number) {}",
        "base_contents": "      compaction_score_(vset->NumberLevels()),\n      compaction_level_(vset->NumberLevels()),\n      offset_manifest_file_(0),\n      version_number_(version_number) {\n}",
        "res_region": "      compaction_score_(num_levels_),\n      compaction_level_(num_levels_),\n      version_number_(version_number) {}",
        "lookback": "      compaction_level_(num_levels_),",
        "lookahead": "",
        "label": [
            "base",
            "complex_conflict"
        ],
        "start_line": 420,
        "end_line": 435,
        "merge_gen_region": null,
        "confidence": 0.7194898724555969,
        "hesitated": true
    },
    {
        "a_contents": "      for (int l = 0; !found && l < vset_->NumberLevels(); l++) {",
        "b_contents": "      for (int l = 0; !found && l < base_->NumberLevels(); l++) {",
        "base_contents": "      for (int l = 0; !found && l < edit->number_levels_; l++) {",
        "res_region": "      for (int l = 0; !found && l < base_->NumberLevels(); l++) {",
        "lookback": "        const std::vector<FileMetaData*>& base_files = base_->files_[l];",
        "lookahead": "        const std::vector<FileMetaData*>& base_files = base_->files_[l];",
        "label": [
            "base",
            "complex_conflict"
        ],
        "start_line": 1219,
        "end_line": 1225,
        "merge_gen_region": null,
        "confidence": 0.4062364101409912,
        "hesitated": true
    },
    {
        "a_contents": "      for (int l = level+1; !found && l < vset_->NumberLevels(); l++) {",
        "b_contents": "      for (int l = level+1; !found && l < base_->NumberLevels(); l++) {",
        "base_contents": "      for (int l = level+1; !found && l < edit->number_levels_; l++) {",
        "res_region": "      for (int l = level+1; !found && l < base_->NumberLevels(); l++) {",
        "lookback": "        const FileSet* added = levels_[l].added_files;",
        "lookahead": "        const FileSet* added = levels_[l].added_files;",
        "label": [
            "base",
            "complex_conflict"
        ],
        "start_line": 1238,
        "end_line": 1244,
        "merge_gen_region": null,
        "confidence": 0.48272156715393066,
        "hesitated": true
    },
    {
        "a_contents": "      compactions_in_progress_(options_->num_levels),",
        "b_contents": "      dummy_versions_(this),\n      current_(nullptr),\n      need_slowdown_for_num_level0_files_(false),",
        "base_contents": "      dummy_versions_(this),\n      current_(nullptr),\n      compactions_in_progress_(options_->num_levels),",
        "res_region": "      need_slowdown_for_num_level0_files_(false),",
        "lookback": "      current_version_number_(0),",
        "lookahead": "      current_version_number_(0),",
        "label": [
            "interleave",
            "complex_conflict"
        ],
        "start_line": 1382,
        "end_line": 1392,
        "merge_gen_region": null,
        "confidence": 0.3755713403224945,
        "hesitated": true
    },
    {
        "a_contents": "      storage_options_compactions_(storage_options_)  {\n  compact_pointer_ = new std::string[options_->num_levels];\n  Init(options_->num_levels);",
        "b_contents": "      storage_options_compactions_(storage_options_) {\n  if (options_->compaction_style == kCompactionStyleUniversal) {\n    compaction_picker_.reset(new UniversalCompactionPicker(options_, &icmp_));\n  } else {\n    compaction_picker_.reset(new LevelCompactionPicker(options_, &icmp_));\n  }\n  AppendVersion(new Version(this, current_version_number_++));",
        "base_contents": "      storage_options_compactions_(storage_options_)  {\n  compact_pointer_ = new std::string[options_->num_levels];\n  Init(options_->num_levels);\n  AppendVersion(new Version(this, current_version_number_++));",
        "res_region": "      storage_options_compactions_(storage_options_) {\n  if (options_->compaction_style == kCompactionStyleUniversal) {\n    compaction_picker_.reset(new UniversalCompactionPicker(options_, &icmp_));\n  } else {\n    compaction_picker_.reset(new LevelCompactionPicker(options_, &icmp_));\n  }",
        "lookback": "  if (options_->compaction_style == kCompactionStyleUniversal) {",
        "lookahead": "}",
        "label": [
            "interleave",
            "complex_conflict"
        ],
        "start_line": 1396,
        "end_line": 1413,
        "merge_gen_region": null,
        "confidence": 0.7133163809776306,
        "hesitated": true
    },
    {
        "a_contents": "  column_family_data->current = v;",
        "b_contents": "  current_ = v;\n  need_slowdown_for_num_level0_files_ =\n      (options_->level0_slowdown_writes_trigger >= 0 && current_ != nullptr &&\n       v->NumLevelFiles(0) >= options_->level0_slowdown_writes_trigger);",
        "base_contents": "  current_ = v;",
        "res_region": "  column_family_data->current = v;\n  need_slowdown_for_num_level0_files_ =\n      (options_->level0_slowdown_writes_trigger >= 0 &&\n       v->NumLevelFiles(0) >= options_->level0_slowdown_writes_trigger);",
        "lookback": "  need_slowdown_for_num_level0_files_ =",
        "lookahead": "  v->Ref();",
        "label": [
            "newcode",
            "complex_conflict"
        ],
        "start_line": 1438,
        "end_line": 1447,
        "merge_gen_region": null,
        "confidence": 0.8653661608695984,
        "hesitated": true
    },
    {
        "a_contents": "Status VersionSet::LogAndApply(ColumnFamilyData* column_family_data,\n                               VersionEdit* edit,\n                               port::Mutex* mu,\n                               bool new_descriptor_log) {",
        "b_contents": "Status VersionSet::LogAndApply(VersionEdit* edit, port::Mutex* mu,\n                               bool new_descriptor_log) {",
        "base_contents": "Status VersionSet::LogAndApply(VersionEdit* edit, port::Mutex* mu,\n    bool new_descriptor_log) {",
        "res_region": "Status VersionSet::LogAndApply(ColumnFamilyData* column_family_data,\n                               VersionEdit* edit,\n                               port::Mutex* mu,\n                               bool new_descriptor_log) {",
        "lookback": "                               VersionEdit* edit,",
        "lookahead": "  mu->AssertHeld();",
        "label": [
            "ours",
            "base_underutilized"
        ],
        "start_line": 1457,
        "end_line": 1468,
        "merge_gen_region": null,
        "confidence": 0.8154638409614563,
        "hesitated": true
    },
    {
        "a_contents": "    manifest_file_size_ = new_manifest_file_size;\n    AppendVersion(column_family_data, v);",
        "b_contents": "    manifest_file_size_ = new_manifest_file_size;\n    AppendVersion(v);",
        "base_contents": "    v->offset_manifest_file_ = new_manifest_file_size;\n    AppendVersion(v);",
        "res_region": "    manifest_file_size_ = new_manifest_file_size;\n    AppendVersion(column_family_data, v);",
        "lookback": "    AppendVersion(column_family_data, v);",
        "lookahead": "    log_number_ = edit->log_number_;",
        "label": [
            "ours",
            "complex_conflict"
        ],
        "start_line": 1601,
        "end_line": 1610,
        "merge_gen_region": null,
        "confidence": 0.8592355847358704,
        "hesitated": true
    },
    {
        "a_contents": "void VersionSet::LogAndApplyHelper(Builder* builder,\n                                   Version* v,\n                                   VersionEdit* edit,\n                                   port::Mutex* mu) {",
        "b_contents": "void VersionSet::LogAndApplyHelper(Builder* builder, Version* v,\n                                   VersionEdit* edit, port::Mutex* mu) {",
        "base_contents": "void VersionSet::LogAndApplyHelper(Builder* builder, Version* v,\n  VersionEdit* edit, port::Mutex* mu) {",
        "res_region": "void VersionSet::LogAndApplyHelper(Builder* builder, Version* v,\n                                   VersionEdit* edit, port::Mutex* mu) {",
        "lookback": "                                   VersionEdit* edit, port::Mutex* mu) {",
        "lookahead": "  mu->AssertHeld();",
        "label": [
            "ours",
            "style_related"
        ],
        "start_line": 1642,
        "end_line": 1653,
        "merge_gen_region": null,
        "confidence": 0.7846081256866455,
        "hesitated": true
    },
    {
        "a_contents": "      if (!s.ok()) {\n        break;\n      }\n\n      if (edit.is_column_family_add_) {\n        ColumnFamilyData* new_cfd =\n            CreateColumnFamily(ColumnFamilyOptions(), &edit);\n        builders.insert(\n            {edit.column_family_, new Builder(this, new_cfd->current)});\n      } else if (edit.is_column_family_drop_) {\n        auto builder = builders.find(edit.column_family_);\n        assert(builder != builders.end());\n        delete builder->second;\n        builders.erase(builder);\n        DropColumnFamily(&edit);\n      } else {\n        // if it isn't column family add or column family drop,\n        // then it's a file add/delete, which should be forwarded\n        // to builder\n        auto builder = builders.find(edit.column_family_);\n        assert(builder != builders.end());\n        builder->second->Apply(&edit);",
        "b_contents": "      if (edit.max_level_ >= current_->NumberLevels()) {\n        s = Status::InvalidArgument(\n            \"db has more levels than options.num_levels\");\n        break;",
        "base_contents": "      if (s.ok()) {\n        builder.Apply(&edit);",
        "res_region": "      if (edit.has_comparator_ &&\n          edit.comparator_ != icmp_.user_comparator()->Name()) {\n        s = Status::InvalidArgument(\n            icmp_.user_comparator()->Name(),\n            \"does not match existing comparator \" + edit.comparator_);\n        break;\n      }\n\n      if (edit.is_column_family_add_) {\n        ColumnFamilyData* new_cfd =\n            CreateColumnFamily(ColumnFamilyOptions(), &edit);\n        builders.insert(\n            {edit.column_family_, new Builder(this, new_cfd->current)});\n      } else if (edit.is_column_family_drop_) {\n        auto builder = builders.find(edit.column_family_);\n        assert(builder != builders.end());\n        delete builder->second;\n        builders.erase(builder);\n        DropColumnFamily(&edit);\n      } else {\n        auto cfd = column_family_data_.find(edit.column_family_);\n        assert(cfd != column_family_data_.end());\n        if (edit.max_level_ >= cfd->second->current->NumberLevels()) {\n          s = Status::InvalidArgument(\n              \"db has more levels than options.num_levels\");\n          break;\n        }\n\n        // if it isn't column family add or column family drop,\n        // then it's a file add/delete, which should be forwarded\n        // to builder\n        auto builder = builders.find(edit.column_family_);\n        assert(builder != builders.end());\n        builder->second->Apply(&edit);",
        "lookback": "          edit.comparator_ != icmp_.user_comparator()->Name()) {",
        "lookahead": "      }",
        "label": [
            "newcode",
            "complex_conflict"
        ],
        "start_line": 1739,
        "end_line": 1770,
        "merge_gen_region": null,
        "confidence": 0.8548702001571655,
        "hesitated": true
    },
    {
        "a_contents": "      // Install recovered version\n      std::vector<uint64_t> size_being_compacted(NumberLevels()-1);\n      SizeBeingCompacted(size_being_compacted);\n      Finalize(v, size_being_compacted);\n      AppendVersion(cfd.second, v);\n    }",
        "b_contents": "    // Install recovered version\n    std::vector<uint64_t> size_being_compacted(v->NumberLevels() - 1);\n    compaction_picker_->SizeBeingCompacted(size_being_compacted);\n    v->Finalize(size_being_compacted);",
        "base_contents": "    // Install recovered version\n    std::vector<uint64_t> size_being_compacted(NumberLevels()-1);\n    SizeBeingCompacted(size_being_compacted);\n    Finalize(v, size_being_compacted);",
        "res_region": "      // Install recovered version\n      std::vector<uint64_t> size_being_compacted(v->NumberLevels() - 1);\n      compaction_picker_->SizeBeingCompacted(size_being_compacted);\n      v->Finalize(size_being_compacted);\n      AppendVersion(cfd.second, v);\n    }",
        "lookback": "      std::vector<uint64_t> size_being_compacted(v->NumberLevels() - 1);",
        "lookahead": "",
        "label": [
            "newcode",
            "complex_conflict"
        ],
        "start_line": 1828,
        "end_line": 1845,
        "merge_gen_region": null,
        "confidence": 0.8355757594108582,
        "hesitated": true
    },
    {
        "a_contents": "    manifest_file_size_ = manifest_file_size;",
        "b_contents": "    manifest_file_size_ = manifest_file_size;\n    AppendVersion(v);",
        "base_contents": "    v->offset_manifest_file_ = manifest_file_size;\n    AppendVersion(v);",
        "res_region": "    manifest_file_size_ = manifest_file_size;",
        "lookback": "    manifest_file_number_ = next_file;",
        "lookahead": "    manifest_file_number_ = next_file;",
        "label": [
            "ours",
            "complex_conflict"
        ],
        "start_line": 1847,
        "end_line": 1855,
        "merge_gen_region": null,
        "confidence": 0.4991447627544403,
        "hesitated": true
    },
    {
        "a_contents": "Status VersionSet::DumpManifest(Options& options,\n                                std::string& dscname,\n                                bool verbose,\n                                bool hex) {",
        "b_contents": "Status VersionSet::DumpManifest(Options& options, std::string& dscname,\n                                bool verbose, bool hex) {",
        "base_contents": "Status VersionSet::DumpManifest(Options& options, std::string& dscname,\n    bool verbose, bool hex) {",
        "res_region": "Status VersionSet::DumpManifest(Options& options, std::string& dscname,\n                                bool verbose, bool hex) {",
        "lookback": "                                bool verbose, bool hex) {",
        "lookahead": "  struct LogReporter : public log::Reader::Reporter {",
        "label": [
            "ours",
            "style_related"
        ],
        "start_line": 1881,
        "end_line": 1892,
        "merge_gen_region": null,
        "confidence": 0.838178277015686,
        "hesitated": true
    },
    {
        "a_contents": "",
        "b_contents": "    // Install recovered version\n    std::vector<uint64_t> size_being_compacted(v->NumberLevels() - 1);\n    compaction_picker_->SizeBeingCompacted(size_being_compacted);\n    v->Finalize(size_being_compacted);\n\n    AppendVersion(v);",
        "base_contents": "    // Install recovered version\n    std::vector<uint64_t> size_being_compacted(NumberLevels()-1);\n    SizeBeingCompacted(size_being_compacted);\n    Finalize(v, size_being_compacted);\n\n    AppendVersion(v);",
        "res_region": "",
        "lookback": "    next_file_number_ = next_file + 1;",
        "lookahead": "    manifest_file_number_ = next_file;",
        "label": [
            "ours",
            "one_side_deletion"
        ],
        "start_line": 1997,
        "end_line": 2012,
        "merge_gen_region": null,
        "confidence": 0.7841700911521912,
        "hesitated": true
    },
    {
        "a_contents": "  // update the max compaction score in levels 1 to n-1\n  v->max_compaction_score_ = max_score;\n  v->max_compaction_score_level_ = max_score_level;\n\n  // sort all the levels based on their score. Higher scores get listed\n  // first. Use bubble sort because the number of entries are small.\n  for (int i = 0; i <  NumberLevels()-2; i++) {\n    for (int j = i+1; j < NumberLevels()-1; j++) {\n      if (v->compaction_score_[i] < v->compaction_score_[j]) {\n        double score = v->compaction_score_[i];\n        int level = v->compaction_level_[i];\n        v->compaction_score_[i] = v->compaction_score_[j];\n        v->compaction_level_[i] = v->compaction_level_[j];\n        v->compaction_score_[j] = score;\n        v->compaction_level_[j] = level;\n      }\n    }\n  }\n}\n\n// A static compator used to sort files based on their size\n// In normal mode: descending size\nstatic bool compareSizeDescending(const VersionSet::Fsize& first,\n  const VersionSet::Fsize& second) {\n  return (first.file->file_size > second.file->file_size);\n}\n// A static compator used to sort files based on their seqno\n// In universal style : descending seqno\nstatic bool compareSeqnoDescending(const VersionSet::Fsize& first,\n  const VersionSet::Fsize& second) {\n  if (first.file->smallest_seqno > second.file->smallest_seqno) {\n    assert(first.file->largest_seqno > second.file->largest_seqno);\n    return true;\n  }\n  assert(first.file->largest_seqno <= second.file->largest_seqno);\n  return false;\n}\n\n// sort all files in level1 to level(n-1) based on file size\nvoid VersionSet::UpdateFilesBySize(Version* v) {\n\n  // No need to sort the highest level because it is never compacted.\n  int max_level = (options_->compaction_style == kCompactionStyleUniversal) ?\n                  NumberLevels() : NumberLevels() - 1;\n\n  for (int level = 0; level < max_level; level++) {\n\n    const std::vector<FileMetaData*>& files = v->files_[level];\n    std::vector<int>& files_by_size = v->files_by_size_[level];\n    assert(files_by_size.size() == 0);\n\n    // populate a temp vector for sorting based on size\n    std::vector<Fsize> temp(files.size());\n    for (unsigned int i = 0; i < files.size(); i++) {\n      temp[i].index = i;\n      temp[i].file = files[i];\n    }\n\n    // sort the top number_of_files_to_sort_ based on file size\n    if (options_->compaction_style == kCompactionStyleUniversal) {\n      int num = temp.size();\n      std::partial_sort(temp.begin(),  temp.begin() + num,\n                        temp.end(), compareSeqnoDescending);\n    } else {\n      int num = Version::number_of_files_to_sort_;\n      if (num > (int)temp.size()) {\n        num = temp.size();\n      }\n      std::partial_sort(temp.begin(),  temp.begin() + num,\n                        temp.end(), compareSizeDescending);\n    }\n    assert(temp.size() == files.size());\n\n    // initialize files_by_size_\n    for (unsigned int i = 0; i < temp.size(); i++) {\n      files_by_size.push_back(temp[i].index);\n    }\n    v->next_file_to_compact_by_size_[level] = 0;\n    assert(v->files_[level].size() == v->files_by_size_[level].size());\n  }\n}\n\nStatus VersionSet::WriteSnapshot(log::Writer* log) {\n  // TODO: Break up into multiple records to reduce memory usage on recovery?\n\n  for (auto cfd : column_family_data_) {\n    {\n      // Store column family info\n      VersionEdit edit(0);\n      if (cfd.first != 0) {\n        // default column family is always there,\n        // no need to explicitly write it\n        edit.AddColumnFamily(cfd.second->name);\n        edit.SetColumnFamily(cfd.first);\n        std::string record;\n        edit.EncodeTo(&record);\n        Status s = log->AddRecord(record);\n        if (!s.ok()) {\n          return s;\n        }\n      }\n    }\n\n    {\n      // Save files\n      VersionEdit edit(NumberLevels());\n      edit.SetColumnFamily(cfd.first);\n\n      for (int level = 0; level < NumberLevels(); level++) {\n        const std::vector<FileMetaData*>& files =\n            cfd.second->current->files_[level];\n        for (size_t i = 0; i < files.size(); i++) {\n          const FileMetaData* f = files[i];\n          edit.AddFile(level,\n                       f->number,\n                       f->file_size,\n                       f->smallest,\n                       f->largest,\n                       f->smallest_seqno,\n                       f->largest_seqno);\n        }\n      }\n      std::string record;\n      edit.EncodeTo(&record);\n      Status s = log->AddRecord(record);\n      if (!s.ok()) {\n        return s;\n      }\n    }\n  }\n\n  // Save metadata\n  VersionEdit edit(NumberLevels());\n  edit.SetComparatorName(icmp_.user_comparator()->Name());\n\n  // Save compaction pointers\n  for (int level = 0; level < NumberLevels(); level++) {\n    if (!compact_pointer_[level].empty()) {\n      InternalKey key;\n      key.DecodeFrom(compact_pointer_[level]);\n      edit.SetCompactPointer(level, key);\n    }\n  }\n\n  std::string record;\n  edit.EncodeTo(&record);\n  return log->AddRecord(record);\n}\n\nint VersionSet::NumLevelFiles(int level) const {\n  assert(level >= 0);\n  assert(level < NumberLevels());\n  // TODO this only works for default column family now\n  assert(column_family_data_.find(0) != column_family_data_.end());\n  Version* version = column_family_data_.find(0)->second->current;\n  return version->files_[level].size();\n}\n\nconst char* VersionSet::LevelSummary(LevelSummaryStorage* scratch) const {\n  // TODO this only works for default column family now\n  Version* version = column_family_data_.find(0)->second->current;\n  int len = snprintf(scratch->buffer, sizeof(scratch->buffer), \"files[\");\n  for (int i = 0; i < NumberLevels(); i++) {\n    int sz = sizeof(scratch->buffer) - len;\n    int ret = snprintf(scratch->buffer + len, sz, \"%d \",\n        int(version->files_[i].size()));\n    if (ret < 0 || ret >= sz)\n      break;\n    len += ret;\n  }\n  snprintf(scratch->buffer + len, sizeof(scratch->buffer) - len, \"]\");\n  return scratch->buffer;\n}\n\nconst char* VersionSet::LevelDataSizeSummary(\n    LevelSummaryStorage* scratch) const {\n  int len = snprintf(scratch->buffer, sizeof(scratch->buffer), \"files_size[\");\n  for (int i = 0; i < NumberLevels(); i++) {\n    int sz = sizeof(scratch->buffer) - len;\n    int ret = snprintf(scratch->buffer + len, sz, \"%lu \",\n        (unsigned long)NumLevelBytes(i));\n    if (ret < 0 || ret >= sz)\n      break;\n    len += ret;\n  }\n  snprintf(scratch->buffer + len, sizeof(scratch->buffer) - len, \"]\");\n  return scratch->buffer;\n}\n\nconst char* VersionSet::LevelFileSummary(Version* v,\n                                         FileSummaryStorage* scratch,\n                                         int level) const {\n  int len = snprintf(scratch->buffer, sizeof(scratch->buffer), \"files_size[\");\n  for (unsigned int i = 0; i < v->files_[level].size(); i++) {\n    FileMetaData* f = v->files_[level][i];\n    int sz = sizeof(scratch->buffer) - len;\n    int ret = snprintf(scratch->buffer + len, sz,\n                       \"#%lu(seq=%lu,sz=%lu,%lu) \",\n                       (unsigned long)f->number,\n                       (unsigned long)f->smallest_seqno,\n                       (unsigned long)f->file_size,\n                       (unsigned long)f->being_compacted);\n    if (ret < 0 || ret >= sz)\n      break;\n    len += ret;\n  }\n  snprintf(scratch->buffer + len, sizeof(scratch->buffer) - len, \"]\");\n  return scratch->buffer;",
        "b_contents": "  std::string record;\n  edit.EncodeTo(&record);\n  return log->AddRecord(record);",
        "base_contents": "  // update the max compaction score in levels 1 to n-1\n  v->max_compaction_score_ = max_score;\n  v->max_compaction_score_level_ = max_score_level;\n\n  // sort all the levels based on their score. Higher scores get listed\n  // first. Use bubble sort because the number of entries are small.\n  for (int i = 0; i <  NumberLevels()-2; i++) {\n    for (int j = i+1; j < NumberLevels()-1; j++) {\n      if (v->compaction_score_[i] < v->compaction_score_[j]) {\n        double score = v->compaction_score_[i];\n        int level = v->compaction_level_[i];\n        v->compaction_score_[i] = v->compaction_score_[j];\n        v->compaction_level_[i] = v->compaction_level_[j];\n        v->compaction_score_[j] = score;\n        v->compaction_level_[j] = level;\n      }\n    }\n  }\n}\n\n// A static compator used to sort files based on their size\n// In normal mode: descending size\nstatic bool compareSizeDescending(const VersionSet::Fsize& first,\n  const VersionSet::Fsize& second) {\n  return (first.file->file_size > second.file->file_size);\n}\n// A static compator used to sort files based on their seqno\n// In universal style : descending seqno\nstatic bool compareSeqnoDescending(const VersionSet::Fsize& first,\n  const VersionSet::Fsize& second) {\n  if (first.file->smallest_seqno > second.file->smallest_seqno) {\n    assert(first.file->largest_seqno > second.file->largest_seqno);\n    return true;\n  }\n  assert(first.file->largest_seqno <= second.file->largest_seqno);\n  return false;\n}\n\n// sort all files in level1 to level(n-1) based on file size\nvoid VersionSet::UpdateFilesBySize(Version* v) {\n\n  // No need to sort the highest level because it is never compacted.\n  int max_level = (options_->compaction_style == kCompactionStyleUniversal) ?\n                  NumberLevels() : NumberLevels() - 1;\n\n  for (int level = 0; level < max_level; level++) {\n\n    const std::vector<FileMetaData*>& files = v->files_[level];\n    std::vector<int>& files_by_size = v->files_by_size_[level];\n    assert(files_by_size.size() == 0);\n\n    // populate a temp vector for sorting based on size\n    std::vector<Fsize> temp(files.size());\n    for (unsigned int i = 0; i < files.size(); i++) {\n      temp[i].index = i;\n      temp[i].file = files[i];\n    }\n\n    // sort the top number_of_files_to_sort_ based on file size\n    if (options_->compaction_style == kCompactionStyleUniversal) {\n      int num = temp.size();\n      std::partial_sort(temp.begin(),  temp.begin() + num,\n                        temp.end(), compareSeqnoDescending);\n    } else {\n      int num = Version::number_of_files_to_sort_;\n      if (num > (int)temp.size()) {\n        num = temp.size();\n      }\n      std::partial_sort(temp.begin(),  temp.begin() + num,\n                        temp.end(), compareSizeDescending);\n    }\n    assert(temp.size() == files.size());\n\n    // initialize files_by_size_\n    for (unsigned int i = 0; i < temp.size(); i++) {\n      files_by_size.push_back(temp[i].index);\n    }\n    v->next_file_to_compact_by_size_[level] = 0;\n    assert(v->files_[level].size() == v->files_by_size_[level].size());\n  }\n}\n\nStatus VersionSet::WriteSnapshot(log::Writer* log) {\n  // TODO: Break up into multiple records to reduce memory usage on recovery?\n\n  // Save metadata\n  VersionEdit edit(NumberLevels());\n  edit.SetComparatorName(icmp_.user_comparator()->Name());\n\n  // Save compaction pointers\n  for (int level = 0; level < NumberLevels(); level++) {\n    if (!compact_pointer_[level].empty()) {\n      InternalKey key;\n      key.DecodeFrom(compact_pointer_[level]);\n      edit.SetCompactPointer(level, key);\n    }\n  }\n\n  // Save files\n  for (int level = 0; level < NumberLevels(); level++) {\n    const std::vector<FileMetaData*>& files = current_->files_[level];\n    for (size_t i = 0; i < files.size(); i++) {\n      const FileMetaData* f = files[i];\n      edit.AddFile(level, f->number, f->file_size, f->smallest, f->largest,\n                   f->smallest_seqno, f->largest_seqno);\n    }\n  }\n\n  std::string record;\n  edit.EncodeTo(&record);\n  return log->AddRecord(record);\n}\n\nint VersionSet::NumLevelFiles(int level) const {\n  assert(level >= 0);\n  assert(level < NumberLevels());\n  return current_->files_[level].size();\n}\n\nconst char* VersionSet::LevelSummary(LevelSummaryStorage* scratch) const {\n  int len = snprintf(scratch->buffer, sizeof(scratch->buffer), \"files[\");\n  for (int i = 0; i < NumberLevels(); i++) {\n    int sz = sizeof(scratch->buffer) - len;\n    int ret = snprintf(scratch->buffer + len, sz, \"%d \",\n        int(current_->files_[i].size()));\n    if (ret < 0 || ret >= sz)\n      break;\n    len += ret;\n  }\n  snprintf(scratch->buffer + len, sizeof(scratch->buffer) - len, \"]\");\n  return scratch->buffer;\n}\n\nconst char* VersionSet::LevelDataSizeSummary(\n    LevelSummaryStorage* scratch) const {\n  int len = snprintf(scratch->buffer, sizeof(scratch->buffer), \"files_size[\");\n  for (int i = 0; i < NumberLevels(); i++) {\n    int sz = sizeof(scratch->buffer) - len;\n    int ret = snprintf(scratch->buffer + len, sz, \"%lu \",\n        (unsigned long)NumLevelBytes(i));\n    if (ret < 0 || ret >= sz)\n      break;\n    len += ret;\n  }\n  snprintf(scratch->buffer + len, sizeof(scratch->buffer) - len, \"]\");\n  return scratch->buffer;\n}\n\nconst char* VersionSet::LevelFileSummary(\n    FileSummaryStorage* scratch, int level) const {\n  int len = snprintf(scratch->buffer, sizeof(scratch->buffer), \"files_size[\");\n  for (unsigned int i = 0; i < current_->files_[level].size(); i++) {\n    FileMetaData* f = current_->files_[level][i];\n    int sz = sizeof(scratch->buffer) - len;\n    int ret = snprintf(scratch->buffer + len, sz,\n                       \"#%lu(seq=%lu,sz=%lu,%lu) \",\n                       (unsigned long)f->number,\n                       (unsigned long)f->smallest_seqno,\n                       (unsigned long)f->file_size,\n                       (unsigned long)f->being_compacted);\n    if (ret < 0 || ret >= sz)\n      break;\n    len += ret;\n  }\n  snprintf(scratch->buffer + len, sizeof(scratch->buffer) - len, \"]\");\n  return scratch->buffer;",
        "res_region": "  for (auto cfd : column_family_data_) {\n    {\n      // Store column family info\n      VersionEdit edit;\n      if (cfd.first != 0) {\n        // default column family is always there,\n        // no need to explicitly write it\n        edit.AddColumnFamily(cfd.second->name);\n        edit.SetColumnFamily(cfd.first);\n        std::string record;\n        edit.EncodeTo(&record);\n        Status s = log->AddRecord(record);\n        if (!s.ok()) {\n          return s;\n        }\n      }\n    }\n\n    {\n      // Save files\n      VersionEdit edit;\n      edit.SetColumnFamily(cfd.first);\n\n      for (int level = 0; level < NumberLevels(); level++) {\n        const std::vector<FileMetaData*>& files =\n            cfd.second->current->files_[level];\n        for (size_t i = 0; i < files.size(); i++) {\n          const FileMetaData* f = files[i];\n          edit.AddFile(level,\n                       f->number,\n                       f->file_size,\n                       f->smallest,\n                       f->largest,\n                       f->smallest_seqno,\n                       f->largest_seqno);\n        }\n      }\n      std::string record;\n      edit.EncodeTo(&record);\n      Status s = log->AddRecord(record);\n      if (!s.ok()) {\n        return s;\n      }\n    }\n  }\n\n  // Save metadata\n  VersionEdit edit;\n  edit.SetComparatorName(icmp_.user_comparator()->Name());\n\n  std::string record;\n  edit.EncodeTo(&record);\n  return log->AddRecord(record);",
        "lookback": "    {",
        "lookahead": "}",
        "label": [
            "newcode",
            "complex_conflict"
        ],
        "start_line": 2055,
        "end_line": 2435,
        "merge_gen_region": null,
        "confidence": 0.6707906723022461,
        "hesitated": true
    },
    {
        "a_contents": "  for (auto cfd : column_family_data_) {\n    for (Version* v = cfd.second->dummy_versions.next_;\n         v != &cfd.second->dummy_versions;\n         v = v->next_) {\n      for (int level = 0; level < NumberLevels(); level++) {\n        total_files += v->files_[level].size();\n      }",
        "b_contents": "  for (Version* v = dummy_versions_.next_;\n       v != &dummy_versions_;\n       v = v->next_) {\n    for (int level = 0; level < v->NumberLevels(); level++) {\n      total_files += v->files_[level].size();",
        "base_contents": "  for (Version* v = dummy_versions_.next_;\n       v != &dummy_versions_;\n       v = v->next_) {\n    for (int level = 0; level < NumberLevels(); level++) {\n      total_files += v->files_[level].size();",
        "res_region": "  for (auto cfd : column_family_data_) {\n    for (Version* v = cfd.second->dummy_versions.next_;\n         v != &cfd.second->dummy_versions; v = v->next_) {\n      for (int level = 0; level < v->NumberLevels(); level++) {\n        total_files += v->files_[level].size();\n      }",
        "lookback": "    for (Version* v = cfd.second->dummy_versions.next_;",
        "lookahead": "    }",
        "label": [
            "newcode",
            "complex_conflict"
        ],
        "start_line": 2503,
        "end_line": 2523,
        "merge_gen_region": null,
        "confidence": 0.8624534010887146,
        "hesitated": true
    },
    {
        "a_contents": "  for (auto cfd : column_family_data_) {\n    for (Version* v = cfd.second->dummy_versions.next_;\n         v != &cfd.second->dummy_versions;\n         v = v->next_) {\n      for (int level = 0; level < NumberLevels(); level++) {\n        for (const auto& f : v->files_[level]) {\n          live_list->push_back(f->number);\n        }",
        "b_contents": "  for (Version* v = dummy_versions_.next_;\n       v != &dummy_versions_;\n       v = v->next_) {\n    for (int level = 0; level < v->NumberLevels(); level++) {\n      for (const auto& f : v->files_[level]) {\n        live_list->push_back(f->number);",
        "base_contents": "  for (Version* v = dummy_versions_.next_;\n       v != &dummy_versions_;\n       v = v->next_) {\n    for (int level = 0; level < NumberLevels(); level++) {\n      for (const auto& f : v->files_[level]) {\n        live_list->push_back(f->number);",
        "res_region": "  for (auto cfd : column_family_data_) {\n    for (Version* v = cfd.second->dummy_versions.next_;\n         v != &cfd.second->dummy_versions; v = v->next_) {\n      for (int level = 0; level < v->NumberLevels(); level++) {\n        for (const auto& f : v->files_[level]) {\n          live_list->push_back(f->number);\n        }",
        "lookback": "    for (Version* v = cfd.second->dummy_versions.next_;",
        "lookahead": "      }",
        "label": [
            "newcode",
            "complex_conflict"
        ],
        "start_line": 2530,
        "end_line": 2553,
        "merge_gen_region": null,
        "confidence": 0.8710503578186035,
        "hesitated": true
    },
    {
        "a_contents": "void VersionSet::AddLiveFilesCurrentVersion(std::set<uint64_t>* live) {\n  // TODO this only works for default column family now\n  Version* v = column_family_data_.find(0)->second->current;\n  for (int level = 0; level < NumberLevels(); level++) {\n    const std::vector<FileMetaData*>& files = v->files_[level];\n    for (size_t i = 0; i < files.size(); i++) {\n      live->insert(files[i]->number);\n    }\n  }\n}\n\nint64_t VersionSet::NumLevelBytes(int level) const {\n  // TODO this only works for default column family now\n  Version* version = column_family_data_.find(0)->second->current;\n  assert(level >= 0);\n  assert(level < NumberLevels());\n  assert(version);\n  return TotalFileSize(version->files_[level]);\n}\n\nint64_t VersionSet::MaxNextLevelOverlappingBytes() {\n  // TODO this only works for default column family now\n  Version* version = column_family_data_.find(0)->second->current;\n  uint64_t result = 0;\n  std::vector<FileMetaData*> overlaps;\n  for (int level = 1; level < NumberLevels() - 1; level++) {\n    for (size_t i = 0; i < version->files_[level].size(); i++) {\n      const FileMetaData* f = version->files_[level][i];\n      version->GetOverlappingInputs(\n          level + 1, &f->smallest, &f->largest, &overlaps);\n      const uint64_t sum = TotalFileSize(overlaps);\n      if (sum > result) {\n        result = sum;\n      }\n    }\n  }\n  return result;\n}\n\n// Stores the minimal range that covers all entries in inputs in\n// *smallest, *largest.\n// REQUIRES: inputs is not empty\nvoid VersionSet::GetRange(const std::vector<FileMetaData*>& inputs,\n                          InternalKey* smallest,\n                          InternalKey* largest) {\n  assert(!inputs.empty());\n  smallest->Clear();\n  largest->Clear();\n  for (size_t i = 0; i < inputs.size(); i++) {\n    FileMetaData* f = inputs[i];\n    if (i == 0) {\n      *smallest = f->smallest;\n      *largest = f->largest;\n    } else {\n      if (icmp_.Compare(f->smallest, *smallest) < 0) {\n        *smallest = f->smallest;\n      }\n      if (icmp_.Compare(f->largest, *largest) > 0) {\n        *largest = f->largest;\n      }\n    }\n  }",
        "b_contents": "Compaction* VersionSet::PickCompaction() {\n  return compaction_picker_->PickCompaction(current_);",
        "base_contents": "void VersionSet::AddLiveFilesCurrentVersion(std::set<uint64_t>* live) {\n  Version* v = current_;\n  for (int level = 0; level < NumberLevels(); level++) {\n    const std::vector<FileMetaData*>& files = v->files_[level];\n    for (size_t i = 0; i < files.size(); i++) {\n      live->insert(files[i]->number);\n    }\n  }\n}\n\nint64_t VersionSet::NumLevelBytes(int level) const {\n  assert(level >= 0);\n  assert(level < NumberLevels());\n  assert(current_);\n  return TotalFileSize(current_->files_[level]);\n}\n\nint64_t VersionSet::MaxNextLevelOverlappingBytes() {\n  uint64_t result = 0;\n  std::vector<FileMetaData*> overlaps;\n  for (int level = 1; level < NumberLevels() - 1; level++) {\n    for (size_t i = 0; i < current_->files_[level].size(); i++) {\n      const FileMetaData* f = current_->files_[level][i];\n      current_->GetOverlappingInputs(level+1, &f->smallest, &f->largest,\n                                     &overlaps);\n      const uint64_t sum = TotalFileSize(overlaps);\n      if (sum > result) {\n        result = sum;\n      }\n    }\n  }\n  return result;\n}\n\n// Stores the minimal range that covers all entries in inputs in\n// *smallest, *largest.\n// REQUIRES: inputs is not empty\nvoid VersionSet::GetRange(const std::vector<FileMetaData*>& inputs,\n                          InternalKey* smallest,\n                          InternalKey* largest) {\n  assert(!inputs.empty());\n  smallest->Clear();\n  largest->Clear();\n  for (size_t i = 0; i < inputs.size(); i++) {\n    FileMetaData* f = inputs[i];\n    if (i == 0) {\n      *smallest = f->smallest;\n      *largest = f->largest;\n    } else {\n      if (icmp_.Compare(f->smallest, *smallest) < 0) {\n        *smallest = f->smallest;\n      }\n      if (icmp_.Compare(f->largest, *largest) > 0) {\n        *largest = f->largest;\n      }\n    }\n  }",
        "res_region": "Compaction* VersionSet::PickCompaction() {\n  // TODO this only works for default column family now\n  Version* version = column_family_data_.find(0)->second->current;\n  return compaction_picker_->PickCompaction(version);",
        "lookback": "  // TODO this only works for default column family now",
        "lookahead": "}",
        "label": [
            "newcode",
            "complex_conflict"
        ],
        "start_line": 2559,
        "end_line": 2683,
        "merge_gen_region": null,
        "confidence": 0.602384626865387,
        "hesitated": true
    },
    {
        "a_contents": "  c->MarkFilesBeingCompacted(false);\n  compactions_in_progress_[c->level()].erase(c);\n  if (!status.ok()) {\n    c->ResetNextCompactionIndex();\n  }\n}\n\n// The total size of files that are currently being compacted\n// at at every level upto the penultimate level.\nvoid VersionSet::SizeBeingCompacted(std::vector<uint64_t>& sizes) {\n  for (int level = 0; level < NumberLevels()-1; level++) {\n    uint64_t total = 0;\n    for (std::set<Compaction*>::iterator it =\n         compactions_in_progress_[level].begin();\n         it != compactions_in_progress_[level].end();\n         ++it) {\n      Compaction* c = (*it);\n      assert(c->level() == level);\n      for (int i = 0; i < c->num_input_files(0); i++) {\n        total += c->input(0,i)->file_size;\n      }\n    }\n    sizes[level] = total;\n  }\n}\n\n//\n// Look at overall size amplification. If size amplification\n// exceeeds the configured value, then do a compaction\n// of the candidate files all the way upto the earliest\n// base file (overrides configured values of file-size ratios,\n// min_merge_width and max_merge_width).\n//\nCompaction* VersionSet::PickCompactionUniversalSizeAmp(int level,\n                                                       double score) {\n  assert (level == 0);\n  // TODO this only works for default column family now\n  Version* version = column_family_data_.find(0)->second->current;\n\n  // percentage flexibilty while reducing size amplification\n  uint64_t ratio = options_->compaction_options_universal.\n                     max_size_amplification_percent;\n\n  // The files are sorted from newest first to oldest last.\n  std::vector<int>& file_by_time = version->files_by_size_[level];\n  assert(file_by_time.size() == version->files_[level].size());\n\n  unsigned int candidate_count = 0;\n  uint64_t candidate_size = 0;\n  unsigned int start_index = 0;\n  FileMetaData* f = nullptr;\n\n  // Skip files that are already being compacted\n  for (unsigned int loop = 0; loop < file_by_time.size() - 1; loop++) {\n    int index = file_by_time[loop];\n    f = version->files_[level][index];\n    if (!f->being_compacted) {\n      start_index = loop;         // Consider this as the first candidate.\n      break;\n    }\n    Log(options_->info_log, \"Universal: skipping file %lu[%d] compacted %s\",\n        (unsigned long)f->number,\n        loop,\n        \" cannot be a candidate to reduce size amp.\\n\");\n    f = nullptr;\n  }\n  if (f == nullptr) {\n    return nullptr;             // no candidate files\n  }\n\n  Log(options_->info_log, \"Universal: First candidate file %lu[%d] %s\",\n      (unsigned long)f->number,\n      start_index,\n      \" to reduce size amp.\\n\");\n\n  // keep adding up all the remaining files\n  for (unsigned int loop = start_index; loop < file_by_time.size() - 1;\n       loop++) {\n    int index = file_by_time[loop];\n    f = version->files_[level][index];\n    if (f->being_compacted) {\n      Log(options_->info_log,\n          \"Universal: Possible candidate file %lu[%d] %s.\",\n          (unsigned long)f->number,\n          loop,\n          \" is already being compacted. No size amp reduction possible.\\n\");\n      return nullptr;\n    }\n    candidate_size += f->file_size;\n    candidate_count++;\n  }\n  if (candidate_count == 0) {\n    return nullptr;\n  }\n\n  // size of earliest file\n  int index = file_by_time[file_by_time.size() - 1];\n  uint64_t earliest_file_size = version->files_[level][index]->file_size;\n\n  // size amplification = percentage of additional size\n  if (candidate_size * 100 < ratio * earliest_file_size) {\n    Log(options_->info_log,\n        \"Universal: size amp not needed. newer-files-total-size %lu \"\n        \"earliest-file-size %lu\",\n        (unsigned long)candidate_size,\n        (unsigned long)earliest_file_size);\n    return nullptr;\n  } else {\n    Log(options_->info_log,\n        \"Universal: size amp needed. newer-files-total-size %lu \"\n        \"earliest-file-size %lu\",\n        (unsigned long)candidate_size,\n        (unsigned long)earliest_file_size);\n  }\n  assert(start_index >= 0 && start_index < file_by_time.size() - 1);\n\n  // create a compaction request\n  // We always compact all the files, so always compress.\n  Compaction* c = new Compaction(level,\n                                 level,\n                                 MaxFileSizeForLevel(level),\n                                 LLONG_MAX,\n                                 NumberLevels(),\n                                 version,\n                                 false,\n                                 true);\n  c->score_ = score;\n  for (unsigned int loop = start_index; loop < file_by_time.size(); loop++) {\n    int index = file_by_time[loop];\n    f = version->files_[level][index];\n    c->inputs_[0].push_back(f);\n    Log(options_->info_log,\n        \"Universal: size amp picking file %lu[%d] with size %lu\",\n        (unsigned long)f->number,\n        index,\n        (unsigned long)f->file_size);\n  }\n  return c;\n}\n\n//\n// Consider compaction files based on their size differences with\n// the next file in time order.\n//\nCompaction* VersionSet::PickCompactionUniversalReadAmp(\n    int level, double score, unsigned int ratio,\n    unsigned int max_number_of_files_to_compact) {\n\n  // TODO this only works for default column family now\n  Version* version = column_family_data_.find(0)->second->current;\n\n  unsigned int min_merge_width =\n    options_->compaction_options_universal.min_merge_width;\n  unsigned int max_merge_width =\n    options_->compaction_options_universal.max_merge_width;\n\n  // The files are sorted from newest first to oldest last.\n  std::vector<int>& file_by_time = version->files_by_size_[level];\n  FileMetaData* f = nullptr;\n  bool done = false;\n  int start_index = 0;\n  unsigned int candidate_count;\n  assert(file_by_time.size() == version->files_[level].size());\n\n  unsigned int max_files_to_compact = std::min(max_merge_width,\n                                       max_number_of_files_to_compact);\n  min_merge_width = std::max(min_merge_width, 2U);\n\n  // Considers a candidate file only if it is smaller than the\n  // total size accumulated so far.\n  for (unsigned int loop = 0; loop < file_by_time.size(); loop++) {\n\n    candidate_count = 0;\n\n    // Skip files that are already being compacted\n    for (f = nullptr; loop < file_by_time.size(); loop++) {\n      int index = file_by_time[loop];\n      f = version->files_[level][index];\n\n      if (!f->being_compacted) {\n        candidate_count = 1;\n        break;\n      }\n      Log(options_->info_log,\n          \"Universal: file %lu[%d] being compacted, skipping\",\n          (unsigned long)f->number, loop);\n      f = nullptr;\n    }\n\n    // This file is not being compacted. Consider it as the\n    // first candidate to be compacted.\n    uint64_t candidate_size =  f != nullptr? f->file_size : 0;\n    if (f != nullptr) {\n      Log(options_->info_log, \"Universal: Possible candidate file %lu[%d].\",\n          (unsigned long)f->number, loop);\n    }\n\n    // Check if the suceeding files need compaction.\n    for (unsigned int i = loop+1;\n         candidate_count < max_files_to_compact && i < file_by_time.size();\n         i++) {\n      int index = file_by_time[i];\n      FileMetaData* f = version->files_[level][index];\n      if (f->being_compacted) {\n        break;\n      }\n      // pick files if the total candidate file size (increased by the\n      // specified ratio) is still larger than the next candidate file.\n      uint64_t sz = (candidate_size * (100L + ratio)) /100;\n      if (sz < f->file_size) {\n        break;\n      }\n      candidate_count++;\n      candidate_size += f->file_size;\n    }\n\n    // Found a series of consecutive files that need compaction.\n    if (candidate_count >= (unsigned int)min_merge_width) {\n      start_index = loop;\n      done = true;\n      break;\n    } else {\n      for (unsigned int i = loop;\n           i < loop + candidate_count && i < file_by_time.size(); i++) {\n       int index = file_by_time[i];\n       FileMetaData* f = version->files_[level][index];\n       Log(options_->info_log,\n           \"Universal: Skipping file %lu[%d] with size %lu %d\\n\",\n           (unsigned long)f->number,\n           i,\n           (unsigned long)f->file_size,\n           f->being_compacted);\n      }\n    }\n  }\n  if (!done || candidate_count <= 1) {\n    return nullptr;\n  }\n  unsigned int first_index_after = start_index + candidate_count;\n  // Compression is enabled if files compacted earlier already reached\n  // size ratio of compression.\n  bool enable_compression = true;\n  int ratio_to_compress =\n      options_->compaction_options_universal.compression_size_percent;\n  if (ratio_to_compress >= 0) {\n    uint64_t total_size = TotalFileSize(version->files_[level]);\n    uint64_t older_file_size = 0;\n    for (unsigned int i = file_by_time.size() - 1; i >= first_index_after;\n        i--) {\n      older_file_size += version->files_[level][file_by_time[i]]->file_size;\n      if (older_file_size * 100L >= total_size * (long) ratio_to_compress) {\n        enable_compression = false;\n        break;\n      }\n    }\n  }\n  Compaction* c = new Compaction(level,\n                                 level,\n                                 MaxFileSizeForLevel(level),\n                                 LLONG_MAX,\n                                 NumberLevels(),\n                                 version,\n                                 false,\n                                 enable_compression);\n  c->score_ = score;\n\n  for (unsigned int i = start_index; i < first_index_after; i++) {\n    int index = file_by_time[i];\n    FileMetaData* f = version->files_[level][index];\n    c->inputs_[0].push_back(f);\n    Log(options_->info_log, \"Universal: Picking file %lu[%d] with size %lu\\n\",\n        (unsigned long)f->number,\n        i,\n        (unsigned long)f->file_size);\n  }\n  return c;",
        "b_contents": "  compaction_picker_->ReleaseCompactionFiles(c, status);",
        "base_contents": "  c->MarkFilesBeingCompacted(false);\n  compactions_in_progress_[c->level()].erase(c);\n  if (!status.ok()) {\n    c->ResetNextCompactionIndex();\n  }\n}\n\n// The total size of files that are currently being compacted\n// at at every level upto the penultimate level.\nvoid VersionSet::SizeBeingCompacted(std::vector<uint64_t>& sizes) {\n  for (int level = 0; level < NumberLevels()-1; level++) {\n    uint64_t total = 0;\n    for (std::set<Compaction*>::iterator it =\n         compactions_in_progress_[level].begin();\n         it != compactions_in_progress_[level].end();\n         ++it) {\n      Compaction* c = (*it);\n      assert(c->level() == level);\n      for (int i = 0; i < c->num_input_files(0); i++) {\n        total += c->input(0,i)->file_size;\n      }\n    }\n    sizes[level] = total;\n  }\n}\n\n//\n// Look at overall size amplification. If size amplification\n// exceeeds the configured value, then do a compaction\n// of the candidate files all the way upto the earliest\n// base file (overrides configured values of file-size ratios,\n// min_merge_width and max_merge_width).\n//\nCompaction* VersionSet::PickCompactionUniversalSizeAmp(\n    int level, double score) {\n  assert (level == 0);\n\n  // percentage flexibilty while reducing size amplification\n  uint64_t ratio = options_->compaction_options_universal.\n                     max_size_amplification_percent;\n\n  // The files are sorted from newest first to oldest last.\n  std::vector<int>& file_by_time = current_->files_by_size_[level];\n  assert(file_by_time.size() == current_->files_[level].size());\n\n  unsigned int candidate_count = 0;\n  uint64_t candidate_size = 0;\n  unsigned int start_index = 0;\n  FileMetaData* f = nullptr;\n\n  // Skip files that are already being compacted\n  for (unsigned int loop = 0; loop < file_by_time.size() - 1; loop++) {\n    int index = file_by_time[loop];\n    f = current_->files_[level][index];\n    if (!f->being_compacted) {\n      start_index = loop;         // Consider this as the first candidate.\n      break;\n    }\n    Log(options_->info_log, \"Universal: skipping file %lu[%d] compacted %s\",\n        (unsigned long)f->number,\n        loop,\n        \" cannot be a candidate to reduce size amp.\\n\");\n    f = nullptr;\n  }\n  if (f == nullptr) {\n    return nullptr;             // no candidate files\n  }\n\n  Log(options_->info_log, \"Universal: First candidate file %lu[%d] %s\",\n      (unsigned long)f->number,\n      start_index,\n      \" to reduce size amp.\\n\");\n\n  // keep adding up all the remaining files\n  for (unsigned int loop = start_index; loop < file_by_time.size() - 1;\n       loop++) {\n    int index = file_by_time[loop];\n    f = current_->files_[level][index];\n    if (f->being_compacted) {\n      Log(options_->info_log,\n          \"Universal: Possible candidate file %lu[%d] %s.\",\n          (unsigned long)f->number,\n          loop,\n          \" is already being compacted. No size amp reduction possible.\\n\");\n      return nullptr;\n    }\n    candidate_size += f->file_size;\n    candidate_count++;\n  }\n  if (candidate_count == 0) {\n    return nullptr;\n  }\n\n  // size of earliest file\n  int index = file_by_time[file_by_time.size() - 1];\n  uint64_t earliest_file_size = current_->files_[level][index]->file_size;\n\n  // size amplification = percentage of additional size\n  if (candidate_size * 100 < ratio * earliest_file_size) {\n    Log(options_->info_log,\n        \"Universal: size amp not needed. newer-files-total-size %lu \"\n        \"earliest-file-size %lu\",\n        (unsigned long)candidate_size,\n        (unsigned long)earliest_file_size);\n    return nullptr;\n  } else {\n    Log(options_->info_log,\n        \"Universal: size amp needed. newer-files-total-size %lu \"\n        \"earliest-file-size %lu\",\n        (unsigned long)candidate_size,\n        (unsigned long)earliest_file_size);\n  }\n  assert(start_index >= 0 && start_index < file_by_time.size() - 1);\n\n  // create a compaction request\n  // We always compact all the files, so always compress.\n  Compaction* c = new Compaction(level, level, MaxFileSizeForLevel(level),\n                                 LLONG_MAX, NumberLevels(), false,\n                                 true);\n  c->score_ = score;\n  for (unsigned int loop = start_index; loop < file_by_time.size(); loop++) {\n    int index = file_by_time[loop];\n    f = current_->files_[level][index];\n    c->inputs_[0].push_back(f);\n    Log(options_->info_log,\n        \"Universal: size amp picking file %lu[%d] with size %lu\",\n        (unsigned long)f->number,\n        index,\n        (unsigned long)f->file_size);\n  }\n  return c;\n}\n\n//\n// Consider compaction files based on their size differences with\n// the next file in time order.\n//\nCompaction* VersionSet::PickCompactionUniversalReadAmp(\n    int level, double score, unsigned int ratio,\n    unsigned int max_number_of_files_to_compact) {\n\n  unsigned int min_merge_width =\n    options_->compaction_options_universal.min_merge_width;\n  unsigned int max_merge_width =\n    options_->compaction_options_universal.max_merge_width;\n\n  // The files are sorted from newest first to oldest last.\n  std::vector<int>& file_by_time = current_->files_by_size_[level];\n  FileMetaData* f = nullptr;\n  bool done = false;\n  int start_index = 0;\n  unsigned int candidate_count;\n  assert(file_by_time.size() == current_->files_[level].size());\n\n  unsigned int max_files_to_compact = std::min(max_merge_width,\n                                       max_number_of_files_to_compact);\n  min_merge_width = std::max(min_merge_width, 2U);\n\n  // Considers a candidate file only if it is smaller than the\n  // total size accumulated so far.\n  for (unsigned int loop = 0; loop < file_by_time.size(); loop++) {\n\n    candidate_count = 0;\n\n    // Skip files that are already being compacted\n    for (f = nullptr; loop < file_by_time.size(); loop++) {\n      int index = file_by_time[loop];\n      f = current_->files_[level][index];\n\n      if (!f->being_compacted) {\n        candidate_count = 1;\n        break;\n      }\n      Log(options_->info_log,\n          \"Universal: file %lu[%d] being compacted, skipping\",\n          (unsigned long)f->number, loop);\n      f = nullptr;\n    }\n\n    // This file is not being compacted. Consider it as the\n    // first candidate to be compacted.\n    uint64_t candidate_size =  f != nullptr? f->file_size : 0;\n    if (f != nullptr) {\n      Log(options_->info_log, \"Universal: Possible candidate file %lu[%d].\",\n          (unsigned long)f->number, loop);\n    }\n\n    // Check if the suceeding files need compaction.\n    for (unsigned int i = loop+1;\n         candidate_count < max_files_to_compact && i < file_by_time.size();\n         i++) {\n      int index = file_by_time[i];\n      FileMetaData* f = current_->files_[level][index];\n      if (f->being_compacted) {\n        break;\n      }\n      // pick files if the total candidate file size (increased by the\n      // specified ratio) is still larger than the next candidate file.\n      uint64_t sz = (candidate_size * (100L + ratio)) /100;\n      if (sz < f->file_size) {\n        break;\n      }\n      candidate_count++;\n      candidate_size += f->file_size;\n    }\n\n    // Found a series of consecutive files that need compaction.\n    if (candidate_count >= (unsigned int)min_merge_width) {\n      start_index = loop;\n      done = true;\n      break;\n    } else {\n      for (unsigned int i = loop;\n           i < loop + candidate_count && i < file_by_time.size(); i++) {\n       int index = file_by_time[i];\n       FileMetaData* f = current_->files_[level][index];\n       Log(options_->info_log,\n           \"Universal: Skipping file %lu[%d] with size %lu %d\\n\",\n           (unsigned long)f->number,\n           i,\n           (unsigned long)f->file_size,\n           f->being_compacted);\n      }\n    }\n  }\n  if (!done || candidate_count <= 1) {\n    return nullptr;\n  }\n  unsigned int first_index_after = start_index + candidate_count;\n  // Compression is enabled if files compacted earlier already reached\n  // size ratio of compression.\n  bool enable_compression = true;\n  int ratio_to_compress =\n      options_->compaction_options_universal.compression_size_percent;\n  if (ratio_to_compress >= 0) {\n    uint64_t total_size = TotalFileSize(current_->files_[level]);\n    uint64_t older_file_size = 0;\n    for (unsigned int i = file_by_time.size() - 1; i >= first_index_after;\n        i--) {\n      older_file_size += current_->files_[level][file_by_time[i]]->file_size;\n      if (older_file_size * 100L >= total_size * (long) ratio_to_compress) {\n        enable_compression = false;\n        break;\n      }\n    }\n  }\n  Compaction* c = new Compaction(level, level, MaxFileSizeForLevel(level),\n                                 LLONG_MAX, NumberLevels(), false,\n                                 enable_compression);\n  c->score_ = score;\n\n  for (unsigned int i = start_index; i < first_index_after; i++) {\n    int index = file_by_time[i];\n    FileMetaData* f = current_->files_[level][index];\n    c->inputs_[0].push_back(f);\n    Log(options_->info_log, \"Universal: Picking file %lu[%d] with size %lu\\n\",\n        (unsigned long)f->number,\n        i,\n        (unsigned long)f->file_size);\n  }\n  return c;",
        "res_region": "  compaction_picker_->ReleaseCompactionFiles(c, status);\n}\n\nStatus VersionSet::GetMetadataForFile(uint64_t number, int* filelevel,\n                                      FileMetaData* meta) {",
        "lookback": "}",
        "lookahead": "  for (auto cfd : column_family_data_) {",
        "label": [
            "newcode",
            "complex_conflict"
        ],
        "start_line": 2789,
        "end_line": 3330,
        "merge_gen_region": null,
        "confidence": 0.22660517692565918,
        "hesitated": true
    },
    {
        "a_contents": "//\n// Universal style of compaction. Pick files that are contiguous in\n// time-range to compact.\n//\nCompaction* VersionSet::PickCompactionUniversal(int level, double score) {\n  assert (level == 0);\n  // TODO this only works for default column family now\n  Version* version = column_family_data_.find(0)->second->current;\n\n  if ((version->files_[level].size() <\n      (unsigned int)options_->level0_file_num_compaction_trigger)) {\n    Log(options_->info_log, \"Universal: nothing to do\\n\");\n    return nullptr;\n  }\n  VersionSet::FileSummaryStorage tmp;\n  Log(options_->info_log,\n      \"Universal: candidate files(%lu): %s\\n\",\n      version->files_[level].size(),\n      LevelFileSummary(version, &tmp, 0));\n\n  // Check for size amplification first.\n  Compaction* c = PickCompactionUniversalSizeAmp(level, score);\n  if (c == nullptr) {\n\n    // Size amplification is within limits. Try reducing read\n    // amplification while maintaining file size ratios.\n    unsigned int ratio = options_->compaction_options_universal.size_ratio;\n    c = PickCompactionUniversalReadAmp(level, score, ratio, UINT_MAX);\n\n    // Size amplification and file size ratios are within configured limits.\n    // If max read amplification is exceeding configured limits, then force\n    // compaction without looking at filesize ratios and try to reduce\n    // the number of files to fewer than level0_file_num_compaction_trigger.\n    if (c == nullptr) {\n      unsigned int num_files = version->files_[level].size() -\n                               options_->level0_file_num_compaction_trigger;\n      c = PickCompactionUniversalReadAmp(level, score, UINT_MAX, num_files);\n    }\n  }\n  if (c == nullptr) {\n    return nullptr;\n  }\n  assert(c->inputs_[0].size() > 1);\n\n  // validate that all the chosen files are non overlapping in time\n  FileMetaData* newerfile __attribute__((unused)) = nullptr;\n  for (unsigned int i = 0; i < c->inputs_[0].size(); i++) {\n    FileMetaData* f = c->inputs_[0][i];\n    assert (f->smallest_seqno <= f->largest_seqno);\n    assert(newerfile == nullptr ||\n           newerfile->smallest_seqno > f->largest_seqno);\n    newerfile = f;\n  }\n\n  // The files are sorted from newest first to oldest last.\n  std::vector<int>& file_by_time = version->files_by_size_[level];\n\n  // Is the earliest file part of this compaction?\n  int last_index = file_by_time[file_by_time.size()-1];\n  FileMetaData* last_file = version->files_[level][last_index];\n  if (c->inputs_[0][c->inputs_[0].size()-1] == last_file) {\n    c->bottommost_level_ = true;\n  }\n\n  // update statistics\n  if (options_->statistics != nullptr) {\n    options_->statistics->measureTime(NUM_FILES_IN_SINGLE_COMPACTION,\n                                      c->inputs_[0].size());\n  }\n\n  // mark all the files that are being compacted\n  c->MarkFilesBeingCompacted(true);\n\n  // remember this currently undergoing compaction\n  compactions_in_progress_[level].insert(c);\n\n  // Record whether this compaction includes all sst files.\n  // For now, it is only relevant in universal compaction mode.\n  c->is_full_compaction_ = (c->inputs_[0].size() == version->files_[0].size());\n\n  return c;\n}\n\nCompaction* VersionSet::PickCompactionBySize(int level, double score) {\n  Compaction* c = nullptr;\n  // TODO this only works for default column family now\n  Version* version = column_family_data_.find(0)->second->current;\n\n  // level 0 files are overlapping. So we cannot pick more\n  // than one concurrent compactions at this level. This\n  // could be made better by looking at key-ranges that are\n  // being compacted at level 0.\n  if (level == 0 && compactions_in_progress_[level].size() == 1) {\n    return nullptr;\n  }\n\n  assert(level >= 0);\n  assert(level+1 < NumberLevels());\n  c = new Compaction(level,\n                     level + 1,\n                     MaxFileSizeForLevel(level + 1),\n                     MaxGrandParentOverlapBytes(level),\n                     NumberLevels(),\n                     version);\n  c->score_ = score;\n\n  // Pick the largest file in this level that is not already\n  // being compacted\n  std::vector<int>& file_size = version->files_by_size_[level];\n\n  // record the first file that is not yet compacted\n  int nextIndex = -1;\n\n  for (unsigned int i = version->next_file_to_compact_by_size_[level];\n       i < file_size.size(); i++) {\n    int index = file_size[i];\n    FileMetaData* f = version->files_[level][index];\n\n    // check to verify files are arranged in descending size\n    assert((i == file_size.size() - 1) ||\n           (i >= Version::number_of_files_to_sort_-1) ||\n          (f->file_size >= version->files_[level][file_size[i+1]]->file_size));\n\n    // do not pick a file to compact if it is being compacted\n    // from n-1 level.\n    if (f->being_compacted) {\n      continue;\n    }\n\n    // remember the startIndex for the next call to PickCompaction\n    if (nextIndex == -1) {\n      nextIndex = i;\n    }\n\n    //if (i > Version::number_of_files_to_sort_) {\n    //  Log(options_->info_log, \"XXX Looking at index %d\", i);\n    //}\n\n    // Do not pick this file if its parents at level+1 are being compacted.\n    // Maybe we can avoid redoing this work in SetupOtherInputs\n    int parent_index = -1;\n    if (ParentRangeInCompaction(&f->smallest, &f->largest, level,\n                                &parent_index)) {\n      continue;\n    }\n    c->inputs_[0].push_back(f);\n    c->base_index_ = index;\n    c->parent_index_ = parent_index;\n    break;\n  }\n\n  if (c->inputs_[0].empty()) {\n    delete c;\n    c = nullptr;\n  }\n\n  // store where to start the iteration in the next call to PickCompaction\n  version->next_file_to_compact_by_size_[level] = nextIndex;\n\n  return c;\n}\n\nCompaction* VersionSet::PickCompaction() {\n  Compaction* c = nullptr;\n  // TODO this only works for default column family now\n  Version* version = column_family_data_.find(0)->second->current;\n  int level = -1;\n\n  // Compute the compactions needed. It is better to do it here\n  // and also in LogAndApply(), otherwise the values could be stale.\n  std::vector<uint64_t> size_being_compacted(NumberLevels()-1);\n  version->vset_->SizeBeingCompacted(size_being_compacted);\n  Finalize(version, size_being_compacted);\n\n  // In universal style of compaction, compact L0 files back into L0.\n  if (options_->compaction_style ==  kCompactionStyleUniversal) {\n    int level = 0;\n    c = PickCompactionUniversal(level, version->compaction_score_[level]);\n    return c;\n  }\n\n  // We prefer compactions triggered by too much data in a level over\n  // the compactions triggered by seeks.\n  //\n  // Find the compactions by size on all levels.\n  for (int i = 0; i < NumberLevels()-1; i++) {\n    assert(i == 0 || version->compaction_score_[i] <=\n                     version->compaction_score_[i-1]);\n    level = version->compaction_level_[i];\n    if ((version->compaction_score_[i] >= 1)) {\n      c = PickCompactionBySize(level, version->compaction_score_[i]);\n      ExpandWhileOverlapping(c);\n      if (c != nullptr) {\n        break;\n      }\n    }\n  }\n\n  // Find compactions needed by seeks\n  FileMetaData* f = version->file_to_compact_;\n  if (c == nullptr && f != nullptr && !f->being_compacted) {\n\n    level = version->file_to_compact_level_;\n    int parent_index = -1;\n\n    // Only allow one level 0 compaction at a time.\n    // Do not pick this file if its parents at level+1 are being compacted.\n    if (level != 0 || compactions_in_progress_[0].empty()) {\n      if(!ParentRangeInCompaction(&f->smallest, &f->largest, level,\n                                  &parent_index)) {\n        c = new Compaction(level,\n                           level + 1,\n                           MaxFileSizeForLevel(level + 1),\n                           MaxGrandParentOverlapBytes(level),\n                           NumberLevels(),\n                           version,\n                           true);\n        c->inputs_[0].push_back(f);\n        c->parent_index_ = parent_index;\n        version->file_to_compact_ = nullptr;\n        ExpandWhileOverlapping(c);\n      }\n    }\n  }\n\n  if (c == nullptr) {\n    return nullptr;\n  }\n\n  // Two level 0 compaction won't run at the same time, so don't need to worry\n  // about files on level 0 being compacted.\n  if (level == 0) {\n    assert(compactions_in_progress_[0].empty());\n    InternalKey smallest, largest;\n    GetRange(c->inputs_[0], &smallest, &largest);\n    // Note that the next call will discard the file we placed in\n    // c->inputs_[0] earlier and replace it with an overlapping set\n    // which will include the picked file.\n    c->inputs_[0].clear();\n    version->GetOverlappingInputs(0, &smallest, &largest, &c->inputs_[0]);\n\n    // If we include more L0 files in the same compaction run it can\n    // cause the 'smallest' and 'largest' key to get extended to a\n    // larger range. So, re-invoke GetRange to get the new key range\n    GetRange(c->inputs_[0], &smallest, &largest);\n    if (ParentRangeInCompaction(&smallest, &largest,\n                                level, &c->parent_index_)) {\n      delete c;\n      return nullptr;\n    }\n    assert(!c->inputs_[0].empty());\n  }\n\n  // Setup \"level+1\" files (inputs_[1])\n  SetupOtherInputs(c);\n\n  // mark all the files that are being compacted\n  c->MarkFilesBeingCompacted(true);\n\n  // Is this compaction creating a file at the bottommost level\n  c->SetupBottomMostLevel(false);\n\n  // remember this currently undergoing compaction\n  compactions_in_progress_[level].insert(c);\n\n  return c;\n}\n\n// Returns true if any one of the parent files are being compacted\nbool VersionSet::ParentRangeInCompaction(const InternalKey* smallest,\n  const InternalKey* largest, int level, int* parent_index) {\n  std::vector<FileMetaData*> inputs;\n  // TODO this only works for default column family now\n  Version* version = column_family_data_.find(0)->second->current;\n\n  version->GetOverlappingInputs(\n      level + 1, smallest, largest, &inputs, *parent_index, parent_index);\n  return FilesInCompaction(inputs);\n}\n\n// Returns true if any one of specified files are being compacted\nbool VersionSet::FilesInCompaction(std::vector<FileMetaData*>& files) {\n  for (unsigned int i = 0; i < files.size(); i++) {\n    if (files[i]->being_compacted) {\n      return true;\n    }\n  }\n  return false;\n}\n\n// Add more files to the inputs on \"level\" to make sure that\n// no newer version of a key is compacted to \"level+1\" while leaving an older\n// version in a \"level\". Otherwise, any Get() will search \"level\" first,\n// and will likely return an old/stale value for the key, since it always\n// searches in increasing order of level to find the value. This could\n// also scramble the order of merge operands. This function should be\n// called any time a new Compaction is created, and its inputs_[0] are\n// populated.\n//\n// Will set c to nullptr if it is impossible to apply this compaction.\nvoid VersionSet::ExpandWhileOverlapping(Compaction* c) {\n  // If inputs are empty then there is nothing to expand.\n  if (!c || c->inputs_[0].empty()) {\n    return;\n  }\n\n  // GetOverlappingInputs will always do the right thing for level-0.\n  // So we don't need to do any expansion if level == 0.\n  if (c->level() == 0) {\n    return;\n  }\n\n  const int level = c->level();\n  InternalKey smallest, largest;\n\n  // Keep expanding c->inputs_[0] until we are sure that there is a\n  // \"clean cut\" boundary between the files in input and the surrounding files.\n  // This will ensure that no parts of a key are lost during compaction.\n  int hint_index = -1;\n  size_t old_size;\n  do {\n    old_size = c->inputs_[0].size();\n    GetRange(c->inputs_[0], &smallest, &largest);\n    c->inputs_[0].clear();\n    c->input_version_->GetOverlappingInputs(\n        level, &smallest, &largest, &c->inputs_[0], hint_index, &hint_index);\n  } while(c->inputs_[0].size() > old_size);\n\n  // Get the new range\n  GetRange(c->inputs_[0], &smallest, &largest);\n\n  // If, after the expansion, there are files that are already under\n  // compaction, then we must drop/cancel this compaction.\n  int parent_index = -1;\n  if (FilesInCompaction(c->inputs_[0]) ||\n      ParentRangeInCompaction(&smallest, &largest, level, &parent_index)) {\n    c->inputs_[0].clear();\n    c->inputs_[1].clear();\n    delete c;\n    c = nullptr;\n  }\n}\n\n// Populates the set of inputs from \"level+1\" that overlap with \"level\".\n// Will also attempt to expand \"level\" if that doesn't expand \"level+1\"\n// or cause \"level\" to include a file for compaction that has an overlapping\n// user-key with another file.\nvoid VersionSet::SetupOtherInputs(Compaction* c) {\n  // If inputs are empty, then there is nothing to expand.\n  if (c->inputs_[0].empty()) {\n    return;\n  }\n\n  const int level = c->level();\n  InternalKey smallest, largest;\n\n  // Get the range one last time.\n  GetRange(c->inputs_[0], &smallest, &largest);\n\n  // Populate the set of next-level files (inputs_[1]) to include in compaction\n  c->input_version_->GetOverlappingInputs(level + 1,\n                                          &smallest,\n                                          &largest,\n                                          &c->inputs_[1],\n                                          c->parent_index_,\n                                          &c->parent_index_);\n\n  // Get entire range covered by compaction\n  InternalKey all_start, all_limit;\n  GetRange2(c->inputs_[0], c->inputs_[1], &all_start, &all_limit);\n\n  // See if we can further grow the number of inputs in \"level\" without\n  // changing the number of \"level+1\" files we pick up. We also choose NOT\n  // to expand if this would cause \"level\" to include some entries for some\n  // user key, while excluding other entries for the same user key. This\n  // can happen when one user key spans multiple files.\n  if (!c->inputs_[1].empty()) {\n    std::vector<FileMetaData*> expanded0;\n    c->input_version_->GetOverlappingInputs(\n        level, &all_start, &all_limit, &expanded0, c->base_index_, nullptr);\n    const uint64_t inputs0_size = TotalFileSize(c->inputs_[0]);\n    const uint64_t inputs1_size = TotalFileSize(c->inputs_[1]);\n    const uint64_t expanded0_size = TotalFileSize(expanded0);\n    uint64_t limit = ExpandedCompactionByteSizeLimit(level);\n    if (expanded0.size() > c->inputs_[0].size() &&\n        inputs1_size + expanded0_size < limit &&\n        !FilesInCompaction(expanded0) &&\n        !c->input_version_->HasOverlappingUserKey(&expanded0, level)) {\n      InternalKey new_start, new_limit;\n      GetRange(expanded0, &new_start, &new_limit);\n      std::vector<FileMetaData*> expanded1;\n      c->input_version_->GetOverlappingInputs(level + 1,\n                                              &new_start,\n                                              &new_limit,\n                                              &expanded1,\n                                              c->parent_index_,\n                                              &c->parent_index_);\n      if (expanded1.size() == c->inputs_[1].size() &&\n          !FilesInCompaction(expanded1)) {\n        Log(options_->info_log,\n            \"Expanding@%lu %lu+%lu (%lu+%lu bytes) to %lu+%lu (%lu+%lu bytes)\"\n            \"\\n\",\n            (unsigned long)level,\n            (unsigned long)(c->inputs_[0].size()),\n            (unsigned long)(c->inputs_[1].size()),\n            (unsigned long)inputs0_size,\n            (unsigned long)inputs1_size,\n            (unsigned long)(expanded0.size()),\n            (unsigned long)(expanded1.size()),\n            (unsigned long)expanded0_size,\n            (unsigned long)inputs1_size);\n        smallest = new_start;\n        largest = new_limit;\n        c->inputs_[0] = expanded0;\n        c->inputs_[1] = expanded1;\n        GetRange2(c->inputs_[0], c->inputs_[1], &all_start, &all_limit);\n      }\n    }\n  }\n\n  // Compute the set of grandparent files that overlap this compaction\n  // (parent == level+1; grandparent == level+2)\n  if (level + 2 < NumberLevels()) {\n    c->input_version_->GetOverlappingInputs(\n        level + 2, &all_start, &all_limit, &c->grandparents_);\n  }\n\n  if (false) {\n    Log(options_->info_log, \"Compacting %d '%s' .. '%s'\",\n        level,\n        smallest.DebugString().c_str(),\n        largest.DebugString().c_str());\n  }\n\n  // Update the place where we will do the next compaction for this level.\n  // We update this immediately instead of waiting for the VersionEdit\n  // to be applied so that if the compaction fails, we will try a different\n  // key range next time.\n  compact_pointer_[level] = largest.Encode().ToString();\n  c->edit_->SetCompactPointer(level, largest);\n}\n\nStatus VersionSet::GetMetadataForFile(\n    uint64_t number,\n    int *filelevel,\n    FileMetaData *meta) {\n  for (auto cfd : column_family_data_) {\n    for (int level = 0; level < NumberLevels(); level++) {\n      const std::vector<FileMetaData*>& files =\n          cfd.second->current->files_[level];\n      for (size_t i = 0; i < files.size(); i++) {\n        if (files[i]->number == number) {\n          *meta = *files[i];\n          *filelevel = level;\n          return Status::OK();\n        }",
        "b_contents": "Status VersionSet::GetMetadataForFile(uint64_t number, int* filelevel,\n                                      FileMetaData* meta) {\n  for (int level = 0; level < NumberLevels(); level++) {\n    const std::vector<FileMetaData*>& files = current_->files_[level];\n    for (size_t i = 0; i < files.size(); i++) {\n      if (files[i]->number == number) {\n        *meta = *files[i];\n        *filelevel = level;\n        return Status::OK();",
        "base_contents": "//\n// Universal style of compaction. Pick files that are contiguous in\n// time-range to compact.\n//\nCompaction* VersionSet::PickCompactionUniversal(int level, double score) {\n  assert (level == 0);\n\n  if ((current_->files_[level].size() <\n      (unsigned int)options_->level0_file_num_compaction_trigger)) {\n    Log(options_->info_log, \"Universal: nothing to do\\n\");\n    return nullptr;\n  }\n  VersionSet::FileSummaryStorage tmp;\n  Log(options_->info_log, \"Universal: candidate files(%lu): %s\\n\",\n      current_->files_[level].size(),\n      LevelFileSummary(&tmp, 0));\n\n  // Check for size amplification first.\n  Compaction* c = PickCompactionUniversalSizeAmp(level, score);\n  if (c == nullptr) {\n\n    // Size amplification is within limits. Try reducing read\n    // amplification while maintaining file size ratios.\n    unsigned int ratio = options_->compaction_options_universal.size_ratio;\n    c = PickCompactionUniversalReadAmp(level, score, ratio, UINT_MAX);\n\n    // Size amplification and file size ratios are within configured limits.\n    // If max read amplification is exceeding configured limits, then force\n    // compaction without looking at filesize ratios and try to reduce\n    // the number of files to fewer than level0_file_num_compaction_trigger.\n    if (c == nullptr) {\n      unsigned int num_files = current_->files_[level].size() -\n                               options_->level0_file_num_compaction_trigger;\n      c = PickCompactionUniversalReadAmp(level, score, UINT_MAX, num_files);\n    }\n  }\n  if (c == nullptr) {\n    return nullptr;\n  }\n  assert(c->inputs_[0].size() > 1);\n\n  // validate that all the chosen files are non overlapping in time\n  FileMetaData* newerfile __attribute__((unused)) = nullptr;\n  for (unsigned int i = 0; i < c->inputs_[0].size(); i++) {\n    FileMetaData* f = c->inputs_[0][i];\n    assert (f->smallest_seqno <= f->largest_seqno);\n    assert(newerfile == nullptr ||\n           newerfile->smallest_seqno > f->largest_seqno);\n    newerfile = f;\n  }\n\n  // The files are sorted from newest first to oldest last.\n  std::vector<int>& file_by_time = current_->files_by_size_[level];\n\n  // Is the earliest file part of this compaction?\n  int last_index = file_by_time[file_by_time.size()-1];\n  FileMetaData* last_file = current_->files_[level][last_index];\n  if (c->inputs_[0][c->inputs_[0].size()-1] == last_file) {\n    c->bottommost_level_ = true;\n  }\n\n  // update statistics\n  if (options_->statistics != nullptr) {\n    options_->statistics->measureTime(NUM_FILES_IN_SINGLE_COMPACTION,\n                                      c->inputs_[0].size());\n  }\n\n  c->input_version_ = current_;\n  c->input_version_->Ref();\n\n  // mark all the files that are being compacted\n  c->MarkFilesBeingCompacted(true);\n\n  // remember this currently undergoing compaction\n  compactions_in_progress_[level].insert(c);\n\n  // Record whether this compaction includes all sst files.\n  // For now, it is only relevant in universal compaction mode.\n  c->is_full_compaction_ = (c->inputs_[0].size() == current_->files_[0].size());\n\n  return c;\n}\n\nCompaction* VersionSet::PickCompactionBySize(int level, double score) {\n  Compaction* c = nullptr;\n\n  // level 0 files are overlapping. So we cannot pick more\n  // than one concurrent compactions at this level. This\n  // could be made better by looking at key-ranges that are\n  // being compacted at level 0.\n  if (level == 0 && compactions_in_progress_[level].size() == 1) {\n    return nullptr;\n  }\n\n  assert(level >= 0);\n  assert(level+1 < NumberLevels());\n  c = new Compaction(level, level+1, MaxFileSizeForLevel(level+1),\n      MaxGrandParentOverlapBytes(level), NumberLevels());\n  c->score_ = score;\n\n  // Pick the largest file in this level that is not already\n  // being compacted\n  std::vector<int>& file_size = current_->files_by_size_[level];\n\n  // record the first file that is not yet compacted\n  int nextIndex = -1;\n\n  for (unsigned int i = current_->next_file_to_compact_by_size_[level];\n       i < file_size.size(); i++) {\n    int index = file_size[i];\n    FileMetaData* f = current_->files_[level][index];\n\n    // check to verify files are arranged in descending size\n    assert((i == file_size.size() - 1) ||\n           (i >= Version::number_of_files_to_sort_-1) ||\n          (f->file_size >= current_->files_[level][file_size[i+1]]->file_size));\n\n    // do not pick a file to compact if it is being compacted\n    // from n-1 level.\n    if (f->being_compacted) {\n      continue;\n    }\n\n    // remember the startIndex for the next call to PickCompaction\n    if (nextIndex == -1) {\n      nextIndex = i;\n    }\n\n    //if (i > Version::number_of_files_to_sort_) {\n    //  Log(options_->info_log, \"XXX Looking at index %d\", i);\n    //}\n\n    // Do not pick this file if its parents at level+1 are being compacted.\n    // Maybe we can avoid redoing this work in SetupOtherInputs\n    int parent_index = -1;\n    if (ParentRangeInCompaction(&f->smallest, &f->largest, level,\n                                &parent_index)) {\n      continue;\n    }\n    c->inputs_[0].push_back(f);\n    c->base_index_ = index;\n    c->parent_index_ = parent_index;\n    break;\n  }\n\n  if (c->inputs_[0].empty()) {\n    delete c;\n    c = nullptr;\n  }\n\n  // store where to start the iteration in the next call to PickCompaction\n  current_->next_file_to_compact_by_size_[level] = nextIndex;\n\n  return c;\n}\n\nCompaction* VersionSet::PickCompaction() {\n  Compaction* c = nullptr;\n  int level = -1;\n\n  // Compute the compactions needed. It is better to do it here\n  // and also in LogAndApply(), otherwise the values could be stale.\n  std::vector<uint64_t> size_being_compacted(NumberLevels()-1);\n  current_->vset_->SizeBeingCompacted(size_being_compacted);\n  Finalize(current_, size_being_compacted);\n\n  // In universal style of compaction, compact L0 files back into L0.\n  if (options_->compaction_style ==  kCompactionStyleUniversal) {\n    int level = 0;\n    c = PickCompactionUniversal(level, current_->compaction_score_[level]);\n    return c;\n  }\n\n  // We prefer compactions triggered by too much data in a level over\n  // the compactions triggered by seeks.\n  //\n  // Find the compactions by size on all levels.\n  for (int i = 0; i < NumberLevels()-1; i++) {\n    assert(i == 0 || current_->compaction_score_[i] <=\n                     current_->compaction_score_[i-1]);\n    level = current_->compaction_level_[i];\n    if ((current_->compaction_score_[i] >= 1)) {\n      c = PickCompactionBySize(level, current_->compaction_score_[i]);\n      ExpandWhileOverlapping(c);\n      if (c != nullptr) {\n        break;\n      }\n    }\n  }\n\n  // Find compactions needed by seeks\n  FileMetaData* f = current_->file_to_compact_;\n  if (c == nullptr && f != nullptr && !f->being_compacted) {\n\n    level = current_->file_to_compact_level_;\n    int parent_index = -1;\n\n    // Only allow one level 0 compaction at a time.\n    // Do not pick this file if its parents at level+1 are being compacted.\n    if (level != 0 || compactions_in_progress_[0].empty()) {\n      if(!ParentRangeInCompaction(&f->smallest, &f->largest, level,\n                                  &parent_index)) {\n        c = new Compaction(level, level+1, MaxFileSizeForLevel(level+1),\n                MaxGrandParentOverlapBytes(level), NumberLevels(), true);\n        c->inputs_[0].push_back(f);\n        c->parent_index_ = parent_index;\n        current_->file_to_compact_ = nullptr;\n        ExpandWhileOverlapping(c);\n      }\n    }\n  }\n\n  if (c == nullptr) {\n    return nullptr;\n  }\n\n  c->input_version_ = current_;\n  c->input_version_->Ref();\n\n  // Two level 0 compaction won't run at the same time, so don't need to worry\n  // about files on level 0 being compacted.\n  if (level == 0) {\n    assert(compactions_in_progress_[0].empty());\n    InternalKey smallest, largest;\n    GetRange(c->inputs_[0], &smallest, &largest);\n    // Note that the next call will discard the file we placed in\n    // c->inputs_[0] earlier and replace it with an overlapping set\n    // which will include the picked file.\n    c->inputs_[0].clear();\n    current_->GetOverlappingInputs(0, &smallest, &largest, &c->inputs_[0]);\n\n    // If we include more L0 files in the same compaction run it can\n    // cause the 'smallest' and 'largest' key to get extended to a\n    // larger range. So, re-invoke GetRange to get the new key range\n    GetRange(c->inputs_[0], &smallest, &largest);\n    if (ParentRangeInCompaction(&smallest, &largest,\n                                level, &c->parent_index_)) {\n      delete c;\n      return nullptr;\n    }\n    assert(!c->inputs_[0].empty());\n  }\n\n  // Setup \"level+1\" files (inputs_[1])\n  SetupOtherInputs(c);\n\n  // mark all the files that are being compacted\n  c->MarkFilesBeingCompacted(true);\n\n  // Is this compaction creating a file at the bottommost level\n  c->SetupBottomMostLevel(false);\n\n  // remember this currently undergoing compaction\n  compactions_in_progress_[level].insert(c);\n\n  return c;\n}\n\n// Returns true if any one of the parent files are being compacted\nbool VersionSet::ParentRangeInCompaction(const InternalKey* smallest,\n  const InternalKey* largest, int level, int* parent_index) {\n  std::vector<FileMetaData*> inputs;\n\n  current_->GetOverlappingInputs(level+1, smallest, largest,\n                                 &inputs, *parent_index, parent_index);\n  return FilesInCompaction(inputs);\n}\n\n// Returns true if any one of specified files are being compacted\nbool VersionSet::FilesInCompaction(std::vector<FileMetaData*>& files) {\n  for (unsigned int i = 0; i < files.size(); i++) {\n    if (files[i]->being_compacted) {\n      return true;\n    }\n  }\n  return false;\n}\n\n// Add more files to the inputs on \"level\" to make sure that\n// no newer version of a key is compacted to \"level+1\" while leaving an older\n// version in a \"level\". Otherwise, any Get() will search \"level\" first,\n// and will likely return an old/stale value for the key, since it always\n// searches in increasing order of level to find the value. This could\n// also scramble the order of merge operands. This function should be\n// called any time a new Compaction is created, and its inputs_[0] are\n// populated.\n//\n// Will set c to nullptr if it is impossible to apply this compaction.\nvoid VersionSet::ExpandWhileOverlapping(Compaction* c) {\n  // If inputs are empty then there is nothing to expand.\n  if (!c || c->inputs_[0].empty()) {\n    return;\n  }\n\n  // GetOverlappingInputs will always do the right thing for level-0.\n  // So we don't need to do any expansion if level == 0.\n  if (c->level() == 0) {\n    return;\n  }\n\n  const int level = c->level();\n  InternalKey smallest, largest;\n\n  // Keep expanding c->inputs_[0] until we are sure that there is a\n  // \"clean cut\" boundary between the files in input and the surrounding files.\n  // This will ensure that no parts of a key are lost during compaction.\n  int hint_index = -1;\n  size_t old_size;\n  do {\n    old_size = c->inputs_[0].size();\n    GetRange(c->inputs_[0], &smallest, &largest);\n    c->inputs_[0].clear();\n    current_->GetOverlappingInputs(level, &smallest, &largest, &c->inputs_[0],\n                                   hint_index, &hint_index);\n  } while(c->inputs_[0].size() > old_size);\n\n  // Get the new range\n  GetRange(c->inputs_[0], &smallest, &largest);\n\n  // If, after the expansion, there are files that are already under\n  // compaction, then we must drop/cancel this compaction.\n  int parent_index = -1;\n  if (FilesInCompaction(c->inputs_[0]) ||\n      ParentRangeInCompaction(&smallest, &largest, level, &parent_index)) {\n    c->inputs_[0].clear();\n    c->inputs_[1].clear();\n    delete c;\n    c = nullptr;\n  }\n}\n\n// Populates the set of inputs from \"level+1\" that overlap with \"level\".\n// Will also attempt to expand \"level\" if that doesn't expand \"level+1\"\n// or cause \"level\" to include a file for compaction that has an overlapping\n// user-key with another file.\nvoid VersionSet::SetupOtherInputs(Compaction* c) {\n  // If inputs are empty, then there is nothing to expand.\n  if (c->inputs_[0].empty()) {\n    return;\n  }\n\n  const int level = c->level();\n  InternalKey smallest, largest;\n\n  // Get the range one last time.\n  GetRange(c->inputs_[0], &smallest, &largest);\n\n  // Populate the set of next-level files (inputs_[1]) to include in compaction\n  current_->GetOverlappingInputs(level+1, &smallest, &largest, &c->inputs_[1],\n                                 c->parent_index_, &c->parent_index_);\n\n  // Get entire range covered by compaction\n  InternalKey all_start, all_limit;\n  GetRange2(c->inputs_[0], c->inputs_[1], &all_start, &all_limit);\n\n  // See if we can further grow the number of inputs in \"level\" without\n  // changing the number of \"level+1\" files we pick up. We also choose NOT\n  // to expand if this would cause \"level\" to include some entries for some\n  // user key, while excluding other entries for the same user key. This\n  // can happen when one user key spans multiple files.\n  if (!c->inputs_[1].empty()) {\n    std::vector<FileMetaData*> expanded0;\n    current_->GetOverlappingInputs(level, &all_start, &all_limit, &expanded0,\n                                   c->base_index_, nullptr);\n    const uint64_t inputs0_size = TotalFileSize(c->inputs_[0]);\n    const uint64_t inputs1_size = TotalFileSize(c->inputs_[1]);\n    const uint64_t expanded0_size = TotalFileSize(expanded0);\n    uint64_t limit = ExpandedCompactionByteSizeLimit(level);\n    if (expanded0.size() > c->inputs_[0].size() &&\n        inputs1_size + expanded0_size < limit &&\n        !FilesInCompaction(expanded0) &&\n        !current_->HasOverlappingUserKey(&expanded0, level)) {\n      InternalKey new_start, new_limit;\n      GetRange(expanded0, &new_start, &new_limit);\n      std::vector<FileMetaData*> expanded1;\n      current_->GetOverlappingInputs(level+1, &new_start, &new_limit,\n                                     &expanded1, c->parent_index_,\n                                     &c->parent_index_);\n      if (expanded1.size() == c->inputs_[1].size() &&\n          !FilesInCompaction(expanded1)) {\n        Log(options_->info_log,\n            \"Expanding@%lu %lu+%lu (%lu+%lu bytes) to %lu+%lu (%lu+%lu bytes)\"\n            \"\\n\",\n            (unsigned long)level,\n            (unsigned long)(c->inputs_[0].size()),\n            (unsigned long)(c->inputs_[1].size()),\n            (unsigned long)inputs0_size,\n            (unsigned long)inputs1_size,\n            (unsigned long)(expanded0.size()),\n            (unsigned long)(expanded1.size()),\n            (unsigned long)expanded0_size,\n            (unsigned long)inputs1_size);\n        smallest = new_start;\n        largest = new_limit;\n        c->inputs_[0] = expanded0;\n        c->inputs_[1] = expanded1;\n        GetRange2(c->inputs_[0], c->inputs_[1], &all_start, &all_limit);\n      }\n    }\n  }\n\n  // Compute the set of grandparent files that overlap this compaction\n  // (parent == level+1; grandparent == level+2)\n  if (level + 2 < NumberLevels()) {\n    current_->GetOverlappingInputs(level + 2, &all_start, &all_limit,\n                                   &c->grandparents_);\n  }\n\n  if (false) {\n    Log(options_->info_log, \"Compacting %d '%s' .. '%s'\",\n        level,\n        smallest.DebugString().c_str(),\n        largest.DebugString().c_str());\n  }\n\n  // Update the place where we will do the next compaction for this level.\n  // We update this immediately instead of waiting for the VersionEdit\n  // to be applied so that if the compaction fails, we will try a different\n  // key range next time.\n  compact_pointer_[level] = largest.Encode().ToString();\n  c->edit_->SetCompactPointer(level, largest);\n}\n\nStatus VersionSet::GetMetadataForFile(\n    uint64_t number,\n    int *filelevel,\n    FileMetaData *meta) {\n  for (int level = 0; level < NumberLevels(); level++) {\n    const std::vector<FileMetaData*>& files = current_->files_[level];\n    for (size_t i = 0; i < files.size(); i++) {\n      if (files[i]->number == number) {\n        *meta = *files[i];\n        *filelevel = level;\n        return Status::OK();",
        "res_region": "Status VersionSet::GetMetadataForFile(uint64_t number, int* filelevel,\n                                      FileMetaData* meta) {\n  for (auto cfd : column_family_data_) {\n    Version* version = cfd.second->current;\n    for (int level = 0; level < version->NumberLevels(); level++) {\n      const std::vector<FileMetaData*>& files = version->files_[level];\n      for (size_t i = 0; i < files.size(); i++) {\n        if (files[i]->number == number) {\n          *meta = *files[i];\n          *filelevel = level;\n          return Status::OK();\n        }",
        "lookback": "                                      FileMetaData* meta) {",
        "lookahead": "      }",
        "label": [
            "newcode",
            "complex_conflict"
        ],
        "start_line": 3333,
        "end_line": 4235,
        "merge_gen_region": null,
        "confidence": 0.6865549087524414,
        "hesitated": true
    },
    {
        "a_contents": "void VersionSet::GetLiveFilesMetaData(std::vector<LiveFileMetaData>* metadata) {\n  for (auto cfd : column_family_data_) {\n    for (int level = 0; level < NumberLevels(); level++) {\n      const std::vector<FileMetaData*>& files =\n          cfd.second->current->files_[level];\n      for (size_t i = 0; i < files.size(); i++) {\n        LiveFileMetaData filemetadata;\n        filemetadata.name = TableFileName(\"\", files[i]->number);\n        filemetadata.level = level;\n        filemetadata.size = files[i]->file_size;\n        filemetadata.smallestkey = files[i]->smallest.user_key().ToString();\n        filemetadata.largestkey = files[i]->largest.user_key().ToString();\n        filemetadata.smallest_seqno = files[i]->smallest_seqno;\n        filemetadata.largest_seqno = files[i]->largest_seqno;\n        metadata->push_back(filemetadata);\n      }",
        "b_contents": "void VersionSet::GetLiveFilesMetaData(std::vector<LiveFileMetaData>* metadata) {\n  for (int level = 0; level < NumberLevels(); level++) {\n    const std::vector<FileMetaData*>& files = current_->files_[level];\n    for (size_t i = 0; i < files.size(); i++) {\n      LiveFileMetaData filemetadata;\n      filemetadata.name = TableFileName(\"\", files[i]->number);\n      filemetadata.level = level;\n      filemetadata.size = files[i]->file_size;\n      filemetadata.smallestkey = files[i]->smallest.user_key().ToString();\n      filemetadata.largestkey = files[i]->largest.user_key().ToString();\n      filemetadata.smallest_seqno = files[i]->smallest_seqno;\n      filemetadata.largest_seqno = files[i]->largest_seqno;\n      metadata->push_back(filemetadata);",
        "base_contents": "void VersionSet::GetLiveFilesMetaData(\n    std::vector<LiveFileMetaData> * metadata) {\n  for (int level = 0; level < NumberLevels(); level++) {\n    const std::vector<FileMetaData*>& files = current_->files_[level];\n    for (size_t i = 0; i < files.size(); i++) {\n      LiveFileMetaData filemetadata;\n      filemetadata.name = TableFileName(\"\", files[i]->number);\n      filemetadata.level = level;\n      filemetadata.size = files[i]->file_size;\n      filemetadata.smallestkey = files[i]->smallest.user_key().ToString();\n      filemetadata.largestkey = files[i]->largest.user_key().ToString();\n      filemetadata.smallest_seqno = files[i]->smallest_seqno;\n      filemetadata.largest_seqno = files[i]->largest_seqno;\n      metadata->push_back(filemetadata);",
        "res_region": "void VersionSet::GetLiveFilesMetaData(std::vector<LiveFileMetaData>* metadata) {\n  for (auto cfd : column_family_data_) {\n    for (int level = 0; level < NumberLevels(); level++) {\n      const std::vector<FileMetaData*>& files =\n          cfd.second->current->files_[level];\n      for (size_t i = 0; i < files.size(); i++) {\n        LiveFileMetaData filemetadata;\n        filemetadata.name = TableFileName(\"\", files[i]->number);\n        filemetadata.level = level;\n        filemetadata.size = files[i]->file_size;\n        filemetadata.smallestkey = files[i]->smallest.user_key().ToString();\n        filemetadata.largestkey = files[i]->largest.user_key().ToString();\n        filemetadata.smallest_seqno = files[i]->smallest_seqno;\n        filemetadata.largest_seqno = files[i]->largest_seqno;\n        metadata->push_back(filemetadata);\n      }",
        "lookback": "  for (auto cfd : column_family_data_) {",
        "lookahead": "    }",
        "label": [
            "ours",
            "base_underutilized"
        ],
        "start_line": 4242,
        "end_line": 4288,
        "merge_gen_region": null,
        "confidence": 0.8384402394294739,
        "hesitated": true
    },
    {
        "a_contents": "ColumnFamilyData* VersionSet::CreateColumnFamily(\n    const ColumnFamilyOptions& options, VersionEdit* edit) {\n  assert(column_families_.find(edit->column_family_name_) ==\n         column_families_.end());\n  assert(edit->is_column_family_add_);\n\n  column_families_.insert({edit->column_family_name_, edit->column_family_});\n  ColumnFamilyData* new_cfd =\n      new ColumnFamilyData(edit->column_family_name_, this, options);\n  column_family_data_.insert({edit->column_family_, new_cfd});\n  max_column_family_ = std::max(max_column_family_, edit->column_family_);\n  AppendVersion(new_cfd, new Version(this, current_version_number_++));\n  return new_cfd;\n}\n\nvoid VersionSet::DropColumnFamily(VersionEdit* edit) {\n  auto cfd = column_family_data_.find(edit->column_family_);\n  assert(cfd != column_family_data_.end());\n  column_families_.erase(cfd->second->name);\n  cfd->second->current->Unref();\n  // List must be empty\n  assert(cfd->second->dummy_versions.next_ == &cfd->second->dummy_versions);\n  // might delete itself\n  cfd->second->Unref();\n  column_family_data_.erase(cfd);\n}\n\nCompaction* VersionSet::CompactRange(int level,\n                                     const InternalKey* begin,\n                                     const InternalKey* end) {\n  std::vector<FileMetaData*> inputs;\n  // TODO this only works for default column family now\n  Version* version = column_family_data_.find(0)->second->current;\n\n  // All files are 'overlapping' in universal style compaction.\n  // We have to compact the entire range in one shot.\n  if (options_->compaction_style == kCompactionStyleUniversal) {\n    begin = nullptr;\n    end = nullptr;\n  }\n  version->GetOverlappingInputs(level, begin, end, &inputs);\n  if (inputs.empty()) {\n    return nullptr;\n  }\n\n  // Avoid compacting too much in one shot in case the range is large.\n  // But we cannot do this for level-0 since level-0 files can overlap\n  // and we must not pick one file and drop another older file if the\n  // two files overlap.\n  if (level > 0) {\n    const uint64_t limit = MaxFileSizeForLevel(level) *\n                         options_->source_compaction_factor;\n    uint64_t total = 0;\n    for (size_t i = 0; i < inputs.size(); ++i) {\n      uint64_t s = inputs[i]->file_size;\n      total += s;\n      if (total >= limit) {\n        inputs.resize(i + 1);\n        break;\n      }\n    }\n  }\n  int out_level = (options_->compaction_style == kCompactionStyleUniversal) ?\n                  level : level+1;\n\n  Compaction* c = new Compaction(level,\n                                 out_level,\n                                 MaxFileSizeForLevel(out_level),\n                                 MaxGrandParentOverlapBytes(level),\n                                 NumberLevels(),\n                                 version);\n\n  c->inputs_[0] = inputs;\n  ExpandWhileOverlapping(c);\n  if (c == nullptr) {\n    Log(options_->info_log, \"Could not compact due to expansion failure.\\n\");\n    return nullptr;\n  }\n\n  SetupOtherInputs(c);\n\n  // These files that are to be manaully compacted do not trample\n  // upon other files because manual compactions are processed when\n  // the system has a max of 1 background compaction thread.\n  c->MarkFilesBeingCompacted(true);\n\n  // Is this compaction creating a file at the bottommost level\n  c->SetupBottomMostLevel(true);\n  return c;\n}\n\nCompaction::Compaction(int level,\n                       int out_level,\n                       uint64_t target_file_size,\n                       uint64_t max_grandparent_overlap_bytes,\n                       int number_levels,\n                       Version* input_version,\n                       bool seek_compaction,\n                       bool enable_compression)\n    : level_(level),\n      out_level_(out_level),\n      max_output_file_size_(target_file_size),\n      maxGrandParentOverlapBytes_(max_grandparent_overlap_bytes),\n      input_version_(input_version),\n      number_levels_(number_levels),\n      seek_compaction_(seek_compaction),\n      enable_compression_(enable_compression),\n      grandparent_index_(0),\n      seen_key_(false),\n      overlapped_bytes_(0),\n      base_index_(-1),\n      parent_index_(-1),\n      score_(0),\n      bottommost_level_(false),\n      is_full_compaction_(false),\n      level_ptrs_(std::vector<size_t>(number_levels)) {\n  input_version_->Ref();\n  edit_ = new VersionEdit(number_levels_);\n  for (int i = 0; i < number_levels_; i++) {\n    level_ptrs_[i] = 0;\n  }\n}\n\nCompaction::~Compaction() {\n  delete edit_;\n  if (input_version_ != nullptr) {\n    input_version_->Unref();\n  }\n}\n\nbool Compaction::IsTrivialMove() const {\n  // Avoid a move if there is lots of overlapping grandparent data.\n  // Otherwise, the move could create a parent file that will require\n  // a very expensive merge later on.\n  return (num_input_files(0) == 1 &&\n          num_input_files(1) == 0 &&\n          TotalFileSize(grandparents_) <= maxGrandParentOverlapBytes_);\n}\n\nvoid Compaction::AddInputDeletions(VersionEdit* edit) {\n  for (int which = 0; which < 2; which++) {\n    for (size_t i = 0; i < inputs_[which].size(); i++) {\n      edit->DeleteFile(level_ + which, inputs_[which][i]->number);\n    }\n  }\n}\n\nbool Compaction::IsBaseLevelForKey(const Slice& user_key) {\n  if (input_version_->vset_->options_->compaction_style ==\n      kCompactionStyleUniversal) {\n    return bottommost_level_;\n  }\n  // Maybe use binary search to find right entry instead of linear search?\n  const Comparator* user_cmp = input_version_->vset_->icmp_.user_comparator();\n  for (int lvl = level_ + 2; lvl < number_levels_; lvl++) {\n    const std::vector<FileMetaData*>& files = input_version_->files_[lvl];\n    for (; level_ptrs_[lvl] < files.size(); ) {\n      FileMetaData* f = files[level_ptrs_[lvl]];\n      if (user_cmp->Compare(user_key, f->largest.user_key()) <= 0) {\n        // We've advanced far enough\n        if (user_cmp->Compare(user_key, f->smallest.user_key()) >= 0) {\n          // Key falls in this file's range, so definitely not base level\n          return false;\n        }\n        break;\n      }\n      level_ptrs_[lvl]++;\n    }\n  }\n  return true;\n}\n\nbool Compaction::ShouldStopBefore(const Slice& internal_key) {\n  // Scan to find earliest grandparent file that contains key.\n  const InternalKeyComparator* icmp = &input_version_->vset_->icmp_;\n  while (grandparent_index_ < grandparents_.size() &&\n      icmp->Compare(internal_key,\n                    grandparents_[grandparent_index_]->largest.Encode()) > 0) {\n    if (seen_key_) {\n      overlapped_bytes_ += grandparents_[grandparent_index_]->file_size;\n    }\n    assert(grandparent_index_ + 1 >= grandparents_.size() ||\n           icmp->Compare(grandparents_[grandparent_index_]->largest.Encode(),\n                         grandparents_[grandparent_index_+1]->smallest.Encode())\n                         < 0);\n    grandparent_index_++;\n  }\n  seen_key_ = true;\n\n  if (overlapped_bytes_ > maxGrandParentOverlapBytes_) {\n    // Too much overlap for current output; start new output\n    overlapped_bytes_ = 0;\n    return true;\n  } else {\n    return false;\n  }\n}\n\n// Mark (or clear) each file that is being compacted\nvoid Compaction::MarkFilesBeingCompacted(bool value) {\n  for (int i = 0; i < 2; i++) {\n    std::vector<FileMetaData*> v = inputs_[i];\n    for (unsigned int j = 0; j < inputs_[i].size(); j++) {\n      assert(value ? !inputs_[i][j]->being_compacted :\n                      inputs_[i][j]->being_compacted);\n      inputs_[i][j]->being_compacted = value;\n    }\n  }\n}\n\n// Is this compaction producing files at the bottommost level?\nvoid Compaction::SetupBottomMostLevel(bool isManual) {\n  if (input_version_->vset_->options_->compaction_style  ==\n         kCompactionStyleUniversal) {\n    // If universal compaction style is used and manual\n    // compaction is occuring, then we are guaranteed that\n    // all files will be picked in a single compaction\n    // run. We can safely set bottommost_level_ = true.\n    // If it is not manual compaction, then bottommost_level_\n    // is already set when the Compaction was created.\n    if (isManual) {\n      bottommost_level_ = true;\n    }\n    return;\n  }\n  bottommost_level_ = true;\n  int num_levels = input_version_->vset_->NumberLevels();\n  for (int i = level() + 2; i < num_levels; i++) {\n    if (input_version_->vset_->NumLevelFiles(i) > 0) {\n      bottommost_level_ = false;\n      break;\n    }\n  }\n}\n\nvoid Compaction::ReleaseInputs() {\n  if (input_version_ != nullptr) {\n    input_version_->Unref();\n    input_version_ = nullptr;\n  }\n}\n\nvoid Compaction::ResetNextCompactionIndex() {\n  input_version_->ResetNextCompactionIndex(level_);\n}\n\nstatic void InputSummary(std::vector<FileMetaData*>& files,\n    char* output,\n    int len) {\n  int write = 0;\n  for (unsigned int i = 0; i < files.size(); i++) {\n    int sz = len - write;\n    int ret = snprintf(output + write, sz, \"%lu(%lu) \",\n        (unsigned long)files.at(i)->number,\n        (unsigned long)files.at(i)->file_size);\n    if (ret < 0 || ret >= sz)\n      break;\n    write += ret;\n  }\n}\n\nvoid Compaction::Summary(char* output, int len) {\n  int write = snprintf(output, len,\n      \"Base version %lu Base level %d, seek compaction:%d, inputs:\",\n      (unsigned long)input_version_->GetVersionNumber(),\n      level_,\n      seek_compaction_);\n  if (write < 0 || write > len) {\n    return;\n  }\n\n  char level_low_summary[100];\n  InputSummary(inputs_[0], level_low_summary, sizeof(level_low_summary));\n  char level_up_summary[100];\n  if (inputs_[1].size()) {\n    InputSummary(inputs_[1], level_up_summary, sizeof(level_up_summary));\n  } else {\n    level_up_summary[0] = '\\0';\n  }\n\n  snprintf(output + write, len - write, \"[%s],[%s]\",\n      level_low_summary, level_up_summary);\n}\n",
        "b_contents": "",
        "base_contents": "Compaction* VersionSet::CompactRange(\n    int level,\n    const InternalKey* begin,\n    const InternalKey* end) {\n  std::vector<FileMetaData*> inputs;\n\n  // All files are 'overlapping' in universal style compaction.\n  // We have to compact the entire range in one shot.\n  if (options_->compaction_style == kCompactionStyleUniversal) {\n    begin = nullptr;\n    end = nullptr;\n  }\n  current_->GetOverlappingInputs(level, begin, end, &inputs);\n  if (inputs.empty()) {\n    return nullptr;\n  }\n\n  // Avoid compacting too much in one shot in case the range is large.\n  // But we cannot do this for level-0 since level-0 files can overlap\n  // and we must not pick one file and drop another older file if the\n  // two files overlap.\n  if (level > 0) {\n    const uint64_t limit = MaxFileSizeForLevel(level) *\n                         options_->source_compaction_factor;\n    uint64_t total = 0;\n    for (size_t i = 0; i < inputs.size(); ++i) {\n      uint64_t s = inputs[i]->file_size;\n      total += s;\n      if (total >= limit) {\n        inputs.resize(i + 1);\n        break;\n      }\n    }\n  }\n  int out_level = (options_->compaction_style == kCompactionStyleUniversal) ?\n                  level : level+1;\n\n  Compaction* c = new Compaction(level, out_level, MaxFileSizeForLevel(out_level),\n    MaxGrandParentOverlapBytes(level), NumberLevels());\n\n  c->inputs_[0] = inputs;\n  ExpandWhileOverlapping(c);\n  if (c == nullptr) {\n    Log(options_->info_log, \"Could not compact due to expansion failure.\\n\");\n    return nullptr;\n  }\n\n  c->input_version_ = current_;\n  c->input_version_->Ref();\n  SetupOtherInputs(c);\n\n  // These files that are to be manaully compacted do not trample\n  // upon other files because manual compactions are processed when\n  // the system has a max of 1 background compaction thread.\n  c->MarkFilesBeingCompacted(true);\n\n  // Is this compaction creating a file at the bottommost level\n  c->SetupBottomMostLevel(true);\n  return c;\n}\n\nCompaction::Compaction(int level, int out_level, uint64_t target_file_size,\n  uint64_t max_grandparent_overlap_bytes, int number_levels,\n  bool seek_compaction, bool enable_compression)\n    : level_(level),\n      out_level_(out_level),\n      max_output_file_size_(target_file_size),\n      maxGrandParentOverlapBytes_(max_grandparent_overlap_bytes),\n      input_version_(nullptr),\n      number_levels_(number_levels),\n      seek_compaction_(seek_compaction),\n      enable_compression_(enable_compression),\n      grandparent_index_(0),\n      seen_key_(false),\n      overlapped_bytes_(0),\n      base_index_(-1),\n      parent_index_(-1),\n      score_(0),\n      bottommost_level_(false),\n      is_full_compaction_(false),\n      level_ptrs_(std::vector<size_t>(number_levels)) {\n  edit_ = new VersionEdit(number_levels_);\n  for (int i = 0; i < number_levels_; i++) {\n    level_ptrs_[i] = 0;\n  }\n}\n\nCompaction::~Compaction() {\n  delete edit_;\n  if (input_version_ != nullptr) {\n    input_version_->Unref();\n  }\n}\n\nbool Compaction::IsTrivialMove() const {\n  // Avoid a move if there is lots of overlapping grandparent data.\n  // Otherwise, the move could create a parent file that will require\n  // a very expensive merge later on.\n  return (num_input_files(0) == 1 &&\n          num_input_files(1) == 0 &&\n          TotalFileSize(grandparents_) <= maxGrandParentOverlapBytes_);\n}\n\nvoid Compaction::AddInputDeletions(VersionEdit* edit) {\n  for (int which = 0; which < 2; which++) {\n    for (size_t i = 0; i < inputs_[which].size(); i++) {\n      edit->DeleteFile(level_ + which, inputs_[which][i]->number);\n    }\n  }\n}\n\nbool Compaction::IsBaseLevelForKey(const Slice& user_key) {\n  if (input_version_->vset_->options_->compaction_style ==\n      kCompactionStyleUniversal) {\n    return bottommost_level_;\n  }\n  // Maybe use binary search to find right entry instead of linear search?\n  const Comparator* user_cmp = input_version_->vset_->icmp_.user_comparator();\n  for (int lvl = level_ + 2; lvl < number_levels_; lvl++) {\n    const std::vector<FileMetaData*>& files = input_version_->files_[lvl];\n    for (; level_ptrs_[lvl] < files.size(); ) {\n      FileMetaData* f = files[level_ptrs_[lvl]];\n      if (user_cmp->Compare(user_key, f->largest.user_key()) <= 0) {\n        // We've advanced far enough\n        if (user_cmp->Compare(user_key, f->smallest.user_key()) >= 0) {\n          // Key falls in this file's range, so definitely not base level\n          return false;\n        }\n        break;\n      }\n      level_ptrs_[lvl]++;\n    }\n  }\n  return true;\n}\n\nbool Compaction::ShouldStopBefore(const Slice& internal_key) {\n  // Scan to find earliest grandparent file that contains key.\n  const InternalKeyComparator* icmp = &input_version_->vset_->icmp_;\n  while (grandparent_index_ < grandparents_.size() &&\n      icmp->Compare(internal_key,\n                    grandparents_[grandparent_index_]->largest.Encode()) > 0) {\n    if (seen_key_) {\n      overlapped_bytes_ += grandparents_[grandparent_index_]->file_size;\n    }\n    assert(grandparent_index_ + 1 >= grandparents_.size() ||\n           icmp->Compare(grandparents_[grandparent_index_]->largest.Encode(),\n                         grandparents_[grandparent_index_+1]->smallest.Encode())\n                         < 0);\n    grandparent_index_++;\n  }\n  seen_key_ = true;\n\n  if (overlapped_bytes_ > maxGrandParentOverlapBytes_) {\n    // Too much overlap for current output; start new output\n    overlapped_bytes_ = 0;\n    return true;\n  } else {\n    return false;\n  }\n}\n\n// Mark (or clear) each file that is being compacted\nvoid Compaction::MarkFilesBeingCompacted(bool value) {\n  for (int i = 0; i < 2; i++) {\n    std::vector<FileMetaData*> v = inputs_[i];\n    for (unsigned int j = 0; j < inputs_[i].size(); j++) {\n      assert(value ? !inputs_[i][j]->being_compacted :\n                      inputs_[i][j]->being_compacted);\n      inputs_[i][j]->being_compacted = value;\n    }\n  }\n}\n\n// Is this compaction producing files at the bottommost level?\nvoid Compaction::SetupBottomMostLevel(bool isManual) {\n  if (input_version_->vset_->options_->compaction_style  ==\n         kCompactionStyleUniversal) {\n    // If universal compaction style is used and manual\n    // compaction is occuring, then we are guaranteed that\n    // all files will be picked in a single compaction\n    // run. We can safely set bottommost_level_ = true.\n    // If it is not manual compaction, then bottommost_level_\n    // is already set when the Compaction was created.\n    if (isManual) {\n      bottommost_level_ = true;\n    }\n    return;\n  }\n  bottommost_level_ = true;\n  int num_levels = input_version_->vset_->NumberLevels();\n  for (int i = level() + 2; i < num_levels; i++) {\n    if (input_version_->vset_->NumLevelFiles(i) > 0) {\n      bottommost_level_ = false;\n      break;\n    }\n  }\n}\n\nvoid Compaction::ReleaseInputs() {\n  if (input_version_ != nullptr) {\n    input_version_->Unref();\n    input_version_ = nullptr;\n  }\n}\n\nvoid Compaction::ResetNextCompactionIndex() {\n  input_version_->ResetNextCompactionIndex(level_);\n}\n\nstatic void InputSummary(std::vector<FileMetaData*>& files,\n    char* output,\n    int len) {\n  int write = 0;\n  for (unsigned int i = 0; i < files.size(); i++) {\n    int sz = len - write;\n    int ret = snprintf(output + write, sz, \"%lu(%lu) \",\n        (unsigned long)files.at(i)->number,\n        (unsigned long)files.at(i)->file_size);\n    if (ret < 0 || ret >= sz)\n      break;\n    write += ret;\n  }\n}\n\nvoid Compaction::Summary(char* output, int len) {\n  int write = snprintf(output, len,\n      \"Base version %lu Base level %d, seek compaction:%d, inputs:\",\n      (unsigned long)input_version_->GetVersionNumber(),\n      level_,\n      seek_compaction_);\n  if (write < 0 || write > len) {\n    return;\n  }\n\n  char level_low_summary[100];\n  InputSummary(inputs_[0], level_low_summary, sizeof(level_low_summary));\n  char level_up_summary[100];\n  if (inputs_[1].size()) {\n    InputSummary(inputs_[1], level_up_summary, sizeof(level_up_summary));\n  } else {\n    level_up_summary[0] = '\\0';\n  }\n\n  snprintf(output + write, len - write, \"[%s],[%s]\",\n      level_low_summary, level_up_summary);\n}\n",
        "res_region": "ColumnFamilyData* VersionSet::CreateColumnFamily(\n    const ColumnFamilyOptions& options, VersionEdit* edit) {\n  assert(column_families_.find(edit->column_family_name_) ==\n         column_families_.end());\n  assert(edit->is_column_family_add_);\n\n  column_families_.insert({edit->column_family_name_, edit->column_family_});\n  ColumnFamilyData* new_cfd =\n      new ColumnFamilyData(edit->column_family_name_, this, options);\n  column_family_data_.insert({edit->column_family_, new_cfd});\n  max_column_family_ = std::max(max_column_family_, edit->column_family_);\n  AppendVersion(new_cfd, new Version(this, current_version_number_++));\n  return new_cfd;\n}\n\nvoid VersionSet::DropColumnFamily(VersionEdit* edit) {\n  auto cfd = column_family_data_.find(edit->column_family_);\n  assert(cfd != column_family_data_.end());\n  column_families_.erase(cfd->second->name);\n  cfd->second->current->Unref();\n  // List must be empty\n  assert(cfd->second->dummy_versions.next_ == &cfd->second->dummy_versions);\n  // might delete itself\n  cfd->second->Unref();\n  column_family_data_.erase(cfd);\n}\n",
        "lookback": "    const ColumnFamilyOptions& options, VersionEdit* edit) {",
        "lookahead": "}  // namespace rocksdb",
        "label": [
            "interleave",
            "base_empty"
        ],
        "start_line": 4298,
        "end_line": 4833,
        "merge_gen_region": null,
        "confidence": 0.6846550107002258,
        "hesitated": true
    }
]